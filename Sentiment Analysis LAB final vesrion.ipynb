{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1327a65-890b-439c-bf77-ca623a4a3351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from gensim.models import word2vec,KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer,AutoModel,AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "688eda69-7a76-4458-94c7-b35a7360d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a55b421-1558-46a0-ba9d-3d1cfb2efa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdd5ed5-349d-4cf7-bfe4-029654a57c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469133d4-e0f6-4bbf-9ca8-bb4c6e395958",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_google_new=api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e96a0b3-63be-4d1c-b61b-0f3e142fc058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/venky/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989d80e8-097d-4891-beb4-5a8438c6fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    #convert to text to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove emails\n",
    "    text=re.sub(r\"^\\S+@\\S+\\.\\S+$\",\"\",text)\n",
    "\n",
    "    #remove special characters\n",
    "    text=re.sub(r'[^A-Za-z0-9\\s]',\"\",text)\n",
    "\n",
    "    #remove any IP adderesses\n",
    "    text=re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b',\"\",text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    #create tokrnizer instance\n",
    "    tokenizer=get_tokenizer('basic_english')\n",
    "    tokens=tokenizer(text)\n",
    "\n",
    "    #remove any stopwords if any\n",
    "    filtered_tokens=[token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "#function to vectorize \n",
    "def vectorize_word2vec_pretrained(tokens):\n",
    "    '''This function will use pretrained word to vec that is trained on google news.\n",
    "       We will pass token by token to vectorize it, if some oov word is found we will ignore it for now'''\n",
    "\n",
    "    vectorized_token=[wv_google_new[token] for token in tokens if token in wv_google_new]\n",
    "    return vectorized_token\n",
    "        \n",
    "\n",
    "\n",
    "def split_train_test_val(data,col):\n",
    "    '''This function will split the data into train,val and test sets and will stratify on the col column passed'''\n",
    "    \n",
    "    train_df,temp_df=train_test_split(data,test_size=0.30,random_state=42,stratify=data[col])\n",
    "    val_df,test_df=train_test_split(temp_df,test_size=0.50,random_state=42,stratify=temp_df[col])\n",
    "\n",
    "    train_df=train_df.reset_index(drop=True)\n",
    "    val_df=val_df.reset_index(drop=True)\n",
    "    test_df=test_df.reset_index(drop=True)\n",
    "\n",
    "    return train_df,val_df,test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c763f-8fed-4682-8090-7b40776dd099",
   "metadata": {},
   "source": [
    "### Load the amazon reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5002cb60-6464-4361-84d2-f68bcd0e2329",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews=pd.read_csv('amazon_cells_labelled_LARGE_25K.txt',delimiter='\\t',names=['Sentence','Class'],header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db9680c-85a9-43da-8ce5-0a4fb3bf9590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I've read this book with much expectation, it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love it...very touch.it's to bad that in the d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The creepiest book I've ever read! It's a cree...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It starts off a bit slow, but once the product...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As good as this book may be, the print quality...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class\n",
       "0  I've read this book with much expectation, it ...      0\n",
       "1  love it...very touch.it's to bad that in the d...      1\n",
       "2  The creepiest book I've ever read! It's a cree...      1\n",
       "3  It starts off a bit slow, but once the product...      1\n",
       "4  As good as this book may be, the print quality...      0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca7703b-4301-43c6-9cb5-a51555961e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the text\n",
    "amazon_reviews['Sentence']=amazon_reviews['Sentence'].apply(lambda x:preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b3f375-e24a-4279-a3b8-b8ff34999e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ive read this book with much expectation it wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love itvery touchits to bad that in the dvd de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the creepiest book ive ever read its a creepy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it starts off a bit slow but once the product ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as good as this book may be the print quality ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class\n",
       "0  ive read this book with much expectation it wa...      0\n",
       "1  love itvery touchits to bad that in the dvd de...      1\n",
       "2  the creepiest book ive ever read its a creepy ...      1\n",
       "3  it starts off a bit slow but once the product ...      1\n",
       "4  as good as this book may be the print quality ...      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f302fad4-b472-44b3-a618-ee492c689cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews['Tokenized Text']=amazon_reviews['Sentence'].apply(lambda x: tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82f6b530-5099-47a6-baf1-d746193dba10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ive read this book with much expectation it wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ive, read, book, much, expectation, boring, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love itvery touchits to bad that in the dvd de...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, itvery, touchits, bad, dvd, description...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the creepiest book ive ever read its a creepy ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[creepiest, book, ive, ever, read, creepy, mys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it starts off a bit slow but once the product ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[starts, bit, slow, product, placement, jokes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as good as this book may be the print quality ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, book, may, print, quality, bad, cannot,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  ive read this book with much expectation it wa...      0   \n",
       "1  love itvery touchits to bad that in the dvd de...      1   \n",
       "2  the creepiest book ive ever read its a creepy ...      1   \n",
       "3  it starts off a bit slow but once the product ...      1   \n",
       "4  as good as this book may be the print quality ...      0   \n",
       "\n",
       "                                      Tokenized Text  \n",
       "0  [ive, read, book, much, expectation, boring, b...  \n",
       "1  [love, itvery, touchits, bad, dvd, description...  \n",
       "2  [creepiest, book, ive, ever, read, creepy, mys...  \n",
       "3  [starts, bit, slow, product, placement, jokes,...  \n",
       "4  [good, book, may, print, quality, bad, cannot,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bbadb5c-714a-4255-8e98-2c8a72e0142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews['Vectorized Tokens Pretrained']=amazon_reviews['Tokenized Text'].apply(lambda x: vectorize_word2vec_pretrained(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6fde9ef-ed50-4a7a-9f8b-1785e15dd89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ive read this book with much expectation it wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ive, read, book, much, expectation, boring, b...</td>\n",
       "      <td>[[-0.41210938, 0.18847656, -0.234375, 0.296875...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love itvery touchits to bad that in the dvd de...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, itvery, touchits, bad, dvd, description...</td>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the creepiest book ive ever read its a creepy ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[creepiest, book, ive, ever, read, creepy, mys...</td>\n",
       "      <td>[[-0.017822266, -0.12158203, 0.16601562, 0.104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it starts off a bit slow but once the product ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[starts, bit, slow, product, placement, jokes,...</td>\n",
       "      <td>[[0.049560547, 0.18261719, -0.18554688, 0.1074...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as good as this book may be the print quality ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, book, may, print, quality, bad, cannot,...</td>\n",
       "      <td>[[0.040527344, 0.0625, -0.017456055, 0.0786132...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  ive read this book with much expectation it wa...      0   \n",
       "1  love itvery touchits to bad that in the dvd de...      1   \n",
       "2  the creepiest book ive ever read its a creepy ...      1   \n",
       "3  it starts off a bit slow but once the product ...      1   \n",
       "4  as good as this book may be the print quality ...      0   \n",
       "\n",
       "                                      Tokenized Text  \\\n",
       "0  [ive, read, book, much, expectation, boring, b...   \n",
       "1  [love, itvery, touchits, bad, dvd, description...   \n",
       "2  [creepiest, book, ive, ever, read, creepy, mys...   \n",
       "3  [starts, bit, slow, product, placement, jokes,...   \n",
       "4  [good, book, may, print, quality, bad, cannot,...   \n",
       "\n",
       "                        Vectorized Tokens Pretrained  \n",
       "0  [[-0.41210938, 0.18847656, -0.234375, 0.296875...  \n",
       "1  [[0.103027344, -0.15234375, 0.025878906, 0.165...  \n",
       "2  [[-0.017822266, -0.12158203, 0.16601562, 0.104...  \n",
       "3  [[0.049560547, 0.18261719, -0.18554688, 0.1074...  \n",
       "4  [[0.040527344, 0.0625, -0.017456055, 0.0786132...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ec675c6-e7e2-4aef-9791-3c5bdc8d02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,val_df,test_df=split_train_test_val(amazon_reviews,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abc66b7f-6174-42f6-8f73-42f8c1a9ee4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it looks great when you install it but it wont...</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, great, install, wont, last, long, woul...</td>\n",
       "      <td>[[0.024047852, 0.31445312, -0.026245117, 0.071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another great transitional and educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, great, transitional, educational, do...</td>\n",
       "      <td>[[0.19433594, -0.01965332, 0.091796875, 0.1044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just when you think oh great another zombie mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, oh, great, another, zombie, movie, fid...</td>\n",
       "      <td>[[-0.046875, 0.06689453, 0.009338379, 0.263671...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very goodthis kept me wanting more could not w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[goodthis, kept, wanting, could, wait, see, ha...</td>\n",
       "      <td>[[0.048828125, 0.084472656, -0.029052734, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love this series i just cant get enough the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, series, cant, get, enough, people, acti...</td>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  it looks great when you install it but it wont...      0   \n",
       "1  this is another great transitional and educati...      1   \n",
       "2  just when you think oh great another zombie mo...      1   \n",
       "3  very goodthis kept me wanting more could not w...      0   \n",
       "4  i love this series i just cant get enough the ...      1   \n",
       "\n",
       "                                      Tokenized Text  \\\n",
       "0  [looks, great, install, wont, last, long, woul...   \n",
       "1  [another, great, transitional, educational, do...   \n",
       "2  [think, oh, great, another, zombie, movie, fid...   \n",
       "3  [goodthis, kept, wanting, could, wait, see, ha...   \n",
       "4  [love, series, cant, get, enough, people, acti...   \n",
       "\n",
       "                        Vectorized Tokens Pretrained  \n",
       "0  [[0.024047852, 0.31445312, -0.026245117, 0.071...  \n",
       "1  [[0.19433594, -0.01965332, 0.091796875, 0.1044...  \n",
       "2  [[-0.046875, 0.06689453, 0.009338379, 0.263671...  \n",
       "3  [[0.048828125, 0.084472656, -0.029052734, 0.07...  \n",
       "4  [[0.103027344, -0.15234375, 0.025878906, 0.165...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba10fd-a5d6-4c88-85a2-2540f32f087a",
   "metadata": {},
   "source": [
    "#### Creating data classes and data loaders for the pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3304c674-c710-4441-a4e4-3e33f20452a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we will create a custom data class, which inherits from Dataset\n",
    "'''\n",
    "\n",
    "class Word2VecData(Dataset):\n",
    "    def __init__(self,data,max_len=500):\n",
    "        self.data=data.reset_index(drop=True)  #here we are initialising the data\n",
    "        self.max_len=max_len  #here we are taking a max length, how many token we want to pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return(len(self.data))  #return how many rows the dataframe has\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        row=self.data.iloc[idx]\n",
    "\n",
    "        features=row['Vectorized Tokens Pretrained']\n",
    "        labels=row['Class']\n",
    "\n",
    "        #before conversion, check if features are numpy array\n",
    "        if isinstance(features,(list,np.array)):\n",
    "            tensors=torch.FloatTensor(features)\n",
    "        else:\n",
    "            tensors=torch.FloatTensor([features])\n",
    "\n",
    "        labels=torch.FloatTensor([labels])\n",
    "\n",
    "        return tensors,labels\n",
    "\n",
    "\n",
    "\n",
    "#create a collate function\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences,labels=zip(*batch)\n",
    "    lengths = torch.tensor([seq.size(0) for seq in sequences], dtype=torch.long)\n",
    "    padded_sequence=pad_sequence(sequences,batch_first=True)\n",
    "    labels=torch.stack(labels)\n",
    "\n",
    "    return padded_sequence,labels,lengths\n",
    "\n",
    "\n",
    "\n",
    "#create dataloaders\n",
    "# train_loader=DataLoader(Word2VecData(train_df),batch_size=16,collate_fn=collate_fn,num_workers=0)\n",
    "# val_loader=DataLoader(Word2VecData(val_df),batch_size=16,collate_fn=collate_fn)\n",
    "# test_loader=DataLoader(Word2VecData(test_df),batch_size=16,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d0914-69ea-425d-86f1-ac2dadaf87f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98f91542-a399-4702-96f6-3c69ac6bdca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it looks great when you install it but it wont...</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, great, install, wont, last, long, woul...</td>\n",
       "      <td>[[0.024047852, 0.31445312, -0.026245117, 0.071...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another great transitional and educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, great, transitional, educational, do...</td>\n",
       "      <td>[[0.19433594, -0.01965332, 0.091796875, 0.1044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just when you think oh great another zombie mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, oh, great, another, zombie, movie, fid...</td>\n",
       "      <td>[[-0.046875, 0.06689453, 0.009338379, 0.263671...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very goodthis kept me wanting more could not w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[goodthis, kept, wanting, could, wait, see, ha...</td>\n",
       "      <td>[[0.048828125, 0.084472656, -0.029052734, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love this series i just cant get enough the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, series, cant, get, enough, people, acti...</td>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17495</th>\n",
       "      <td>didnt work right out of the box so i asked for...</td>\n",
       "      <td>0</td>\n",
       "      <td>[didnt, work, right, box, asked, refund, leery...</td>\n",
       "      <td>[[-0.075683594, 0.033691406, -0.064941406, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17496</th>\n",
       "      <td>product was a gba charger not a ds charger the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[product, gba, charger, ds, charger, gba, char...</td>\n",
       "      <td>[[-0.061523438, 0.095214844, 0.13378906, 0.064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17497</th>\n",
       "      <td>this is one of the oldies but goodies i enjoye...</td>\n",
       "      <td>1</td>\n",
       "      <td>[one, oldies, goodies, enjoyed, movie, child, ...</td>\n",
       "      <td>[[0.045654297, -0.14550781, 0.15625, 0.1660156...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17498</th>\n",
       "      <td>if this book didnt have quotes it would be a p...</td>\n",
       "      <td>0</td>\n",
       "      <td>[book, didnt, quotes, would, pamphlet, definit...</td>\n",
       "      <td>[[0.11279297, -0.026123047, -0.044921875, 0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17499</th>\n",
       "      <td>wont waste your time item was exactly what i w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wont, waste, time, item, exactly, wanted, fit...</td>\n",
       "      <td>[[0.18457031, 0.020141602, -0.022949219, 0.277...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  Class  \\\n",
       "0      it looks great when you install it but it wont...      0   \n",
       "1      this is another great transitional and educati...      1   \n",
       "2      just when you think oh great another zombie mo...      1   \n",
       "3      very goodthis kept me wanting more could not w...      0   \n",
       "4      i love this series i just cant get enough the ...      1   \n",
       "...                                                  ...    ...   \n",
       "17495  didnt work right out of the box so i asked for...      0   \n",
       "17496  product was a gba charger not a ds charger the...      0   \n",
       "17497  this is one of the oldies but goodies i enjoye...      1   \n",
       "17498  if this book didnt have quotes it would be a p...      0   \n",
       "17499  wont waste your time item was exactly what i w...      1   \n",
       "\n",
       "                                          Tokenized Text  \\\n",
       "0      [looks, great, install, wont, last, long, woul...   \n",
       "1      [another, great, transitional, educational, do...   \n",
       "2      [think, oh, great, another, zombie, movie, fid...   \n",
       "3      [goodthis, kept, wanting, could, wait, see, ha...   \n",
       "4      [love, series, cant, get, enough, people, acti...   \n",
       "...                                                  ...   \n",
       "17495  [didnt, work, right, box, asked, refund, leery...   \n",
       "17496  [product, gba, charger, ds, charger, gba, char...   \n",
       "17497  [one, oldies, goodies, enjoyed, movie, child, ...   \n",
       "17498  [book, didnt, quotes, would, pamphlet, definit...   \n",
       "17499  [wont, waste, time, item, exactly, wanted, fit...   \n",
       "\n",
       "                            Vectorized Tokens Pretrained  \n",
       "0      [[0.024047852, 0.31445312, -0.026245117, 0.071...  \n",
       "1      [[0.19433594, -0.01965332, 0.091796875, 0.1044...  \n",
       "2      [[-0.046875, 0.06689453, 0.009338379, 0.263671...  \n",
       "3      [[0.048828125, 0.084472656, -0.029052734, 0.07...  \n",
       "4      [[0.103027344, -0.15234375, 0.025878906, 0.165...  \n",
       "...                                                  ...  \n",
       "17495  [[-0.075683594, 0.033691406, -0.064941406, 0.1...  \n",
       "17496  [[-0.061523438, 0.095214844, 0.13378906, 0.064...  \n",
       "17497  [[0.045654297, -0.14550781, 0.15625, 0.1660156...  \n",
       "17498  [[0.11279297, -0.026123047, -0.044921875, 0.06...  \n",
       "17499  [[0.18457031, 0.020141602, -0.022949219, 0.277...  \n",
       "\n",
       "[17500 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363ee2c-5c50-422a-a706-4e79b8e4ee22",
   "metadata": {},
   "source": [
    "### Create RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99e8ff5-e1e0-4aba-b41a-02b2280ad1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordToVecRNNPT(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(WordToVecRNNPT,self).__init__()\n",
    "        self.rnn=nn.RNN(input_size,hidden_size)\n",
    "        self.hidden_to_op=nn.Linear(hidden_size,output_size)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,tensors,length):\n",
    "        '''we padded the sequences so that the input is of same shape, we need to unpack it so rnn learns from real input and not\n",
    "        padded one'''\n",
    "\n",
    "        pack_padded_tensors=pack_padded_sequence(tensors,length.cpu(),batch_first=True,enforce_sorted=False)\n",
    "        rnn_out,hidden=self.rnn(pack_padded_tensors)\n",
    "        output=self.hidden_to_op(hidden[-1])\n",
    "        output=self.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3a6f145-5cc7-4890-b283-79d197fa7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input size will number of dimension of vector\n",
    "input_size=300\n",
    "hidden_size=128 #we give it\n",
    "output_size=1  #binary classification\n",
    "\n",
    "word2vec_pt_model=WordToVecRNNPT(input_size,hidden_size,output_size)\n",
    "print(word2vec_pt_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384fc014-54ec-4011-b46b-29e7d7917785",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a53063a-35dc-49f4-9002-1f3f86f4a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters for training\n",
    "epochs=150\n",
    "lr=1e-5\n",
    "criterion=nn.BCELoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e466d424-091d-4095-b045-7eb120ad0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model,train_loader,val_loader,epochs,lr,criterion):\n",
    "    optimizer=SGD(model.parameters(),lr=lr)\n",
    "    train_loss_epoch=[]\n",
    "    val_loss_epoch=[]\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        curr_train_loss=0\n",
    "        avg_train_loss=0\n",
    "        curr_val_loss=0\n",
    "        avg_val_loss=0\n",
    "\n",
    "        for ix,train_data in enumerate(train_loader):\n",
    "            \n",
    "            inputs,labels,lengths=train_data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            #set training mode\n",
    "            model.train()\n",
    "\n",
    "            #reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            #forward pass\n",
    "            output=model(inputs,lengths)\n",
    "\n",
    "            #calculate loss\n",
    "            \n",
    "            loss=criterion(output,labels)\n",
    "\n",
    "            curr_train_loss+=loss.item()\n",
    "\n",
    "            #compute gradient\n",
    "            loss.backward()\n",
    "\n",
    "            #update params\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        avg_train_loss=curr_train_loss/len(train_loader)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx,val_data in enumerate(val_loader):\n",
    "                inputs,labels,lengths=val_data\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                lengths = lengths.to(device)\n",
    "\n",
    "                val_output=model(inputs,lengths)\n",
    "                val_loss=criterion(val_output,labels)\n",
    "\n",
    "                curr_val_loss+=val_loss.item()\n",
    "\n",
    "\n",
    "            avg_val_loss=curr_val_loss/len(val_loader)\n",
    "\n",
    "        train_loss_epoch.append(avg_train_loss)\n",
    "        val_loss_epoch.append(avg_val_loss)\n",
    "\n",
    "        if (i+1)%5==0:\n",
    "            print(f\"epoch {i+1}:  training loss : {avg_train_loss}, val loss: {avg_val_loss}\")\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48806e41-3ed1-445a-817b-39a46a87f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to test our model\n",
    "def test_model(model,test_loader):\n",
    "    all_predictions=[]\n",
    "    all_labels=[]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, test_data in enumerate(test_loader):\n",
    "            inputs, labels,lengths=test_data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "\n",
    "            outputs=model.forward(inputs,lengths)  #gives probablities \n",
    "            predictions=(outputs>0.5).float()\n",
    "\n",
    "            labels=labels.float()  #ensure they are also float\n",
    "\n",
    "            all_labels.append(labels.view(-1))\n",
    "            all_predictions.append(predictions.view(-1))\n",
    "\n",
    "\n",
    "        all_predictions=torch.cat(all_predictions,dim=0)\n",
    "        all_labels=torch.cat(all_labels,dim=0)\n",
    "\n",
    "        accuracy=(all_predictions==all_labels).float().mean()\n",
    "\n",
    "\n",
    "        return accuracy.item()\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d89d29d-0f52-43f7-9eb2-7db20b31c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/d0lxwjpj4v527f9nxwhfz6m80000gn/T/ipykernel_40516/3378725027.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)\n",
      "  tensors=torch.FloatTensor(features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:  training loss : 0.6936645545405923, val loss: 0.6930644631385803\n",
      "epoch 10:  training loss : 0.6910569034721123, val loss: 0.6905001533792374\n",
      "epoch 15:  training loss : 0.688707331471295, val loss: 0.6881897431738833\n",
      "epoch 20:  training loss : 0.6865866309001197, val loss: 0.6861045365637921\n",
      "epoch 25:  training loss : 0.6846698105771102, val loss: 0.6842199858198774\n",
      "epoch 30:  training loss : 0.6829351796964405, val loss: 0.6825147377683761\n",
      "epoch 35:  training loss : 0.6813638733880394, val loss: 0.6809702609447723\n",
      "epoch 40:  training loss : 0.679939319326847, val loss: 0.6795702122627421\n",
      "epoch 45:  training loss : 0.678646739647201, val loss: 0.6783000441307717\n",
      "epoch 50:  training loss : 0.6774729965918679, val loss: 0.6771468213263978\n",
      "epoch 55:  training loss : 0.6764063080551419, val loss: 0.6760989171393375\n",
      "epoch 60:  training loss : 0.6754360090552998, val loss: 0.6751458259339028\n",
      "epoch 65:  training loss : 0.6745525524428819, val loss: 0.6742781344880449\n",
      "epoch 70:  training loss : 0.6737471728481802, val loss: 0.6734872143319313\n",
      "epoch 75:  training loss : 0.6730120595020176, val loss: 0.6727653538927119\n",
      "epoch 80:  training loss : 0.672340043393088, val loss: 0.6721054893858889\n",
      "epoch 85:  training loss : 0.6717245617448957, val loss: 0.6715011398842994\n",
      "epoch 90:  training loss : 0.6711597262832321, val loss: 0.6709465206937587\n",
      "epoch 95:  training loss : 0.6706401461112216, val loss: 0.6704363117826746\n",
      "epoch 100:  training loss : 0.6701609734207447, val loss: 0.6699657209376071\n",
      "epoch 105:  training loss : 0.6697177601470807, val loss: 0.6695303868740163\n",
      "epoch 110:  training loss : 0.6693065347357685, val loss: 0.6691263972444738\n",
      "epoch 115:  training loss : 0.6689236688221613, val loss: 0.6687501772921136\n",
      "epoch 120:  training loss : 0.6685659002778299, val loss: 0.6683985053224767\n",
      "epoch 125:  training loss : 0.6682303430720264, val loss: 0.6680685606408626\n",
      "epoch 130:  training loss : 0.6679143152681521, val loss: 0.667757684372841\n",
      "epoch 135:  training loss : 0.6676154812794499, val loss: 0.667463584656411\n",
      "epoch 140:  training loss : 0.6673317114112582, val loss: 0.6671841631544397\n",
      "epoch 145:  training loss : 0.6670611525257502, val loss: 0.6669176243721171\n",
      "epoch 150:  training loss : 0.6668021039291535, val loss: 0.6666622707184325\n"
     ]
    }
   ],
   "source": [
    "#train the network\n",
    "train_network(word2vec_pt_model,train_loader,val_loader,epochs,lr,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7e037d7-c11a-4eea-a274-d0dc34fd74b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60\n"
     ]
    }
   ],
   "source": [
    "rnn_test_acc=test_model(word2vec_pt_model,test_loader)\n",
    "print(f\"{rnn_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4156b6a6-f40a-4933-8ea7-161204306b95",
   "metadata": {},
   "source": [
    "### Creating an LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753de4df-f0b0-4e4e-ad84-a6d2ecdaa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(LSTMModel,self).__init__()\n",
    "\n",
    "        self.LSTM=nn.LSTM(input_size,hidden_size,num_layers=1)\n",
    "        self.output_layer=nn.Linear(hidden_size,output_size)\n",
    "        self.sigmoid_layer=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,input_tensors,length):\n",
    "\n",
    "        #pack padded sequence so that padded tokens does not go into the model\n",
    "        packed_tensors=pack_padded_sequence(input_tensors,length.cpu(),enforce_sorted=False,batch_first=True)\n",
    "        x,(h_n,c_n)=self.LSTM(packed_tensors)\n",
    "        output=self.output_layer(h_n[-1])\n",
    "        output=self.sigmoid_layer(output)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "becf85d2-23ed-43d6-bf62-86aef19abd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (LSTM): LSTM(300, 128)\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid_layer): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_model=LSTMModel(input_size,hidden_size,output_size)\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a9fd877-a3ae-49ee-b301-176739d388d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:  training loss : 0.683785950516434, val loss: 0.6837001224781605\n",
      "epoch 10:  training loss : 0.6832430493461149, val loss: 0.683159091371171\n",
      "epoch 15:  training loss : 0.6827206838283504, val loss: 0.6826384805618448\n",
      "epoch 20:  training loss : 0.6822179961248115, val loss: 0.6821374609115276\n",
      "epoch 25:  training loss : 0.6817342518561501, val loss: 0.6816552725244076\n",
      "epoch 30:  training loss : 0.6812686652653396, val loss: 0.6811911476419327\n",
      "epoch 35:  training loss : 0.6808205084565353, val loss: 0.6807443588338\n",
      "epoch 40:  training loss : 0.6803890882397047, val loss: 0.6803142101206678\n",
      "epoch 45:  training loss : 0.6799737056495938, val loss: 0.6799000184586708\n",
      "epoch 50:  training loss : 0.6795737334208689, val loss: 0.6795011560967628\n",
      "epoch 55:  training loss : 0.6791885692959948, val loss: 0.6791170120239258\n",
      "epoch 60:  training loss : 0.6788176134691813, val loss: 0.6787469934909902\n",
      "epoch 65:  training loss : 0.6784603073784377, val loss: 0.6783905574615966\n",
      "epoch 70:  training loss : 0.6781161094060546, val loss: 0.6780471469493623\n",
      "epoch 75:  training loss : 0.6777844703611552, val loss: 0.6777162260197579\n",
      "epoch 80:  training loss : 0.677464915041078, val loss: 0.677397328742007\n",
      "epoch 85:  training loss : 0.6771569497733491, val loss: 0.6770899519007256\n",
      "epoch 90:  training loss : 0.6768601113960991, val loss: 0.6767936239851282\n",
      "epoch 95:  training loss : 0.6765739760298616, val loss: 0.6765079536336533\n",
      "epoch 100:  training loss : 0.6762981008894918, val loss: 0.676232461472775\n",
      "epoch 105:  training loss : 0.6760320842157116, val loss: 0.6759667878455304\n",
      "epoch 110:  training loss : 0.675775527572719, val loss: 0.6757105203385049\n",
      "epoch 115:  training loss : 0.6755280380275175, val loss: 0.6754632442555529\n",
      "epoch 120:  training loss : 0.6752892918952859, val loss: 0.6752246633489081\n",
      "epoch 125:  training loss : 0.6750589410282358, val loss: 0.6749944349552722\n",
      "epoch 130:  training loss : 0.6748365875353979, val loss: 0.6747721471685044\n",
      "epoch 135:  training loss : 0.6746219929437097, val loss: 0.6745575722227706\n",
      "epoch 140:  training loss : 0.6744148439746452, val loss: 0.674350384448437\n",
      "epoch 145:  training loss : 0.6742147883941748, val loss: 0.6741502655313371\n",
      "epoch 150:  training loss : 0.6740216032024712, val loss: 0.6739569494064818\n"
     ]
    }
   ],
   "source": [
    "#train the lstm model \n",
    "train_network(lstm_model,train_loader,val_loader,epochs,lr,criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b625b59-26c4-4c00-98bf-9a75126ffa42",
   "metadata": {},
   "source": [
    "#### Testing out LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95b22e00-4abc-4835-ae72-d5e642a1f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc=test_model(lstm_model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52c13e18-117e-47ca-a41c-780050fd7523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.60\n"
     ]
    }
   ],
   "source": [
    "print(f\"{test_acc: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416b577-403b-40fe-a62e-14f2b5b29ff0",
   "metadata": {},
   "source": [
    "### Building a GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ab3f48-ee89-47c4-b49f-042d1e79dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(GRUModel,self).__init__()\n",
    "\n",
    "        #GRU layer\n",
    "        self.GRU=nn.GRU(input_size,hidden_size,num_layers=1)\n",
    "        self.output_layer=nn.Linear(hidden_size,output_size)\n",
    "        self.sigmoid_layer=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,input_tensors,length):\n",
    "        packed_tensors=pack_padded_sequence(input_tensors,length.cpu(),batch_first=True,enforce_sorted=False)\n",
    "\n",
    "        op,hidden=self.GRU(packed_tensors)\n",
    "        output=self.output_layer(hidden[-1])\n",
    "        output=self.sigmoid_layer(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "53a0d335-dd73-48a0-933c-361ba567f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:  training loss : 0.6576787329787964, val loss: 0.6560546451426567\n",
      "epoch 10:  training loss : 0.6448155138130816, val loss: 0.6421988010406494\n",
      "epoch 15:  training loss : 0.625260347830748, val loss: 0.6209359014287908\n",
      "epoch 20:  training loss : 0.5937525889609806, val loss: 0.586508390497654\n",
      "epoch 25:  training loss : 0.5385269783369148, val loss: 0.5257469597014975\n",
      "epoch 30:  training loss : 0.44376516852943293, val loss: 0.43049704692465196\n",
      "epoch 35:  training loss : 0.4044157382802292, val loss: 0.39969099880533016\n",
      "epoch 40:  training loss : 0.3947364233665754, val loss: 0.3911111587856678\n",
      "epoch 45:  training loss : 0.38898592940795573, val loss: 0.38610516631222785\n",
      "epoch 50:  training loss : 0.38501914937927695, val loss: 0.3827920995494153\n",
      "epoch 55:  training loss : 0.3820755751071399, val loss: 0.38043162401052233\n",
      "epoch 60:  training loss : 0.37977183099420675, val loss: 0.37866233374844205\n",
      "epoch 65:  training loss : 0.377898078520784, val loss: 0.37729008194613967\n",
      "epoch 70:  training loss : 0.3763305726350443, val loss: 0.3762000600708292\n",
      "epoch 75:  training loss : 0.3749905278398851, val loss: 0.37531832111008623\n",
      "epoch 80:  training loss : 0.3738243468582521, val loss: 0.3745941514030416\n",
      "epoch 85:  training loss : 0.37279366056215174, val loss: 0.37399107243152374\n",
      "epoch 90:  training loss : 0.37186999753853084, val loss: 0.3734821219076502\n",
      "epoch 95:  training loss : 0.3710316728112685, val loss: 0.37304689484707854\n",
      "epoch 100:  training loss : 0.37026187418611, val loss: 0.3726697271808665\n",
      "epoch 105:  training loss : 0.3695473992819106, val loss: 0.37233859956898585\n",
      "epoch 110:  training loss : 0.3688777206530301, val loss: 0.3720440120139021\n",
      "epoch 115:  training loss : 0.36824442032159355, val loss: 0.3717785769637595\n",
      "epoch 120:  training loss : 0.36764067129472056, val loss: 0.3715365093122137\n",
      "epoch 125:  training loss : 0.36706090817939646, val loss: 0.37131305609611753\n",
      "epoch 130:  training loss : 0.3665005751498983, val loss: 0.37110462857687726\n",
      "epoch 135:  training loss : 0.3659558872840413, val loss: 0.3709081877419289\n",
      "epoch 140:  training loss : 0.3654237071723018, val loss: 0.3707213263879431\n",
      "epoch 145:  training loss : 0.3649013869437156, val loss: 0.3705420696354927\n",
      "epoch 150:  training loss : 0.36438671388481175, val loss: 0.37036884880446375\n"
     ]
    }
   ],
   "source": [
    "#create an instnace of the model\n",
    "gru_model=GRUModel(input_size,hidden_size,output_size)\n",
    "\n",
    "\n",
    "#train the model\n",
    "train_network(gru_model,train_loader,val_loader,epochs,lr,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c00d707d-f12a-4243-bb4a-52aadb3ad732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83\n"
     ]
    }
   ],
   "source": [
    "gru_test_acc=test_model(gru_model,test_loader)\n",
    "print(f\"{gru_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "641cd9a3-2651-4ac6-8529-4257c1d32f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvenky_ai\u001b[0m (\u001b[33mvenky_ai-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### wandb login\n",
    "# !pip3 install wandb\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588eb8e3-e928-4dfb-a216-b018917c0fbc",
   "metadata": {},
   "source": [
    "### Hugging Face Transformers\n",
    "\n",
    "Here we will use different pre trained models from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa339c9-42f9-45ad-9996-2514add890bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_using_hf(text,tokenizer):\n",
    "    '''This function will tokenize the text using the particular \n",
    "       tokenizer from hugging face'''\n",
    "\n",
    "    tokenized_text=tokenizer(text,\n",
    "                            padding='max_length', #padding the tokens so al inputs are of same length\n",
    "                            truncation=True,\n",
    "                            max_length=500,\n",
    "                            return_tensors='pt')   #returns a dictionary\n",
    "    return (tokenized_text['input_ids'].squeeze(),tokenized_text['attention_mask'].squeeze())\n",
    "\n",
    "    \n",
    "\n",
    "recall=evaluate.load('recall')\n",
    "precision=evaluate.load('precision')\n",
    "f1_score=evaluate.load('f1')\n",
    "accuracy=evaluate.load('accuracy')\n",
    "    \n",
    "def eval_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=1)  # For classification\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "        # \"f1\": f1_score.compute(predictions=predictions, references=labels)[\"f1\"],\n",
    "        # \"precision\": precision.compute(predictions=predictions, references=labels)[\"precision\"],\n",
    "        # \"recall\": recall.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def create_model(model,num_labels): \n",
    "    model=AutoModelForSequenceClassification.from_pretrained(model,num_labels=num_labels) \n",
    "    return model\n",
    "\n",
    "\n",
    "# def train_model(tokenizer,model,train_data,val_data,learning_rate,epochs,run_name,data_collator):\n",
    "#     print(\"setting training arguments\")\n",
    "#     training_arguments=TrainingArguments(per_device_train_batch_size=4,  #reduce batch size to prevent overfitting, just a try\n",
    "#                                          per_device_eval_batch_size=4,\n",
    "#                                          learning_rate=learning_rate,\n",
    "#                                          weight_decay=0.01,  #since for next dataset we have hige amout of data else keep it 0.2/0.5\n",
    "#                                         num_train_epochs=epochs,\n",
    "#                                         report_to='wandb',\n",
    "#                                         run_name=run_name,\n",
    "#                                         output_dir='./results',  #maybe this helps to log in wandb\n",
    "#                                         eval_strategy='epoch',\n",
    "#                                         logging_strategy='epoch',\n",
    "#                                         logging_dir='./logs',\n",
    "#                                         push_to_hub=True,\n",
    "#                                         hub_model_id=\"Venky19/Bert_for_sentiment_analysis_lab\")\n",
    "\n",
    "def train_model(tokenizer,model,train_data,val_data,learning_rate,epochs,run_name,data_collator):  \n",
    "    print(\"setting training arguments\")\n",
    "    training_arguments=TrainingArguments(per_device_train_batch_size=4,  #batch size is 4 for smaller dataset, 32 for larger one\n",
    "                                         per_device_eval_batch_size=4,\n",
    "                                         #gradient_accumulation_steps=2, \n",
    "                                         fp16=True,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         weight_decay=0.02,  #since for next dataset we have hige amout of data else keep it 0.2/0.5\n",
    "                                         num_train_epochs=epochs,\n",
    "                                         report_to='wandb',\n",
    "                                         run_name=run_name,\n",
    "                                        output_dir='./small_data/results',  #maybe this helps to log in wandb\n",
    "                                        eval_strategy='epoch',\n",
    "                                        logging_strategy='epoch',\n",
    "                                        logging_dir='./small_data/logs',\n",
    "                                        save_strategy='epoch',\n",
    "                                        load_best_model_at_end=True, \n",
    "                                        metric_for_best_model=\"eval_loss\",\n",
    "                                        greater_is_better=False)\n",
    "                                        #remove_unused_columns=False) \n",
    "                                        # push_to_hub=True,\n",
    "                                        # hub_model_id=\"Venky19/Bert_for_sentiment_analysis_lab\")    \n",
    "    print(\"Setting up the trainer....\")\n",
    "\n",
    "    print(\"move to cuda\")\n",
    "    model.to(\"cuda\")\n",
    "    model_is_on_cuda = next(model.parameters()).is_cuda\n",
    "    print(model_is_on_cuda)\n",
    "\n",
    "    wandb.init(project=\"lab1_Sentiment analysis small dataset\", name=run_name)  #project name \n",
    "    wandb.config.update({\n",
    "    \"learning_rate\":learning_rate,\n",
    "    \"epochs\":epochs,\n",
    "    \"model\":model,\n",
    "    \"batch_size\":4})  #batch size 4 for smaller size, 32 for large\n",
    "\n",
    "\n",
    "    trainer=Trainer(model=model,\n",
    "                    args=training_arguments,\n",
    "                    train_dataset=train_data,\n",
    "                    eval_dataset=val_data,\n",
    "                    tokenizer=tokenizer,\n",
    "                    compute_metrics=eval_metrics,\n",
    "                    data_collator=data_collator) #removed data collator\n",
    "                    \n",
    "\n",
    "    print(\"Training started\")\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    res=trainer.evaluate()\n",
    "\n",
    "    return trainer,res\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84af3140-a266-47f8-bd01-727082f87de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict={\"Model\":[],\n",
    "          \"Accuracy\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4201009-76e2-4202-b087-be2cd99ca964",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a custom data class\n",
    "class HuggingFaceDataset(Dataset):\n",
    "    def __init__(self,input_ids,attention_mask,labels):\n",
    "        self.input_ids=input_ids\n",
    "        self.attention_mask=attention_mask\n",
    "        self.labels=labels.tolist() if hasattr(labels,'tolist') else labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, list):\n",
    "            return {\n",
    "                'input_ids': [self.input_ids[i] for i in idx],\n",
    "                'attention_mask': [self.attention_mask[i] for i in idx],\n",
    "                'labels': torch.tensor([self.labels[i] for i in idx], dtype=torch.long)\n",
    "            }\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c713d1f-3cf3-4429-a5cd-fde08d767e29",
   "metadata": {},
   "source": [
    "#### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e930b4c7-2568-455d-89e2-65644e3a7258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer=AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "#tokenize train_df\n",
    "train_df['Bert Tokenizer']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[0])\n",
    "train_df['Bert Attention_Mask']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_df['Bert Tokenizer']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[0])\n",
    "val_df['Bert Attention_Mask']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[1])\n",
    "\n",
    "#create bert model\n",
    "bert_model=create_model('google-bert/bert-base-uncased',num_labels=2)\n",
    "\n",
    "#we will create the class of our dataset\n",
    "bert_train_df=HuggingFaceDataset(train_df['Bert Tokenizer'],train_df['Bert Attention_Mask'],train_df['Class'])\n",
    "bert_val_df=HuggingFaceDataset(val_df['Bert Tokenizer'],val_df['Bert Attention_Mask'],val_df['Class'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b961fab9-d781-47e6-b6f9-375906385603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aadf9bf4-ff20-4664-9c57-5d28fc389248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250417_202612-jhpry40k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis/runs/jhpry40k' target=\"_blank\">Bert Model</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis/runs/jhpry40k' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis/runs/jhpry40k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9104/2092782252.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43750' max='43750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43750/43750 1:01:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.264700</td>\n",
       "      <td>0.369842</td>\n",
       "      <td>0.918667</td>\n",
       "      <td>0.934196</td>\n",
       "      <td>0.914660</td>\n",
       "      <td>0.954586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.446230</td>\n",
       "      <td>0.925067</td>\n",
       "      <td>0.938820</td>\n",
       "      <td>0.927312</td>\n",
       "      <td>0.950617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.443489</td>\n",
       "      <td>0.924267</td>\n",
       "      <td>0.938767</td>\n",
       "      <td>0.918565</td>\n",
       "      <td>0.959877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.633386</td>\n",
       "      <td>0.922133</td>\n",
       "      <td>0.934470</td>\n",
       "      <td>0.951554</td>\n",
       "      <td>0.917989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.641733</td>\n",
       "      <td>0.926933</td>\n",
       "      <td>0.940409</td>\n",
       "      <td>0.927897</td>\n",
       "      <td>0.953263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.672033</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>0.938151</td>\n",
       "      <td>0.943379</td>\n",
       "      <td>0.932981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.796337</td>\n",
       "      <td>0.924533</td>\n",
       "      <td>0.936760</td>\n",
       "      <td>0.949705</td>\n",
       "      <td>0.924162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.765888</td>\n",
       "      <td>0.928533</td>\n",
       "      <td>0.941382</td>\n",
       "      <td>0.934028</td>\n",
       "      <td>0.948854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.787075</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.940186</td>\n",
       "      <td>0.944791</td>\n",
       "      <td>0.935626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.782092</td>\n",
       "      <td>0.928267</td>\n",
       "      <td>0.940579</td>\n",
       "      <td>0.942452</td>\n",
       "      <td>0.938713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2814' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 03:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_trainer,bert_res=train_model(bert_tokenizer,bert_model,bert_train_df,bert_val_df,learning_rate=lr,epochs=10,run_name=\"Bert Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aab61801-5cc5-4491-8669-8c2a8b7f3e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.36984163522720337,\n",
       " 'eval_accuracy': 0.9186666666666666,\n",
       " 'eval_f1': 0.9341963322545846,\n",
       " 'eval_precision': 0.9146599070553443,\n",
       " 'eval_recall': 0.9545855379188712,\n",
       " 'eval_runtime': 16.8399,\n",
       " 'eval_samples_per_second': 222.685,\n",
       " 'eval_steps_per_second': 55.701,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f247b040-2a54-45ca-82a9-dd47a546557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize test data\n",
    "test_df['Bert Tokenizer']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[0])\n",
    "test_df['Bert Attention_Mask']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[1])\n",
    "\n",
    "\n",
    "bert_test_df=HuggingFaceDataset(test_df['Bert Tokenizer'],test_df['Bert Attention_Mask'],test_df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a07bb1d-5488-4cf6-ad50-554d3a090480",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_res=bert_trainer.evaluate(bert_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bba0cd8-5f01-46c6-bca2-3fbbca238f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.39687129855155945,\n",
       " 'eval_accuracy': 0.9162666666666667,\n",
       " 'eval_f1': 0.9325601374570447,\n",
       " 'eval_precision': 0.9087484303055672,\n",
       " 'eval_recall': 0.9576532862814292,\n",
       " 'eval_runtime': 18.8973,\n",
       " 'eval_samples_per_second': 198.441,\n",
       " 'eval_steps_per_second': 49.637,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df034f58-638b-4313-b20c-a78480d07533",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict['Model'].append(\"Bert\")\n",
    "acc_dict['Accuracy'].append(bert_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd40bc10-55cd-4aa2-86db-ca999a55a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bert_res_dict.txt',\"w\") as f:\n",
    "    for key, value in bert_test_res.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2e814-b3f9-4482-908a-2fade583295a",
   "metadata": {},
   "source": [
    "#### Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03e10aa5-b539-4f9b-832f-b0efb6a80da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "roberta_tokenizer=AutoTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "\n",
    "##tokenize train_df\n",
    "train_df['Roberta Tokenizer']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[0])\n",
    "train_df['Roberta Attention_Mask']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_df['Roberta Tokenizer']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[0])\n",
    "val_df['Roberta Attention_Mask']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[1])\n",
    "\n",
    "\n",
    "#create roberta model\n",
    "roberta_model=create_model('FacebookAI/roberta-base',num_labels=2)\n",
    "\n",
    "\n",
    "#create the class of our dataset\n",
    "roberta_train_df=HuggingFaceDataset(train_df['Roberta Tokenizer'],train_df['Roberta Attention_Mask'],train_df['Class'])\n",
    "roberta_val_df=HuggingFaceDataset(val_df['Roberta Tokenizer'],val_df['Roberta Attention_Mask'],val_df['Class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a95aee3-153d-4e95-936a-d55b5d70164b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n",
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9104/4213672134.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43750' max='43750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43750/43750 1:04:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.384194</td>\n",
       "      <td>0.926933</td>\n",
       "      <td>0.941503</td>\n",
       "      <td>0.912666</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.344738</td>\n",
       "      <td>0.943733</td>\n",
       "      <td>0.953411</td>\n",
       "      <td>0.954887</td>\n",
       "      <td>0.951940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.148400</td>\n",
       "      <td>0.398546</td>\n",
       "      <td>0.934933</td>\n",
       "      <td>0.946042</td>\n",
       "      <td>0.948980</td>\n",
       "      <td>0.943122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.439301</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.950078</td>\n",
       "      <td>0.956230</td>\n",
       "      <td>0.944004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.431416</td>\n",
       "      <td>0.943733</td>\n",
       "      <td>0.953205</td>\n",
       "      <td>0.958947</td>\n",
       "      <td>0.947531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.472982</td>\n",
       "      <td>0.939733</td>\n",
       "      <td>0.950976</td>\n",
       "      <td>0.935952</td>\n",
       "      <td>0.966490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.473464</td>\n",
       "      <td>0.940800</td>\n",
       "      <td>0.950928</td>\n",
       "      <td>0.953457</td>\n",
       "      <td>0.948413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.471903</td>\n",
       "      <td>0.942933</td>\n",
       "      <td>0.953029</td>\n",
       "      <td>0.948864</td>\n",
       "      <td>0.957231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.544222</td>\n",
       "      <td>0.941600</td>\n",
       "      <td>0.952131</td>\n",
       "      <td>0.944083</td>\n",
       "      <td>0.960317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.534662</td>\n",
       "      <td>0.942667</td>\n",
       "      <td>0.952820</td>\n",
       "      <td>0.948449</td>\n",
       "      <td>0.957231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1876' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 07:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train the model\n",
    "roberta_trainer,roberta_res=train_model(roberta_tokenizer,roberta_model,roberta_train_df,roberta_val_df,lr,epochs=10)\n",
    "#removed parameter run name as already rep to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91050f3e-3c55-43bd-bc50-bac3ac738219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34473755955696106,\n",
       " 'eval_accuracy': 0.9437333333333333,\n",
       " 'eval_f1': 0.953411349083683,\n",
       " 'eval_precision': 0.9548872180451128,\n",
       " 'eval_recall': 0.9519400352733686,\n",
       " 'eval_runtime': 17.3227,\n",
       " 'eval_samples_per_second': 216.479,\n",
       " 'eval_steps_per_second': 54.149,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9f2fc01-bdc7-4b83-bcee-34d9f38dbf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model here\n",
    "\n",
    "#tokenize test data\n",
    "test_df['Roberta Tokenizer']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[0])\n",
    "test_df['Roberta Attention_Mask']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[1])\n",
    "\n",
    "\n",
    "roberta_test_df=HuggingFaceDataset(test_df['Roberta Tokenizer'],test_df['Roberta Attention_Mask'],test_df['Class'])\n",
    "\n",
    "roberta_test_res=roberta_trainer.evaluate(roberta_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5c5b41f4-ecc3-4f57-9ff1-c3e1b16bc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict['Model'].append(\"Roberta\")\n",
    "acc_dict['Accuracy'].append(roberta_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70f7b5-570a-416c-bfc9-09ca370a454a",
   "metadata": {},
   "source": [
    "#### XLNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "04341ec7-9d74-41d9-8ab4-a075108c184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet/xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "xlnet_tokenizer=AutoTokenizer.from_pretrained('xlnet/xlnet-base-cased')\n",
    "\n",
    "##tokenize train_df\n",
    "train_df['XLNet Tokenizer']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[0])\n",
    "train_df['XLNet Attention_Mask']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_df['XLNet Tokenizer']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[0])\n",
    "val_df['XLNet Attention_Mask']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[1])\n",
    "\n",
    "\n",
    "#create roberta model\n",
    "xlnet_model=create_model('xlnet/xlnet-base-cased',num_labels=2)\n",
    "\n",
    "\n",
    "#create the class of our dataset\n",
    "xlnet_train_df=HuggingFaceDataset(train_df['XLNet Tokenizer'],train_df['XLNet Attention_Mask'],train_df['Class'])\n",
    "xlnet_val_df=HuggingFaceDataset(val_df['XLNet Tokenizer'],val_df['XLNet Attention_Mask'],val_df['Class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f5c80e0-f021-4c93-9c76-6582b74b1dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "      <th>Bert Tokenizer</th>\n",
       "      <th>Bert Attention_Mask</th>\n",
       "      <th>Roberta Tokenizer</th>\n",
       "      <th>Roberta Attention_Mask</th>\n",
       "      <th>XLNet Tokenizer</th>\n",
       "      <th>XLNet Attention_Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it looks great when you install it but it wont...</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, great, install, wont, last, long, woul...</td>\n",
       "      <td>[[0.024047852, 0.31445312, -0.026245117, 0.071...</td>\n",
       "      <td>[tensor(101), tensor(2009), tensor(3504), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(405), tensor(1326), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another great transitional and educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, great, transitional, educational, do...</td>\n",
       "      <td>[[0.19433594, -0.01965332, 0.091796875, 0.1044...</td>\n",
       "      <td>[tensor(101), tensor(2023), tensor(2003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(9226), tensor(16), tensor(2...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just when you think oh great another zombie mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, oh, great, another, zombie, movie, fid...</td>\n",
       "      <td>[[-0.046875, 0.06689453, 0.009338379, 0.263671...</td>\n",
       "      <td>[tensor(101), tensor(2074), tensor(2043), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(8987), tensor(77), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very goodthis kept me wanting more could not w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[goodthis, kept, wanting, could, wait, see, ha...</td>\n",
       "      <td>[[0.048828125, 0.084472656, -0.029052734, 0.07...</td>\n",
       "      <td>[tensor(101), tensor(2200), tensor(2204), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(5525), tensor(205), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love this series i just cant get enough the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, series, cant, get, enough, people, acti...</td>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "      <td>[tensor(101), tensor(1045), tensor(2293), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(118), tensor(657), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  it looks great when you install it but it wont...      0   \n",
       "1  this is another great transitional and educati...      1   \n",
       "2  just when you think oh great another zombie mo...      1   \n",
       "3  very goodthis kept me wanting more could not w...      0   \n",
       "4  i love this series i just cant get enough the ...      1   \n",
       "\n",
       "                                      Tokenized Text  \\\n",
       "0  [looks, great, install, wont, last, long, woul...   \n",
       "1  [another, great, transitional, educational, do...   \n",
       "2  [think, oh, great, another, zombie, movie, fid...   \n",
       "3  [goodthis, kept, wanting, could, wait, see, ha...   \n",
       "4  [love, series, cant, get, enough, people, acti...   \n",
       "\n",
       "                        Vectorized Tokens Pretrained  \\\n",
       "0  [[0.024047852, 0.31445312, -0.026245117, 0.071...   \n",
       "1  [[0.19433594, -0.01965332, 0.091796875, 0.1044...   \n",
       "2  [[-0.046875, 0.06689453, 0.009338379, 0.263671...   \n",
       "3  [[0.048828125, 0.084472656, -0.029052734, 0.07...   \n",
       "4  [[0.103027344, -0.15234375, 0.025878906, 0.165...   \n",
       "\n",
       "                                      Bert Tokenizer  \\\n",
       "0  [tensor(101), tensor(2009), tensor(3504), tens...   \n",
       "1  [tensor(101), tensor(2023), tensor(2003), tens...   \n",
       "2  [tensor(101), tensor(2074), tensor(2043), tens...   \n",
       "3  [tensor(101), tensor(2200), tensor(2204), tens...   \n",
       "4  [tensor(101), tensor(1045), tensor(2293), tens...   \n",
       "\n",
       "                                 Bert Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                   Roberta Tokenizer  \\\n",
       "0  [tensor(0), tensor(405), tensor(1326), tensor(...   \n",
       "1  [tensor(0), tensor(9226), tensor(16), tensor(2...   \n",
       "2  [tensor(0), tensor(8987), tensor(77), tensor(4...   \n",
       "3  [tensor(0), tensor(5525), tensor(205), tensor(...   \n",
       "4  [tensor(0), tensor(118), tensor(657), tensor(4...   \n",
       "\n",
       "                              Roberta Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                     XLNet Tokenizer  \\\n",
       "0  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "1  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "2  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "3  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "4  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "\n",
       "                                XLNet Attention_Mask  \n",
       "0  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n",
       "1  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n",
       "2  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n",
       "3  [tensor(0), tensor(0), tensor(0), tensor(0), t...  \n",
       "4  [tensor(0), tensor(0), tensor(0), tensor(0), t...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "486f99ab-8e6b-452e-ae96-774f7cd82dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n",
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9104/4213672134.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43750' max='43750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43750/43750 2:46:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.421688</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>0.940950</td>\n",
       "      <td>0.913965</td>\n",
       "      <td>0.969577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.363595</td>\n",
       "      <td>0.935467</td>\n",
       "      <td>0.946341</td>\n",
       "      <td>0.951829</td>\n",
       "      <td>0.940917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.361289</td>\n",
       "      <td>0.934400</td>\n",
       "      <td>0.946288</td>\n",
       "      <td>0.937284</td>\n",
       "      <td>0.955467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.483797</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.935792</td>\n",
       "      <td>0.967514</td>\n",
       "      <td>0.906085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.544247</td>\n",
       "      <td>0.934400</td>\n",
       "      <td>0.945958</td>\n",
       "      <td>0.942644</td>\n",
       "      <td>0.949295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>0.549074</td>\n",
       "      <td>0.933067</td>\n",
       "      <td>0.945730</td>\n",
       "      <td>0.927874</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.591051</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>0.946417</td>\n",
       "      <td>0.946625</td>\n",
       "      <td>0.946208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.669135</td>\n",
       "      <td>0.934933</td>\n",
       "      <td>0.946655</td>\n",
       "      <td>0.938855</td>\n",
       "      <td>0.954586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.661072</td>\n",
       "      <td>0.934933</td>\n",
       "      <td>0.946585</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.953263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.693527</td>\n",
       "      <td>0.934667</td>\n",
       "      <td>0.946448</td>\n",
       "      <td>0.938448</td>\n",
       "      <td>0.954586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1876' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 06:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlnet_trainer,xlnet_res=train_model(xlnet_tokenizer,xlnet_model,xlnet_train_df,xlnet_val_df,learning_rate=lr,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0740ef17-b524-4939-9d5c-250ae13a2100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3612889349460602,\n",
       " 'eval_accuracy': 0.9344,\n",
       " 'eval_f1': 0.9462882096069869,\n",
       " 'eval_precision': 0.9372837370242214,\n",
       " 'eval_recall': 0.9554673721340388,\n",
       " 'eval_runtime': 103.2542,\n",
       " 'eval_samples_per_second': 36.318,\n",
       " 'eval_steps_per_second': 9.084,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlnet_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e3915118-7f6f-4f90-9da1-82606fd84d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test xlnst\n",
    "\n",
    "test_df['XLNET Tokenizer']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[0])\n",
    "test_df['XLNET Attention_Mask']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[1])\n",
    "\n",
    "\n",
    "xlnet_test_df=HuggingFaceDataset(test_df['XLNET Tokenizer'],test_df['XLNET Attention_Mask'],test_df['Class'])\n",
    "\n",
    "xlnet_test_res=xlnet_trainer.evaluate(xlnet_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a2f29505-3bed-4e9a-8645-9f0ce7a57e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict['Model'].append(\"XLNET\")\n",
    "acc_dict['Accuracy'].append(xlnet_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8058b7-f0d7-4bdc-9d24-54e5382efecc",
   "metadata": {},
   "source": [
    "#### ALBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2bada7b1-50ba-4877-a284-0781d21b4473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "albert_tokenizer=AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "#tokenize train df\n",
    "train_df['Albert Tokenizer']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[0])\n",
    "train_df['Albert Attention_Mask']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_df['Albert Tokenizer']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[0])\n",
    "val_df['Albert Attention_Mask']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[1])\n",
    "\n",
    "\n",
    "#create roberta model\n",
    "albert_model=create_model('albert-base-v2',num_labels=2)\n",
    "\n",
    "\n",
    "#create the class of our dataset\n",
    "albert_train_df=HuggingFaceDataset(train_df['Albert Tokenizer'],train_df['Albert Attention_Mask'],train_df['Class'])\n",
    "albert_val_df=HuggingFaceDataset(val_df['Albert Tokenizer'],val_df['Albert Attention_Mask'],val_df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa04480f-8407-42e3-b027-a0bf242670e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "      <th>Bert Tokenizer</th>\n",
       "      <th>Bert Attention_Mask</th>\n",
       "      <th>Roberta Tokenizer</th>\n",
       "      <th>Roberta Attention_Mask</th>\n",
       "      <th>XLNet Tokenizer</th>\n",
       "      <th>XLNet Attention_Mask</th>\n",
       "      <th>Albert Tokenizer</th>\n",
       "      <th>Albert Attention_Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it looks great when you install it but it wont...</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, great, install, wont, last, long, woul...</td>\n",
       "      <td>[[0.024047852, 0.31445312, -0.026245117, 0.071...</td>\n",
       "      <td>[tensor(101), tensor(2009), tensor(3504), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(405), tensor(1326), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(32), tensor(1879), tensor(3...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another great transitional and educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, great, transitional, educational, do...</td>\n",
       "      <td>[[0.19433594, -0.01965332, 0.091796875, 0.1044...</td>\n",
       "      <td>[tensor(101), tensor(2023), tensor(2003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(9226), tensor(16), tensor(2...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(48), tensor(25), tensor(226...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just when you think oh great another zombie mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, oh, great, another, zombie, movie, fid...</td>\n",
       "      <td>[[-0.046875, 0.06689453, 0.009338379, 0.263671...</td>\n",
       "      <td>[tensor(101), tensor(2074), tensor(2043), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(8987), tensor(77), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(114), tensor(76), tensor(42...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very goodthis kept me wanting more could not w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[goodthis, kept, wanting, could, wait, see, ha...</td>\n",
       "      <td>[[0.048828125, 0.084472656, -0.029052734, 0.07...</td>\n",
       "      <td>[tensor(101), tensor(2200), tensor(2204), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(5525), tensor(205), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(253), tensor(254), tensor(1...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love this series i just cant get enough the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, series, cant, get, enough, people, acti...</td>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "      <td>[tensor(101), tensor(1045), tensor(2293), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(118), tensor(657), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(31), tensor(339), tensor(48...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  it looks great when you install it but it wont...      0   \n",
       "1  this is another great transitional and educati...      1   \n",
       "2  just when you think oh great another zombie mo...      1   \n",
       "3  very goodthis kept me wanting more could not w...      0   \n",
       "4  i love this series i just cant get enough the ...      1   \n",
       "\n",
       "                                      Tokenized Text  \\\n",
       "0  [looks, great, install, wont, last, long, woul...   \n",
       "1  [another, great, transitional, educational, do...   \n",
       "2  [think, oh, great, another, zombie, movie, fid...   \n",
       "3  [goodthis, kept, wanting, could, wait, see, ha...   \n",
       "4  [love, series, cant, get, enough, people, acti...   \n",
       "\n",
       "                        Vectorized Tokens Pretrained  \\\n",
       "0  [[0.024047852, 0.31445312, -0.026245117, 0.071...   \n",
       "1  [[0.19433594, -0.01965332, 0.091796875, 0.1044...   \n",
       "2  [[-0.046875, 0.06689453, 0.009338379, 0.263671...   \n",
       "3  [[0.048828125, 0.084472656, -0.029052734, 0.07...   \n",
       "4  [[0.103027344, -0.15234375, 0.025878906, 0.165...   \n",
       "\n",
       "                                      Bert Tokenizer  \\\n",
       "0  [tensor(101), tensor(2009), tensor(3504), tens...   \n",
       "1  [tensor(101), tensor(2023), tensor(2003), tens...   \n",
       "2  [tensor(101), tensor(2074), tensor(2043), tens...   \n",
       "3  [tensor(101), tensor(2200), tensor(2204), tens...   \n",
       "4  [tensor(101), tensor(1045), tensor(2293), tens...   \n",
       "\n",
       "                                 Bert Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                   Roberta Tokenizer  \\\n",
       "0  [tensor(0), tensor(405), tensor(1326), tensor(...   \n",
       "1  [tensor(0), tensor(9226), tensor(16), tensor(2...   \n",
       "2  [tensor(0), tensor(8987), tensor(77), tensor(4...   \n",
       "3  [tensor(0), tensor(5525), tensor(205), tensor(...   \n",
       "4  [tensor(0), tensor(118), tensor(657), tensor(4...   \n",
       "\n",
       "                              Roberta Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                     XLNet Tokenizer  \\\n",
       "0  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "1  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "2  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "3  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "4  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "\n",
       "                                XLNet Attention_Mask  \\\n",
       "0  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "1  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "2  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "3  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "4  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "\n",
       "                                    Albert Tokenizer  \\\n",
       "0  [tensor(2), tensor(32), tensor(1879), tensor(3...   \n",
       "1  [tensor(2), tensor(48), tensor(25), tensor(226...   \n",
       "2  [tensor(2), tensor(114), tensor(76), tensor(42...   \n",
       "3  [tensor(2), tensor(253), tensor(254), tensor(1...   \n",
       "4  [tensor(2), tensor(31), tensor(339), tensor(48...   \n",
       "\n",
       "                               Albert Attention_Mask  \n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "237f49ee-d098-402d-a1e6-42c98d11f5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n",
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9104/4213672134.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43750' max='43750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43750/43750 1:00:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.445200</td>\n",
       "      <td>0.370251</td>\n",
       "      <td>0.927733</td>\n",
       "      <td>0.941278</td>\n",
       "      <td>0.925437</td>\n",
       "      <td>0.957672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312100</td>\n",
       "      <td>0.425010</td>\n",
       "      <td>0.924267</td>\n",
       "      <td>0.936776</td>\n",
       "      <td>0.946043</td>\n",
       "      <td>0.927690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.210800</td>\n",
       "      <td>0.400764</td>\n",
       "      <td>0.926933</td>\n",
       "      <td>0.939327</td>\n",
       "      <td>0.943505</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.525242</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>0.941019</td>\n",
       "      <td>0.953804</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.565172</td>\n",
       "      <td>0.930667</td>\n",
       "      <td>0.942605</td>\n",
       "      <td>0.943855</td>\n",
       "      <td>0.941358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.669188</td>\n",
       "      <td>0.927733</td>\n",
       "      <td>0.940348</td>\n",
       "      <td>0.938901</td>\n",
       "      <td>0.941799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.713279</td>\n",
       "      <td>0.926933</td>\n",
       "      <td>0.939273</td>\n",
       "      <td>0.944296</td>\n",
       "      <td>0.934303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.834169</td>\n",
       "      <td>0.926133</td>\n",
       "      <td>0.939081</td>\n",
       "      <td>0.936814</td>\n",
       "      <td>0.941358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.832910</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.940536</td>\n",
       "      <td>0.929402</td>\n",
       "      <td>0.951940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.822708</td>\n",
       "      <td>0.927733</td>\n",
       "      <td>0.940479</td>\n",
       "      <td>0.936980</td>\n",
       "      <td>0.944004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2814' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 1:05:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train the model\n",
    "albert_outputs=train_model(albert_tokenizer,albert_model,albert_train_df,albert_val_df,lr,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9512fdcd-e147-44fd-a121-92966b793f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_trainer=albert_outputs[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6122557-c077-41c3-8bcc-d64b5d856052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test albert\n",
    "\n",
    "test_df['Albert Tokenizer']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[0])\n",
    "test_df['Albert Attention_Mask']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[1])\n",
    "\n",
    "\n",
    "albert_test_df=HuggingFaceDataset(test_df['Albert Tokenizer'],test_df['Albert Attention_Mask'],test_df['Class'])\n",
    "\n",
    "albert_test_res=albert_trainer.evaluate(albert_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "565837b2-5c88-423e-ba1a-e7569619aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict['Model'].append(\"Albert\")\n",
    "acc_dict['Accuracy'].append(albert_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f91a56-5805-4221-a560-73b8bf101ba7",
   "metadata": {},
   "source": [
    "#### DistillBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fcbbe748-b406-45e8-bd1a-b2cc475dd671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "distilbert_tokenizer=AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "\n",
    "#tokenize traindf\n",
    "train_df['DistilBert Tokenizer']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[0])\n",
    "train_df['DistilBert Attention_Mask']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[1])\n",
    "\n",
    "#tokenize valdf\n",
    "val_df['DistilBert Tokenizer']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[0])\n",
    "val_df['DistilBert Attention_Mask']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[1])\n",
    "\n",
    "\n",
    "\n",
    "#create model\n",
    "distilbert_model=create_model('distilbert/distilbert-base-uncased',num_labels=2)\n",
    "\n",
    "#create instance from data classes\n",
    "distilbert_train_df=HuggingFaceDataset(train_df['DistilBert Tokenizer'],train_df['DistilBert Attention_Mask'],train_df['Class'])\n",
    "distilbert_val_df=HuggingFaceDataset(val_df['DistilBert Tokenizer'],val_df['DistilBert Attention_Mask'],val_df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3f91c2fa-a289-4e68-83de-81ac27a0e3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "      <th>Bert Tokenizer</th>\n",
       "      <th>Bert Attention_Mask</th>\n",
       "      <th>Roberta Tokenizer</th>\n",
       "      <th>Roberta Attention_Mask</th>\n",
       "      <th>XLNet Tokenizer</th>\n",
       "      <th>XLNet Attention_Mask</th>\n",
       "      <th>Albert Tokenizer</th>\n",
       "      <th>Albert Attention_Mask</th>\n",
       "      <th>DistilBert Tokenizer</th>\n",
       "      <th>DistilBert Attention_Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it looks great when you install it but it wont...</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, great, install, wont, last, long, woul...</td>\n",
       "      <td>[[0.024047852, 0.31445312, -0.026245117, 0.071...</td>\n",
       "      <td>[tensor(101), tensor(2009), tensor(3504), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(405), tensor(1326), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(32), tensor(1879), tensor(3...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2009), tensor(3504), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another great transitional and educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, great, transitional, educational, do...</td>\n",
       "      <td>[[0.19433594, -0.01965332, 0.091796875, 0.1044...</td>\n",
       "      <td>[tensor(101), tensor(2023), tensor(2003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(9226), tensor(16), tensor(2...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(48), tensor(25), tensor(226...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2023), tensor(2003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just when you think oh great another zombie mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, oh, great, another, zombie, movie, fid...</td>\n",
       "      <td>[[-0.046875, 0.06689453, 0.009338379, 0.263671...</td>\n",
       "      <td>[tensor(101), tensor(2074), tensor(2043), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(8987), tensor(77), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(114), tensor(76), tensor(42...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2074), tensor(2043), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very goodthis kept me wanting more could not w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[goodthis, kept, wanting, could, wait, see, ha...</td>\n",
       "      <td>[[0.048828125, 0.084472656, -0.029052734, 0.07...</td>\n",
       "      <td>[tensor(101), tensor(2200), tensor(2204), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(5525), tensor(205), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(253), tensor(254), tensor(1...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2200), tensor(2204), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love this series i just cant get enough the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, series, cant, get, enough, people, acti...</td>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "      <td>[tensor(101), tensor(1045), tensor(2293), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(118), tensor(657), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(31), tensor(339), tensor(48...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(1045), tensor(2293), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  it looks great when you install it but it wont...      0   \n",
       "1  this is another great transitional and educati...      1   \n",
       "2  just when you think oh great another zombie mo...      1   \n",
       "3  very goodthis kept me wanting more could not w...      0   \n",
       "4  i love this series i just cant get enough the ...      1   \n",
       "\n",
       "                                      Tokenized Text  \\\n",
       "0  [looks, great, install, wont, last, long, woul...   \n",
       "1  [another, great, transitional, educational, do...   \n",
       "2  [think, oh, great, another, zombie, movie, fid...   \n",
       "3  [goodthis, kept, wanting, could, wait, see, ha...   \n",
       "4  [love, series, cant, get, enough, people, acti...   \n",
       "\n",
       "                        Vectorized Tokens Pretrained  \\\n",
       "0  [[0.024047852, 0.31445312, -0.026245117, 0.071...   \n",
       "1  [[0.19433594, -0.01965332, 0.091796875, 0.1044...   \n",
       "2  [[-0.046875, 0.06689453, 0.009338379, 0.263671...   \n",
       "3  [[0.048828125, 0.084472656, -0.029052734, 0.07...   \n",
       "4  [[0.103027344, -0.15234375, 0.025878906, 0.165...   \n",
       "\n",
       "                                      Bert Tokenizer  \\\n",
       "0  [tensor(101), tensor(2009), tensor(3504), tens...   \n",
       "1  [tensor(101), tensor(2023), tensor(2003), tens...   \n",
       "2  [tensor(101), tensor(2074), tensor(2043), tens...   \n",
       "3  [tensor(101), tensor(2200), tensor(2204), tens...   \n",
       "4  [tensor(101), tensor(1045), tensor(2293), tens...   \n",
       "\n",
       "                                 Bert Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                   Roberta Tokenizer  \\\n",
       "0  [tensor(0), tensor(405), tensor(1326), tensor(...   \n",
       "1  [tensor(0), tensor(9226), tensor(16), tensor(2...   \n",
       "2  [tensor(0), tensor(8987), tensor(77), tensor(4...   \n",
       "3  [tensor(0), tensor(5525), tensor(205), tensor(...   \n",
       "4  [tensor(0), tensor(118), tensor(657), tensor(4...   \n",
       "\n",
       "                              Roberta Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                     XLNet Tokenizer  \\\n",
       "0  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "1  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "2  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "3  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "4  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "\n",
       "                                XLNet Attention_Mask  \\\n",
       "0  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "1  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "2  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "3  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "4  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "\n",
       "                                    Albert Tokenizer  \\\n",
       "0  [tensor(2), tensor(32), tensor(1879), tensor(3...   \n",
       "1  [tensor(2), tensor(48), tensor(25), tensor(226...   \n",
       "2  [tensor(2), tensor(114), tensor(76), tensor(42...   \n",
       "3  [tensor(2), tensor(253), tensor(254), tensor(1...   \n",
       "4  [tensor(2), tensor(31), tensor(339), tensor(48...   \n",
       "\n",
       "                               Albert Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                DistilBert Tokenizer  \\\n",
       "0  [tensor(101), tensor(2009), tensor(3504), tens...   \n",
       "1  [tensor(101), tensor(2023), tensor(2003), tens...   \n",
       "2  [tensor(101), tensor(2074), tensor(2043), tens...   \n",
       "3  [tensor(101), tensor(2200), tensor(2204), tens...   \n",
       "4  [tensor(101), tensor(1045), tensor(2293), tens...   \n",
       "\n",
       "                           DistilBert Attention_Mask  \n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5b4135ab-e722-4c26-97ae-18f1b12959e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n",
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9104/4213672134.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43750' max='43750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43750/43750 42:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.323119</td>\n",
       "      <td>0.918933</td>\n",
       "      <td>0.933884</td>\n",
       "      <td>0.921459</td>\n",
       "      <td>0.946649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>0.366971</td>\n",
       "      <td>0.923200</td>\n",
       "      <td>0.935972</td>\n",
       "      <td>0.943946</td>\n",
       "      <td>0.928131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.424087</td>\n",
       "      <td>0.920533</td>\n",
       "      <td>0.934361</td>\n",
       "      <td>0.933539</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.630108</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.924541</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.899471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.557188</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>0.936670</td>\n",
       "      <td>0.924796</td>\n",
       "      <td>0.948854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.685152</td>\n",
       "      <td>0.921600</td>\n",
       "      <td>0.935864</td>\n",
       "      <td>0.926166</td>\n",
       "      <td>0.945767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.774625</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.930109</td>\n",
       "      <td>0.936132</td>\n",
       "      <td>0.924162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.838809</td>\n",
       "      <td>0.917067</td>\n",
       "      <td>0.931422</td>\n",
       "      <td>0.931628</td>\n",
       "      <td>0.931217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.920267</td>\n",
       "      <td>0.934213</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>0.936067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.905294</td>\n",
       "      <td>0.921333</td>\n",
       "      <td>0.935008</td>\n",
       "      <td>0.934390</td>\n",
       "      <td>0.935626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1876' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 16:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#strat training\n",
    "distilbert_outputs=train_model(distilbert_tokenizer,distilbert_model,distilbert_train_df,distilbert_val_df,learning_rate=lr,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1ae7cea0-20dd-491b-bd0b-c7a846117066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ad84d527-a2f8-4439-b2f6-2c4d2567126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "\n",
    "distilbert_trainer=distilbert_outputs[0]\n",
    "\n",
    "test_df['DistilBert Tokenizer']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[0])\n",
    "test_df['DistilBert Attention_Mask']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[1])\n",
    "\n",
    "\n",
    "distilbert_test_df=HuggingFaceDataset(test_df['DistilBert Tokenizer'],test_df['DistilBert Attention_Mask'],test_df['Class'])\n",
    "\n",
    "distilbert_test_res=distilbert_trainer.evaluate(distilbert_test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "364a6dbe-d486-4000-b1f9-ef08deca9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict['Model'].append(\"DistilBert\")\n",
    "acc_dict['Accuracy'].append(distilbert_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434db06b-8aeb-4dde-9281-9af6a0e4e219",
   "metadata": {},
   "source": [
    "#### Electra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca90a60-7bbe-4665-9bfa-be3ed6d8fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#tokenizer\n",
    "electra_tokenizer=AutoTokenizer.from_pretrained('google/electra-base-discriminator')\n",
    "\n",
    "#tokenize train df\n",
    "train_df['Electra Tokenizer']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,electra_tokenizer)[0])\n",
    "train_df['Electra Attention_Mask']=train_df['Sentence'].apply(lambda x: tokenize_using_hf(x,electra_tokenizer)[1])\n",
    "\n",
    "\n",
    "#tokeize val df\n",
    "val_df['Electra Tokenizer']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,electra_tokenizer)[0])\n",
    "val_df['Electra Attention_Mask']=val_df['Sentence'].apply(lambda x: tokenize_using_hf(x,electra_tokenizer)[1])   \n",
    "\n",
    "\n",
    "#create model\n",
    "electra_model=create_model('google/electra-base-discriminator',num_labels=2)\n",
    "\n",
    "#create instances of dataclass\n",
    "electra_train_df=HuggingFaceDataset(train_df['Electra Tokenizer'],train_df['Electra Attention_Mask'],train_df['Class'])\n",
    "electra_val_df=HuggingFaceDataset(val_df['Electra Tokenizer'],val_df['Electra Attention_Mask'],val_df['Class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8cf8b453-c052-48e3-a3b7-490fe58e18bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "      <th>Bert Tokenizer</th>\n",
       "      <th>Bert Attention_Mask</th>\n",
       "      <th>Roberta Tokenizer</th>\n",
       "      <th>Roberta Attention_Mask</th>\n",
       "      <th>XLNet Tokenizer</th>\n",
       "      <th>XLNet Attention_Mask</th>\n",
       "      <th>Albert Tokenizer</th>\n",
       "      <th>Albert Attention_Mask</th>\n",
       "      <th>DistilBert Tokenizer</th>\n",
       "      <th>DistilBert Attention_Mask</th>\n",
       "      <th>Electra Tokenizer</th>\n",
       "      <th>Electra Attention_Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it looks great when you install it but it wont...</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, great, install, wont, last, long, woul...</td>\n",
       "      <td>[[0.024047852, 0.31445312, -0.026245117, 0.071...</td>\n",
       "      <td>[tensor(101), tensor(2009), tensor(3504), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(405), tensor(1326), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(32), tensor(1879), tensor(3...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2009), tensor(3504), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2009), tensor(3504), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is another great transitional and educati...</td>\n",
       "      <td>1</td>\n",
       "      <td>[another, great, transitional, educational, do...</td>\n",
       "      <td>[[0.19433594, -0.01965332, 0.091796875, 0.1044...</td>\n",
       "      <td>[tensor(101), tensor(2023), tensor(2003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(9226), tensor(16), tensor(2...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(48), tensor(25), tensor(226...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2023), tensor(2003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2023), tensor(2003), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just when you think oh great another zombie mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[think, oh, great, another, zombie, movie, fid...</td>\n",
       "      <td>[[-0.046875, 0.06689453, 0.009338379, 0.263671...</td>\n",
       "      <td>[tensor(101), tensor(2074), tensor(2043), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(8987), tensor(77), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(114), tensor(76), tensor(42...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2074), tensor(2043), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2074), tensor(2043), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>very goodthis kept me wanting more could not w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[goodthis, kept, wanting, could, wait, see, ha...</td>\n",
       "      <td>[[0.048828125, 0.084472656, -0.029052734, 0.07...</td>\n",
       "      <td>[tensor(101), tensor(2200), tensor(2204), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(5525), tensor(205), tensor(...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(253), tensor(254), tensor(1...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2200), tensor(2204), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(2200), tensor(2204), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love this series i just cant get enough the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[love, series, cant, get, enough, people, acti...</td>\n",
       "      <td>[[0.103027344, -0.15234375, 0.025878906, 0.165...</td>\n",
       "      <td>[tensor(101), tensor(1045), tensor(2293), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(0), tensor(118), tensor(657), tensor(4...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(5), tensor(5), tensor(5), tensor(5), t...</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>[tensor(2), tensor(31), tensor(339), tensor(48...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(1045), tensor(2293), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "      <td>[tensor(101), tensor(1045), tensor(2293), tens...</td>\n",
       "      <td>[tensor(1), tensor(1), tensor(1), tensor(1), t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  it looks great when you install it but it wont...      0   \n",
       "1  this is another great transitional and educati...      1   \n",
       "2  just when you think oh great another zombie mo...      1   \n",
       "3  very goodthis kept me wanting more could not w...      0   \n",
       "4  i love this series i just cant get enough the ...      1   \n",
       "\n",
       "                                      Tokenized Text  \\\n",
       "0  [looks, great, install, wont, last, long, woul...   \n",
       "1  [another, great, transitional, educational, do...   \n",
       "2  [think, oh, great, another, zombie, movie, fid...   \n",
       "3  [goodthis, kept, wanting, could, wait, see, ha...   \n",
       "4  [love, series, cant, get, enough, people, acti...   \n",
       "\n",
       "                        Vectorized Tokens Pretrained  \\\n",
       "0  [[0.024047852, 0.31445312, -0.026245117, 0.071...   \n",
       "1  [[0.19433594, -0.01965332, 0.091796875, 0.1044...   \n",
       "2  [[-0.046875, 0.06689453, 0.009338379, 0.263671...   \n",
       "3  [[0.048828125, 0.084472656, -0.029052734, 0.07...   \n",
       "4  [[0.103027344, -0.15234375, 0.025878906, 0.165...   \n",
       "\n",
       "                                      Bert Tokenizer  \\\n",
       "0  [tensor(101), tensor(2009), tensor(3504), tens...   \n",
       "1  [tensor(101), tensor(2023), tensor(2003), tens...   \n",
       "2  [tensor(101), tensor(2074), tensor(2043), tens...   \n",
       "3  [tensor(101), tensor(2200), tensor(2204), tens...   \n",
       "4  [tensor(101), tensor(1045), tensor(2293), tens...   \n",
       "\n",
       "                                 Bert Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                   Roberta Tokenizer  \\\n",
       "0  [tensor(0), tensor(405), tensor(1326), tensor(...   \n",
       "1  [tensor(0), tensor(9226), tensor(16), tensor(2...   \n",
       "2  [tensor(0), tensor(8987), tensor(77), tensor(4...   \n",
       "3  [tensor(0), tensor(5525), tensor(205), tensor(...   \n",
       "4  [tensor(0), tensor(118), tensor(657), tensor(4...   \n",
       "\n",
       "                              Roberta Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                     XLNet Tokenizer  \\\n",
       "0  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "1  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "2  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "3  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "4  [tensor(5), tensor(5), tensor(5), tensor(5), t...   \n",
       "\n",
       "                                XLNet Attention_Mask  \\\n",
       "0  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "1  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "2  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "3  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "4  [tensor(0), tensor(0), tensor(0), tensor(0), t...   \n",
       "\n",
       "                                    Albert Tokenizer  \\\n",
       "0  [tensor(2), tensor(32), tensor(1879), tensor(3...   \n",
       "1  [tensor(2), tensor(48), tensor(25), tensor(226...   \n",
       "2  [tensor(2), tensor(114), tensor(76), tensor(42...   \n",
       "3  [tensor(2), tensor(253), tensor(254), tensor(1...   \n",
       "4  [tensor(2), tensor(31), tensor(339), tensor(48...   \n",
       "\n",
       "                               Albert Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                DistilBert Tokenizer  \\\n",
       "0  [tensor(101), tensor(2009), tensor(3504), tens...   \n",
       "1  [tensor(101), tensor(2023), tensor(2003), tens...   \n",
       "2  [tensor(101), tensor(2074), tensor(2043), tens...   \n",
       "3  [tensor(101), tensor(2200), tensor(2204), tens...   \n",
       "4  [tensor(101), tensor(1045), tensor(2293), tens...   \n",
       "\n",
       "                           DistilBert Attention_Mask  \\\n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...   \n",
       "\n",
       "                                   Electra Tokenizer  \\\n",
       "0  [tensor(101), tensor(2009), tensor(3504), tens...   \n",
       "1  [tensor(101), tensor(2023), tensor(2003), tens...   \n",
       "2  [tensor(101), tensor(2074), tensor(2043), tens...   \n",
       "3  [tensor(101), tensor(2200), tensor(2204), tens...   \n",
       "4  [tensor(101), tensor(1045), tensor(2293), tens...   \n",
       "\n",
       "                              Electra Attention_Mask  \n",
       "0  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "1  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "2  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "3  [tensor(1), tensor(1), tensor(1), tensor(1), t...  \n",
       "4  [tensor(1), tensor(1), tensor(1), tensor(1), t...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4895124c-9fa3-4bf8-aa9f-6945caacba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12002/4213672134.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvenky_ai\u001b[0m (\u001b[33mvenky_ai-lule-university-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250418_171013-9m75s0vy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/huggingface/runs/9m75s0vy' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/huggingface' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/huggingface/runs/9m75s0vy' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/huggingface/runs/9m75s0vy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43750' max='43750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43750/43750 1:13:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.301700</td>\n",
       "      <td>0.247005</td>\n",
       "      <td>0.942933</td>\n",
       "      <td>0.952946</td>\n",
       "      <td>0.950439</td>\n",
       "      <td>0.955467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>0.329193</td>\n",
       "      <td>0.941600</td>\n",
       "      <td>0.952298</td>\n",
       "      <td>0.941025</td>\n",
       "      <td>0.963845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.372312</td>\n",
       "      <td>0.942133</td>\n",
       "      <td>0.952444</td>\n",
       "      <td>0.946841</td>\n",
       "      <td>0.958113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.469762</td>\n",
       "      <td>0.940267</td>\n",
       "      <td>0.951028</td>\n",
       "      <td>0.943192</td>\n",
       "      <td>0.958995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.489358</td>\n",
       "      <td>0.938400</td>\n",
       "      <td>0.949968</td>\n",
       "      <td>0.933589</td>\n",
       "      <td>0.966931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.542041</td>\n",
       "      <td>0.941333</td>\n",
       "      <td>0.951349</td>\n",
       "      <td>0.954303</td>\n",
       "      <td>0.948413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.611622</td>\n",
       "      <td>0.942400</td>\n",
       "      <td>0.952234</td>\n",
       "      <td>0.955191</td>\n",
       "      <td>0.949295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.606037</td>\n",
       "      <td>0.942133</td>\n",
       "      <td>0.952108</td>\n",
       "      <td>0.953160</td>\n",
       "      <td>0.951058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.646769</td>\n",
       "      <td>0.942933</td>\n",
       "      <td>0.952822</td>\n",
       "      <td>0.952822</td>\n",
       "      <td>0.952822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.663447</td>\n",
       "      <td>0.942133</td>\n",
       "      <td>0.952297</td>\n",
       "      <td>0.949584</td>\n",
       "      <td>0.955026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2814' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 31:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train model\n",
    "electra_trainer,electra_res=train_model(electra_tokenizer,electra_model,electra_train_df,electra_val_df,learning_rate=lr,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56128fdb-924f-44b0-959d-2d476e0a78f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24700497090816498,\n",
       " 'eval_accuracy': 0.9429333333333333,\n",
       " 'eval_f1': 0.9529463500439753,\n",
       " 'eval_precision': 0.9504385964912281,\n",
       " 'eval_recall': 0.9554673721340388,\n",
       " 'eval_runtime': 22.9049,\n",
       " 'eval_samples_per_second': 163.72,\n",
       " 'eval_steps_per_second': 40.952,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baadf369-2f76-455a-8900-83c4debe1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "test_df['Electra Tokenizer']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,electra_tokenizer)[0])\n",
    "test_df['Electra Attention_Mask']=test_df['Sentence'].apply(lambda x: tokenize_using_hf(x,electra_tokenizer)[1])\n",
    "\n",
    "\n",
    "electra_test_df=HuggingFaceDataset(test_df['Electra Tokenizer'],test_df['Electra Attention_Mask'],test_df['Class'])\n",
    "\n",
    "electra_test_res=electra_trainer.evaluate(electra_test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bca172b8-5bd5-49ef-90b0-eef0e48939cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict[\"Model\"].append(\"Electra\")\n",
    "acc_dict['Accuracy'].append(electra_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd362e4-6274-415f-a5f2-99331237fadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Bert', 'Roberta', 'XLNET', 'Albert', 'DistilBert', 'Electra'],\n",
       " 'Accuracy': [0.9162666666666667,\n",
       "  0.9362666666666667,\n",
       "  0.932,\n",
       "  0.9112,\n",
       "  0.916,\n",
       "  0.9429333333333333]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac31fc3c-259e-450b-b41d-810f247d1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict['Model'].extend([\"RNN\",\"LSTM\",\"GRU\"])\n",
    "acc_dict['Accuracy'].extend([0.60,0.60,0.83])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a879914-dfbe-4269-ae5f-fd77946529e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df=pd.DataFrame(acc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec6cca6-bed3-4379-ac0d-56ecd9e4129b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bert</td>\n",
       "      <td>0.916267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roberta</td>\n",
       "      <td>0.936267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XLNET</td>\n",
       "      <td>0.932000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albert</td>\n",
       "      <td>0.911200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DistilBert</td>\n",
       "      <td>0.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Electra</td>\n",
       "      <td>0.942933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GRU</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Accuracy\n",
       "0        Bert  0.916267\n",
       "1     Roberta  0.936267\n",
       "2       XLNET  0.932000\n",
       "3      Albert  0.911200\n",
       "4  DistilBert  0.916000\n",
       "5     Electra  0.942933\n",
       "6         RNN  0.600000\n",
       "7        LSTM  0.600000\n",
       "8         GRU  0.830000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250037fb-a99e-4840-9711-34827463a025",
   "metadata": {},
   "source": [
    "#### Plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627b15b-caf3-42d6-9a74-ca134f4b0bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9566d172-d10f-44ab-ab5f-a01f89d19088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Model', ylabel='Accuracy'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.barplot(data=acc_df, x='Model', y='Accuracy', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525935e-ddc7-4fab-bb0a-f6c253ab23b1",
   "metadata": {},
   "source": [
    "## New Dataset- Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "287123d7-8ab2-4f5f-9d6f-e97111681346",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=load_dataset('yassiracharki/Amazon_Reviews_for_Sentiment_Analysis_fine_grained_5_classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28248e80-890b-4dce-9ab9-a010d0024e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class_index', 'review_title', 'review_text'],\n",
       "        num_rows: 3000000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['class_index', 'review_title', 'review_text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62f74676-a4c0-4817-958f-925ed8358308",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_val=reviews['train']\n",
    "reviews_test=reviews['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6b7f858-acef-47fb-a428-ff0eaa157ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['class_index', 'review_title', 'review_text'],\n",
       "    num_rows: 650000\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4d4c310-70ab-4c2d-b4b1-a053c7d0e95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['class_index', 'review_title', 'review_text'],\n",
       "    num_rows: 3000000\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dd76ec2-e375-4595-bee7-4e620050e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train val \n",
    "reviews_train_val=reviews_train_val.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9a10044-ded5-4522-b7a7-a128ab7deb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['class_index', 'review_title', 'review_text'],\n",
       "        num_rows: 2400000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['class_index', 'review_title', 'review_text'],\n",
       "        num_rows: 600000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87781d02-5946-4e74-ab3a-783e4c1d034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train=reviews_train_val['train']\n",
    "reviews_val=reviews_train_val['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6e681cc-5d2b-4481-806e-5d7f74d467b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['class_index', 'review_title', 'review_text'],\n",
       "    num_rows: 2400000\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b605cce3-7547-4814-a053-43bb2637d63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['class_index', 'review_title', 'review_text'],\n",
       "    num_rows: 600000\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac060de5-5e1c-443a-8d8a-f8c3f5ec0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to tokenize \n",
    "def change_to_string(data):\n",
    "    data['review_title']=[str(x) if x is not None else \"No Title\" for x in data['review_title']]\n",
    "    data['review_text']=[str(x) if x is not None else \"No Review\" for x in data['review_text']]\n",
    "\n",
    "    return data\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def tokenize(data,tokenizer):\n",
    "    return tokenizer(data['review_title'],\n",
    "                     data['review_text'],\n",
    "                     truncation=True,\n",
    "                     padding='max_length',\n",
    "                     max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12e5b674-cc8e-4e54-bdad-576655d6f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_bert_tokenizer=AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbd1ee77-0ad6-4f86-a0e2-9bc74439a7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9aae22e1-62fe-4305-beb2-caea86677bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400000/2400000 [00:38<00:00, 62686.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "reviews_train=reviews_train.map(change_to_string,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0735ceac-a60e-4c1e-8e66-e451b2c59bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600000/600000 [00:10<00:00, 58786.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "reviews_val=reviews_val.map(change_to_string,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "197db460-3273-48b0-b81c-820d7585f3bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "class_index: int64\n",
       "review_title: string\n",
       "review_text: string\n",
       "----\n",
       "class_index: [[3,1,3,5,3,...,3,5,3,2,1],[1,4,4,1,4,...,5,1,2,1,3],...,[3,1,1,5,1,...,4,4,3,3,3],[3,3,1,1,4,...,4,3,3,3,3]]\n",
       "review_title: [[\"20th century Asian history\",\"Get them for free from Bose\",\"A bit flimsy and the dough does stick.\",\"SHolmes Fan\",\"3 out of 5\",...,\"i hate this toy\",\"Great Dual Alarm Clock Radio\",\"Black Notice\",\"Airpot\",\"Great movie, HORRIBLE BLU-RAY DISC\"],[\"Only if you want a disposable one\",\"Something new.\",\"Pretty good\",\"Simply wretched!\",\"Long awaited, well worth it...\",...,\"Wonderful Natural Remedy to ease sinusitis!\",\"They DO NOT LAST!\",\"Not too great\",\"Damaged Goods\",\"Insult to the movie tombstone\"],...,[\"Another one damaged by shipping\",\"Doesn't work with almost ALL computers\",\"Junk!\",\"I Really LIke This This Book\",\"don't waste your time\",...,\"addleman Seat\",\"Close to perfect for these things\",\"Good but gloopy\",\"Helpful? Reference, yes. Teaching/learning, no.\",\"Look Out For Lippy!\"],[\"Bond, Jane Bond.\",\"Something like it, but not really..\",\"Pass on Loopers\",\"One BIG advertisment\",\"The Empire od the Steppes\",...,\"Good but not great\",\"Buyer Beware\",\"smelling good.\",\"not toooooooo bad\",\"good glass... lots of plastic\"]]\n",
       "review_text: [[\"This was truly eye-opening b/c of the lack of awareness that this event ever occured. That is the point and the double-tragedy of what happened. Much like the Nazi atrocities of WW2, it is hard to believe that man could act in such a way to others. The book is informative yet somewhat repetitive. A couple of times I swear I was reading word for word what I had already read. Otherwise, it was very interesting.\",\"What a rip off. I went to the local Bose store and they gave these to me for free. $4.99 a piece, WOW!\",\"While this does cut nice round doughnuts, the steel used is rather thin. Thus, it's fairly easy to misshape.It's also slow to work with because I find that the dough sticks to it and I have to push each doughnut out before cutting the next.After paying attention at some doughnut shops, it looks like the pros use a cutter of a different design. Look for the ones by Johnson-Rose and KegWorks.Stainless Steel Donut Cutter\",\"Well written, directed and brillantly acted. An excellent first entry for the series. You won't be disappointed unless you are a 'purist.' Very entertaining.\",\"NOTE: I'm not actually 1. I just can't be bothered to creat an account.Normally I'm a huge Forsyth fan, but this I can't say that this was one of his materpieces (in the tardition of Jackal, Odessa, etc). Having said that, \"The Veteran\" is one of the finest stories I've read in a long time. Ot had me going the entire time. \"The Citizen\" was riviting, yet mildly predictable. But then again, the prediction I made turned out to be wrong. And \"The Art of the Matter\" was great for its detective work. The other two didn't do it for me. Despite winning the Edgar for it, \"Snakes\" let me down - it seemed too easy and rushed. And \"Wind\" was too implauasible for me. He gets cred for trying, but it just didn't pan out. This is a fabulous airplane book - will get you from North America to the UK. Buy it for cheap at the airport, or get it from you local library for free.\",...,\"This zoids is to big for me you sould proboly be an advanced zoid buider. the age should 12-21\",\"I purchased two of these clocks, one for me and one for my son. I know clock radio's are not meant to have the greatest sound but I was impressed with the way it picks up many of my favorite stations with no static at all. It can't be strictly my location because this clock picks up better than any I have ever had.....and this is my first Sony. I love the convenience of the dual alarm and honestly don't think I can ever go back to a single alarm again. I didn't have to read the instructions to operate this clock and the time was right on both clocks when I plugged them in. I gave it five stars because I'm so pleased with the clock but if I could change one thing about it, I would make the beep alarm a little louder. I have absolutely no regrets with my purchase!\",\"I was disappointed in this book. It wasn't nearly as exciting as some of the earlier ones. It's almost as though her style of writing has changed over the series. I hope The Last Precinct will be better, but some of the reviews I've seen makes me doubtful.\",\"This item worked great for a while and then the spring went on it, it did not last long enough to be worth it.\",\"Be very careful when you buy this blu-ray, will not work in some Sony 3D blu-ray players, and some Samsung. There are some work-arounds online, but they don't always work and who wants to do that everytime. It's too bad Fox really dropped the ball on this blu-ray, it is a great movie.\"],[\"It's cute, but crappy. Even when it worked, the interface was confusing and it had no volume. With good care and limited use, it lasted less than two years. It died on me yesterday - the buttons stopped working entirely. I recommend that this model be avoided.\",\"Some people really dont like this album and some people loves it.This is not the kind of \"fun\" album that AA use to produce. Its deeper and more complicated than Dont Censor me and BloOm.Some people say that the lyrics are too simple but they are not!I like Some Kind of Zombie, Mark shows the people that he might be one of the best singers in the world!\",\"This headset is strictly for chatting. You cannot use it for anything else (like hear things in the game). The ear piece cover shows wear early, it's fragile, and the ear piece itself sometimes disconnects, but you can just put it back in. Not meant for people with big heads, the adjustable band has a short limit, and the mouthpiece will wind up in your mouth. The cord is long, which is good. Adjustable hearing volume, with button to turn off speaking if need be. Chatting quality is decent. Functions through USB only, not Bluetooth.\",\"This was the second book I've ever read of this author and it will be the last! The other books where so good. This one was a major let down. To sum up my feelings on this book, it's rather like eating all these wonderful desserts one right behind the other. Then realizing you've eaten way to much, and tossing your cookies as a result. The only thing you're left with, is a bad taste in your mouth!\",\"I just picked Weird Al's new album this morning. On the way back to work I popped the CD into my car CD player and began listening to \"Couch Potato\", the parody of Eminem's \"Lose Yourself\". From that point on I had a smile on my face. This is a great album, fun to listen to. I may be a nerd for picking it up when the store opened this morning, but I'm proud to be associated with such a creative artist.\",...,\"I absolutely agree with all the positive reviews of this product. It's a great simple natural remedy that really works to support breathing functions and relief the symptoms of sinusitis, allergies, asthma, etc. Lots of info on the internet about Fenugreek-Thyme, including some claims that it lowers sugar in the body - helpful for diabetics as well. Thanks to Amazon for making this and other products available at reasonable prices - I am a huge fan! :-)\",\"I've had these pans for 4 years and they all have peeling on the insides. The worst are the skillets! I have never put them in the dishwasher or on high heat. I loved these pans for the first 1-2 years and now they look horrible!! Even the outside has changed so much that they look like there 100 years old. Don't waste your money!!!\",\"Sorry, but this just doesn't do it for me. the first 4 tracks are good, but the rest is bland - just too heavy. I think they took a better approach to their second cd - the loopholes and great excuses EP, and i think they will keep getting better with age.Ans on the special edition of this cd - the live songs aren't worth anything - they sound like they were bootlegged by someone in the crowd with a dodgy tape recorder from the '70s.But if you are like me, you'll probably buy this anyway for your collection.\",\"I usually buy this item at the store and they are fine and clean.These arrived damaged, dirty and full of particles.\",\"This killed it for me.. Tombstone was sooooooo good and then Val Kilmer does this....saddened....lol Nobody can replace his doc holiday...\"],...,[\"As many have stated already, purchasing this is a roll of the dice. You may end up with one that comes as it is supposed to, then again you may end up like me and so many others with a damaged product. The box is not sufficient for UPS and Fedex. It looks like the box is meant to be shipped with like product in bulk.The box came to me with footprints on it and the aluminum rails were bent up pretty bad. More than could have been bent back and still used. I gave it 3 stars as it looks like it will work well once I get one that I can use.Also, there are no replacements for this. You must return it and wait for a refund and try again. If you need this product and cant wait, go to the store and just pay for it. If you are a patient person and want the best deal.....roll the dice.Good luck!\",\"Terrible. Simply doesn't work with almost any computer. Does not work on Macs\",\"Fully charged battery overnight, installed it on my PV-GS320, and...NOTHING.It does not work. Total waste of money.\",\"I had a wonderful time reading this book. It's more of a *long* essay than it is a book. The writing is excellent, and there are lots of wonderful photographs interspersed within. I don't know if it really matters, but for what it's worth I read part of this book while sitting at a table at Birdland on 46th Street in NYC waiting for my favorite up and coming piano player, David Epstein, to return from break to play another set. I have three books by Gary Giddins and like him so much I'm going to buy more of his books. By coinicidence I saw a video recently on the life of Charlie Parker, and it used many of the same photographs that were used in this book. I am not aware of a tie in between the two products however. In conclusion, this book was enjoyable and I learned a great deal about Charlie Parker from the point of view of his personality and his character and what his life was like.\",\"Definitely one of the worst \"horror\" movies I've ever seen. Not scary, not entertaining, not even worth the time to write much more about.\",...,\"Seat is very very comfortable on long trips. I thought the firmness of the seat would be otherwise but I am very pleased.\",\"This product is probably as good as these things get. I think Bluetooth as a technology generally sucks no matter what brand of headset you buy. So far, it has performed better than any I've owned so far. It has better sound quality, resistance to wind noise and is hardier than the two Motorola's I've had. It fits better, too. It's a little too easy to hang up on people when your messing with your hair or whatever, but you'll get used to it and stop doing it after a couple of days. You might thing that it looks a little big and not entirely stylish, but that's a judgment call, right? Personally, I'm happy to have something that's close to working the way that these things are supposed to work.\",\"This stuff tastes good, but it doesn't mix very well. Even with vigorous stirring or shaking, you're usually left with a watery concoction in which chunks of mix either float to the top, or settle at the bottom to leave you with a last doughy mouthful. I like to mix one scoop of this and one scoop of the regular Muscle Milk Cookies n' Cream(which tastes EXCELLENT, by the way; and ends up being only 50 calories more than 2 scoops of Muscle Milk Light) to facilitate mixing. This works moderately well.\",\"I've used this book for two quarters at ohio university in physics 272 and 273. Other than the fact that the lab courses doen't do much in the way of actual teaching, this text and it's sarcasm is awful.It does ask a lot of questions, but as a teaching text for an introductory set of labs, it is terrible. You may think I'm just too stupid to benefit from sarcastic questions with no material in the text to answer them, but if that is the case, you are probably some nerd type that knows very little in the way of education.I'll be selling this set of texts, which is unusual, since as a teacher, I generally keep all my texts for future reference (even my introduction to particle physics by Das, and my quantum mechanics text by Ira Levine)...save money and use google, or wikipedia you will find more detailed answers.\",\"This film was a bit above the standard genre fare being released at the time. I remember going to see it after reading the book and it really freaked me out. Towards the end, when all the \"poor souls from hell\" appear, they actually used some disfigured people (saving money on fx make-up?). There was this man from Coney Island, we used to call him \"Lippy,\" that scared the hell outta me when I was a kid (in the 60's). He was probably a nice man, but I after the 1st time I saw him I had nightmares for months. I did a double take when I saw him in this movie... it was incredible to have one of your childhood fears realized like that... got me again dude (lol). So look for Lippy at the end of the film...ya can't miss him ; )\"],[\"This book was a lot of fun. Very campy, but very much a parody of the whole James Bond franchise. This author is apparently one for parody ~ her other books are a series of mysteries involving a teen sleuth named Nancy Clue.I'm not a fan of mysteries by any definition, but I thoroughly enjoyed this book. It kept me turning the page until I reached the end, and I would definitely read another in this series.\",\"The flow of the piece itself expelled alot of talent, and Im sure took alot of time and energy. Though to completely glorify this album at all, would not be my own doing. Tricky's work has been compared to Massive Attack and Portishead. Maybe MA has a flow like Trickys recent work, but cant be even be compared to an album like Portisheads' \"Dummy\" album. This album, Maxin-something or another, has alot of work to do. Maybe a good remix, or maybe ten of them, would help. I only give it three stars, because I couldnt do what he does, but I think Im just in a polite mood tonight. So if you like a good sensual electronic flow, MA and Portishead will have to do you. Tricky is too wanna-be Deltron3030 mixed with a bad breakbeat/trance album.\",\"The ads for this movie are the best parts. Pass on this movie, very over rated by the reviews here, lacks story line, typical earth at an end crime lords rule. Writing 20 words in order to simply give this one a one star.\",\"This book lulls you into feeling you're reading an informative diet book, but all the sudden it turns into an advertisement for Twin Labs. If your looking for an objective diet book don't buy this one.\",\"An unbelievably well-researched account of how countless hordes of Mongol and Turcic peoples terrorized most of the world with their incessant east-to-west murderous migrations for more than 23 centuries of RECORDED history. Reading this book gives the reader a good understanding of the reasons for the great cultural and political chasm existing today between the West and the Near East/Central Asia.\",...,\"I LOVE Joanie Madden and I think that all of her music is worth listening to but I wasn't as impressed with this CD as much as Song of the Irish Whistle 1 & 2. Those CDs transport me to heaven when I listen!\",\"I have rated this a three for the effort made to make this game available on DVD. The problem isn't the game but the quality. I purchased under the assumption that the game had been copied for NBC's archives. This doesn't appear to be the case. Literally! The quality lends one to believe that this game was drawn from someone's VHS or BETA tape. The tracking lines and other little \"blips\" common with videotape playback seem to support this belief. Don't misunderstand, the game is watchable. It just wasn't the professional production that I thought was being purchased.\",\"I love this lotion a lot. It moisturizes my skin and leaves it smelling good. It's good for all types of skin from normal to extra dry. It's good for me because other lotions break my skin out and this lotion doesn't. You can even use it on a daily basis. You won't regret using it.\",\"Comparing with her previous J.Lo make better things this time. Although first 6 songs sound like one long drip of molasses, the entire production is far better than it used to be with the first two records of hers. However, if you've given up on looking cool, this record is exactly for you. But if you like good music, spend you money on something else. And J.Lo-STOP SINGING!!!!!\",\"so I pick this lens because it was much more easier on my pocket book then the 50 f1.4.... I kinda wish I would have ponyed up the cash and bought that one... but I bought this one... the pics are good... great glass, fast, and great depth of field... but I just cannot get over the plastic... feels like an egg in my hand... I'm afraid I'm going to break it... however for the price it is a fantastic lens... a great buy... just treat it carefuly... also the auto focus seems to hunt around for a bit and is pretty loud... it is an old style motor so what could you expect... all and all it ia an ok lens and lives on my camera most of the time... but when it breaks I'll get the 50 f 1.4... but for the price I could not say no :)\"]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_val.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d66ddc2-6101-419f-9664-4626c5136626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to labels\n",
    "reviews_train=reviews_train.rename_column('class_index','label')\n",
    "reviews_val=reviews_val.rename_column('class_index','label')\n",
    "reviews_test=reviews_test.rename_column('class_index','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12cfca58-43d4-4978-b131-a8c1ecdbfdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review_title', 'review_text'],\n",
       "    num_rows: 600000\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fc52c02-87ef-4cdb-8026-5465b08266cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400000/2400000 [06:10<00:00, 6473.72 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600000/600000 [01:30<00:00, 6658.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dbert_tokenized_train_data=reviews_train.map(partial(tokenize,tokenizer=distilled_bert_tokenizer),batched=True)\n",
    "dbert_tokenized_val_data=reviews_val.map(partial(tokenize,tokenizer=distilled_bert_tokenizer),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4868012a-c76d-4b0c-bade-25e998799427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bba884da-4968-4a8b-b86b-05d3b42545b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in tokenized data: dict_keys(['label', 'review_title', 'review_text', 'input_ids', 'attention_mask'])\n",
      "Input IDs length: 128\n",
      "Attention mask length: 128\n",
      "Token type IDs length: 0\n",
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "# Check the first tokenized example\n",
    "sample = dbert_tokenized_train_data[0]\n",
    "print(\"Keys in tokenized data:\", sample.keys())\n",
    "print(\"Input IDs length:\", len(sample[\"input_ids\"]))\n",
    "print(\"Attention mask length:\", len(sample[\"attention_mask\"]))\n",
    "print(\"Token type IDs length:\", len(sample.get(\"token_type_ids\", [])))  # Only for BERT-like models\n",
    "print(\"Label:\", sample[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "193a9c7a-fa8b-4de9-b6c0-3e7e17a1973a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "label: int64\n",
       "review_title: string\n",
       "review_text: string\n",
       "input_ids: list<item: int32>\n",
       "  child 0, item: int32\n",
       "attention_mask: list<item: int8>\n",
       "  child 0, item: int8\n",
       "----\n",
       "label: [[5,2,3,1,3,...,2,3,4,3,3],[3,4,5,3,5,...,3,5,1,2,4],...,[3,4,5,3,3,...,1,4,5,4,5],[1,1,2,2,4,...,5,5,2,4,5]]\n",
       "review_title: [[\"excellent product\",\"This is a cheap keyboard and its made that way\",\"Not as expected.\",\"This book is a definite New-Age point of view.\",\"Visitations\",...,\"Didn't do anything for me.\",\"Sometimes they work and sometimes they don't\",\"NOT THE FIRST, NOT THE SECOND, BUT GOOD\",\"The first, not the best\",\"Detroit Love\"],[\"Not what I expected\",\"READ THIS REVIEW!\",\"Another great book by Joyce Meyer!\",\"You Must read My Review\",\"WOW!\",...,\"The famous mistake of a tower.\",\"Finally! Comfort and practical\",\"Absolute rubbish\",\"Only Amazon.com Purchase I ever returned\",\"I like it\"],...,[\"Good, but not great\",\"Entertaining\",\"nano\",\"How to write a Gomez song\",\"Very small, cannot be used on stove top\",...,\"Great design, does not work\",\"OK\",\"Please somebody tell me I'm not the only one...\",\"Big Al's review\",\"Thrilling adventure\"],[\"Raging piece of garbage\",\"could not get past the first half hour!\",\"Offers Little to the Marketer\",\"Not for the extreme chewers!\",\"Lots of fun!\",...,\"End The Cycle\",\"Rocky Gause--One of a Kind\",\"Very flimsy; starts to collapse over time - NOT recommended.\",\"A Silly Treasure Hunt\",\"Best machine offered I found in web or stores.\"]]\n",
       "review_text: [[\"This coffee maker is great for one cup coffee drinkers. I have worn out one (the swtich no longer works), so I purchased another one yesterday. For the people that complain that coffee continues to drip out after removing the mug/cup...I've never had that happen and if it did, I would just leave the cup under it a little longer. Minor inconvenience in my opinion. I've never had the coffee grinds splash into the water reservoir either. Not even sure how that could happen. I've only had excellent use from my coffee maker. Well worth the money. The first one was a gift several years ago and I used it almost once daily. I guess the switch wore out. Replacing the entire thing is cheap, plus now I'll have more than two cups to use (my original came with two insulated mugs).\",\"I bought this keyboard hoping it was a step up from the $5 one that I had before. Imagine my surprise when it was worse! As soon as I took it out of the box the wrist guard snapped off! The keys got stuck so much it was almost impossible to use without having to lift the key back up in order to use it...don't bother with this one.\",\"Not what I was looking for. You have to find the words that the creater wants for you to find in each word search. Also, there is no reward for finding a correct word. You are not sure if it knows that you circled a word or not. Not recommended.\",\"The articles contained in Awakening:The upside of Y2k are uplifting, encouraging hope and possibility of inner growth toward a god that is nebulous and unknown and brought down to the human level. If this is the way you view God, then you will love this book. Otherwise you may find hope through more of a fact rather than feeling based information source.\",\"She put together many stories from her personal experience and experiences from many other people as well. I get distracted from this style and have found I put it down often and not read it for awhile. I does give us hope and knowledge we are not alone.\",...,\"I did not find this book useful, but to be fair I wasn't entirely sure what I was looking for. I was hoping it would help me to remove myself from a lot of the negative dreams I have been used to having and I would have liked to have found some help in it to get me dreaming better, but it didn't work for me.\",\"I've been using these tubes for years. When I get a puncture, I can hear the air coming out. Sometimes they seal before all the air leakes out and sometimes they don't seal at all. I've even gotten flats with the extra thick version of these tubes. The last time I got a flat, I was 19 miles away from home. From that point on, I carry an extra tube with me with all the tools to fix a flat out on the road. These aren't bad tubes, just don't expect miracles.\",\"First off, the Darkness isn't even the first band to revive the 70's rock style. PLEASE do yourselves a favor and check out such bands as The Tight Bros From Way Back When, the Cherry Valence, and Those Peabodys. All great variations on this theme. But anyway, this album is just fun, and it's a throwback, but it is NOT ironic. It's outlandish because this kind of over-the-top, ebullient music has become so foreign to us. And that's why it's so refreshing to listen to it! There was a time when rock wasn't brooding, rock wasn't about being an obnoxious Hot Topic \"punk\", rock was... just invigorating! Celebratory! That's what this album is.\",\"This was not as good an album as I had remembered from the days of vinyl, but the recording is faithful to the original. It has a very dated sound, and sounds very much like an album from a band which has not yet found itself. That said, this disc is not without its moments, and no Nektar fan really wants to be without it. In terms of production and writing, though, it is fairly two-dimensional. I still enjoy it very much.\",\"I had to get this album because I heard Frank N Dank on the Welcome to Detroit CD and they blew it up. I'm suprised that this was not put out commercially. Frank N Dank have skills, not lyrical but they can with out a doubt hold up their own in the hip-hop ring. This CD is a really good CD through and through for anybody who loves hip hop. And the production by Jay Dee is real good, the brotha ain't going to loose his style. I can't wait til they make a commercial debut.\"],[\"Although this book is rather short, it has some very heavy information that can be useful if studied and practiced thoroughly. However, the information in the book only pertains to limit Hold 'em, not No Limit. Furthermore, the book is a little out of date. However, it does have a lot of good stuff in it, and I would recommend it to any novice player.\",\"Ok, first of all, these tables give you a good bang for your buck.they are made of solid wood however, quality is lacking.after reading the reviews I was hoping that I would get the dark brown color unfortunately I got black which is exactly as advertised.I bought 2 tables although one table is far better than the other.no real complaints about the good table however the bad table has some workmanship issues. it came scratched and the top had been machined improperly.assembly was a breeze.... 1 piece of advice would be to install all the screws loosly and in the end Tighten all the bolts together.\",\"Joyce Meyer is, in my view, the most inspired and relevant Christian author alive today. I have read four of her books so far, and I consider each very rewarding, and potentially life-changing. This book, like Joyce's other books, accomplishes exactly what the title suggests. If you want to truly know how to walk closely with God, and live a rich Christian life, read this or any one of Joyce's books. I have also enjoyed \"How to Hear from God\", \"Seven Things That Steal Your Joy\" and \"In Pursuit of Patience\", all of which are available on Amazon. Joyce, if you are reading this, may God bless you, for you have been a blessing from God for me.\",\"I love all of Hillermans books seeing how he is a wonderful author when writing about modern day Indian mysteries. The only thing I would suggest is that if you read voraciously as I do you might want to peruse used book stores and pick up Tony's books at a bargain price. He has passed away so I don't think he will mind...\",\"Hilary is doing great after the show Lizzie McGuire! She has three new songs and some of her old songs are remixed! She even has her sister singing on here! It is amazing and you should definately buy it!\",...,\"Shrady does a good review of how the Leaning Tower of Pisa came about. It was due to building the tower on land that once belonged to a bog. When some of the sand shifted, the tower (companile) leaned. In 17 commissions over the years, more damage was done until finally in 1990, the tower was in danger of falling over. This is an interesting book about the history of the city state of Pisa, and its remarkable tower and cathedral. The book made me want to visit this historic building.This is a nice read about an interesting building and place. I actually learned something about architecture and soil conditions.\",\"I don't ever write reviews, I just read them. However my baby and I enjoy this product so much that I felt the need to share.It is not difficult to use. If you can't understand the simple directions on the side of the box they have also included a DVD for those who need to see it in action.It is comfortable and I can wear it for hours carrying my baby around doing everything from housework to playing the organ! (Yes, organ! and she is so comfortable she sleeps right through it.)I like that it feels like I am as close to my baby as when we're snuggling at feeding time.It doesn't have lots of pockets, straps, head rests or other things that make infant carriers bulky. It has one large pocket that I use for a burp rag and pacifier. That's all I need.\",\"I am very sorry I bought this tire pressure guage. It worked three times and simply stopped working. Buyer beware this is an absolute piece of rubbish.\",\"I bought this book with the best of intentions, however the lessons were stilted and labored. I continued reading on hoping to find something valuable I could take away from the book, but in the end I came away with nothing. In being fair to the author, I suppose if you are a complete beginner, this book may hold some value, however if you have spent any amount of time writing in any form, there is little here that will aid you in your quest to write flash fiction.This was the only Amazon.com purchase I have returned in all the years and hundreds of books I have purchased. If you have read any books on writing in the past, please don't waste your money. I ended up losing in the end because I paid for shipping and was hit with some other fee, so I actually paid over $5 to read the book. Oh well.\",\"I agree with other reviewers - this shoe is a bit small so order half size bigger. Its very cute though I would have liked it to be more comfy.\"],...,[\"I was a big fan of the Go-Betweens from back in the late '80s. The new \"Oceans Apart\" is a pretty strong recording, with some thoughtful lyrics and a few good tunes, but I found it a little disappointing. It doesn't have the same bounce and enthusiastic rhythm of the band's old material, but it's still worth a purchase for those of you who like the 'tweens.\",\"An entertaining, even relaxed and conversational, read. It recovers some the intellectual history of archaeological theory, albeit not in the most unbiased manner. The most attractive quality of the book remains its ease of reading, it reads just like a fascinating lecture, which, in fact, it was. The book can easily be read over the course of a weekend, or an especially long plane ride.What one will get from the volume depends greatly on what one brings to the volume. The best portion stands as the superlative and unsurpassed ethnoarchaeological work in Northern Alaska, the worst as a very weak discussion of sociocultural evolution.Still, warts and all, it's a valuable read to anyone vaguely interested in archaeology or in the evolutionary development of human culture, which may or may not be an adaptive system relative to its environment.\",\"Very good book. To understand it you need to have a lot of knowledge in science. If you like astrophysics then you will like this.\",\"This album seems like 10 variations on the same song. A lot of bluesy jangling that meanders along with some calm vocal accompaniment, then an energetic freak out with some gravelly shouting, then back again, over and over. It all hangs together, and it's not *bad*, but it doesn't really stand out. The melodies are capable, but nothing exceptional. The vocals sound like something from a Subway commercial. Again, competent, but so what? They do present you with a full sound, and a lot of details, but it's like they did it just so I could say that. None of the touches really accents anything.idk. I bought this disc because Amazon kept pushing it on me. I want to like it, but maybe it's just like a girl that's pretty and all, but doesn't click with me. It's just there, employing some musicians and entertaining those who do seem to like it.Amazon isn't always wrong, though. I loved The Shins.\",\"Its a very small piece, and I was not expecting some one like this when I ordered. Also, it is specified as not to be used on stove top. It is meant for decoration only, and not meant for cooking.\",...,\"The Elite Cuisine coffemaker is a sleek, modern design, but it does not function. As read in other reviews (should have heeded the advice), the water is not heated sufficiently to effectively brew coffee. Tried it several times, same result: warm brown water, with almost no coffee taste. Returned.\",\"A little hard to use with my 7yr old grandson. He loved the experiment, just a little over his head is following the directions.\",\"that sees some (strong) resemblance between \"Love of my life\" and the third movement of Brahms most famous Symphony (sorry, I can't recall the number). I'm a Santana's fan since I was a teenager and Abraxas was one of my favourites ever. I love this cd, it's wonderful, and of course being a Dave Mathews fan it's a real nice surprise to listen to them together. Don't miss it, it's great!!!\",\"This cd definately offers a very nice variaty of songs of the Democratic Republic Congo, the former Zare. All artists are well known and very succesful for years. Some of them performed in Africa, Europe, U.S. and even Japan.There is only one little downside though: half the songs kan be found on other cd's with various artists. So .... check before you buy!\",\"I thought that \"A Long Way from Chicago\" was a very good book. It had funny parts, yet it also had a very good meaning. I thought the meaning was to say that everyone has weird realatives but you grow to love them. Becuase in the beginning of the book Joey and Mary Alice didnt want to go, but in the end they really loved it. I think that everyone should read it, old or young.\"],[\"Stay away from this badly designed, badly written program. Yes, it does everything advertised but it is horrendously convoluted, non-intuitive, and frquently crashes the entire machine. I've never seen a piece of software that burns so much produciton time carrying out the simplistic funcitons that this program tries to provide. If you are looking for business software that will streamline the way you do things & help make you more productive, this is not it. If you buy it, you're wasting your money. If you use it already, you're wasting time, you can do much better than this garbage.\",\"Terrible!Boring.Stupid.Sad to have actually spent money on this movie! BOMB. What was I thinking? JUST SAY NO\",\"I give up! I struggled to read through this book. Phil Carpenter attempts to present how Internet brands are developed by presenting \"case studies\" on six well-known companies. The marketing strategies of these companies (iVillage, CDNow, BarnesandNoble.com, Yahoo!, FogDog Sports, and OnSale) are detailed in a manner filled with \"buzzwords\" but little in the way of thoughtful analysis or performance measurement.Carpenter follows the same business methodology of many Internet companies today in believing that \"big numbers\" translate into success. As we have seen this is a flawed formula. Further, the simplisitic discussions of banner ads, viral marketing, etc., provides little insight into eBrand management for your organization. Specifically, Carpenter never makes the connection between an eBrand and profit.If you are attempting to formulate an internet-based marketing strategy a much better read is Seth Yodin's book on Permission Marketing.\",\"Well I must say for the 3 hours my Pitbull had this toy he loved it-until he chewed it in half.This toy is not very durable-the ends (thicker areas) seemed to hold up ok but the middle was chewed through in no time. This might be an ok toy to fetch/play with but NOT recommended for the serious chewers out there.\",\"I bought this for my son when he was 8 months old. He learned rather quickly how to put the balls in the holes (which we were thrilled about!). I feared that he'd tire of it pretty quickly, however. Now at almost 11 months he still stops to play with it about once a day. I enjoy that it's so sturdy (since my son uses it to crawl over quite frequently). I was wary of buying this because of the ill comments about how the pendulum gets stuck. Ours has only stuck every once in a while; not a big enough deal to not buy it. Too, sometimes the ball doesn't come all the way out from the middle hole (with pendulum). I don't know if it's just uneven plastic on ours or if that's how all of them are. Like I said, though, it's enough fun for the babies to get it anyway.\",...,\"This book changed the way I view relationships forever. In it the author describes the rules to creating a godly marriage and I would say a godly relationship between a man and a women. It is a simple and easy read that once you start to pull back the layers on this onion makes you say wow I understand that. The message is simple and the title explains it all. Get the book if your struggling in your marriage get the book if your not get the book so that you can prevent it.\",\"Rocky Gause, an Army pilot without a plane, made a wartime escape from the Japanese that was remarkable. His accounting of the day to day adventure is also remarkable. The story, in his own handwriting, was a family treasure until a few years ago, when Rocky's son was asked by his mother to see if it could be published for others to read. Rocky Gause had gone back to war, but this time he didn't return. His story was left as a gift to future generations. If Rocky had returned he might have been a writer--the story is that good. I am an aviation writer myself.\",\"This is a pretty good design for taking advantage of the but is very flimsy.The problem is the faster of the second level becomes weak and won't hold it up so you inevitably have it collapse when putting bottles on it to dry.This results in a large noise, once clean now dirty bottles hitting the floor, etc...you get the idea.Not recommended.\",\"The setting is at an island of the emerald eye. There is shining blue water and birds squeaking in the trees. There is soft sand and a crowd of people in the water. Geronimo , Trap, Benjamin and Thea were looking for lost treasure .They found the emerald eye a wonderful treasure. They were looking for hidden treasure . They found it at the end of the story the emerald eye. They had many problems like there ship sinking, falling in the quicksand running away from many bees. They could not find the treasure but they found it at the end of the story by using a map and they saw the emerald eye from the airplane. I think that this was the most funny and interesting story I have ever read! It was cool. And there were many exciting parts.\",\"I made extensive research of many different brands and price listings. I decided on this machine because of the feedback left by others. I am not disappointed, it meets all the expectations others wrote about. I truly recommend to you this product.\"]]\n",
       "input_ids: [[[101,6581,4031,102,2023,...,2026,4157,9338,1012,102],[101,2023,2003,1037,10036,...,0,0,0,0,0],...,[101,1996,2034,1010,2025,...,0,0,0,0,0],[101,5626,2293,102,1045,...,0,0,0,0,0]],[[101,2025,2054,1045,3517,...,0,0,0,0,0],[101,3191,2023,3319,999,...,2015,8840,2891,2135,102],...,[101,2069,9733,1012,4012,...,2035,1996,2086,1998,102],[101,1045,2066,2009,102,...,0,0,0,0,0]],...,[[101,2204,1010,2021,2025,...,0,0,0,0,0],[101,14036,102,2019,14036,...,7397,1010,1996,5409,102],...,[101,2502,2632,1005,1055,...,0,0,0,0,0],[101,26162,6172,102,1045,...,0,0,0,0,0]],[[101,17559,3538,1997,13044,...,2017,2224,2009,2525,102],[101,2071,2025,2131,2627,...,0,0,0,0,0],...,[101,1037,10021,8813,5690,...,1996,2466,2011,2478,102],[101,2190,3698,3253,1045,...,0,0,0,0,0]]]\n",
       "attention_mask: [[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,0,0,0,0,0],...,[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,0,0,0,0,0]],[[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,0,0,0,0,0]],...,[[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,0,0,0,0,0]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,0,0,0,0,0],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,0,0,0,0,0]]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbert_tokenized_train_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04f14442-42fd-4188-a872-89337d1c0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dbert_model=create_model('distilbert/distilbert-base-uncased',num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a05286e2-3bba-4412-9d4a-ceeff57f17ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbert_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "20590f96-474a-494f-a233-693760a1ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #remove token type one\n",
    "# bert_tokenized_train_data=bert_tokenized_train_data.remove_columns(['token_type_ids'])\n",
    "# bert_tokenized_val_data=bert_tokenized_val_data.remove_columns(['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "654eef1e-4483-4668-91ee-921ff494cd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400000/2400000 [02:26<00:00, 16361.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600000/600000 [00:36<00:00, 16335.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dbert_tokenized_train_data = dbert_tokenized_train_data.map(lambda x: {\"label\": x[\"label\"]-1})\n",
    "\n",
    "dbert_tokenized_val_data = dbert_tokenized_val_data.map(lambda x: {\"label\": x[\"label\"]-1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9c5ab-7441-46e1-b401-f9eca193590e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e50a5825-5159-462f-b397-a566585d3d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dbert_tokenized_val_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c681fb11-a6df-40dc-af3d-0d1216e41a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n",
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12638/4224854880.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375000' max='375000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375000/375000 13:38:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613500</td>\n",
       "      <td>0.973408</td>\n",
       "      <td>0.624690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.829046</td>\n",
       "      <td>0.642483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.842257</td>\n",
       "      <td>0.640160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692400</td>\n",
       "      <td>0.869971</td>\n",
       "      <td>0.639007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.667600</td>\n",
       "      <td>0.877341</td>\n",
       "      <td>0.636570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>0.891006</td>\n",
       "      <td>0.634143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.620100</td>\n",
       "      <td>0.937614</td>\n",
       "      <td>0.628990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.598100</td>\n",
       "      <td>0.964792</td>\n",
       "      <td>0.628108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.578400</td>\n",
       "      <td>0.981911</td>\n",
       "      <td>0.625595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.995959</td>\n",
       "      <td>0.624348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39063' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 1:05:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator=DataCollatorWithPadding(tokenizer=distilled_bert_tokenizer,pad_to_multiple_of=8)\n",
    "res=train_model(distilled_bert_tokenizer,dbert_model,dbert_tokenized_train_data,dbert_tokenized_val_data,learning_rate=lr,epochs=10,\n",
    "                # run_name='DistilledBert',\n",
    "                data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf694171-d654-45ed-8727-2d982a457e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<transformers.trainer.Trainer at 0x7f1a915a2e50>,\n",
       " {'eval_loss': 0.8290456533432007,\n",
       "  'eval_accuracy': 0.6424833333333333,\n",
       "  'eval_runtime': 352.7352,\n",
       "  'eval_samples_per_second': 1700.992,\n",
       "  'eval_steps_per_second': 53.156,\n",
       "  'epoch': 10.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82718d80-00b3-4c32-a18d-34ab56f5aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_trainer=res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "180602a1-ec25-468c-b3bb-8096bd2d89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert test data review to string,just in case\n",
    "reviews_test=reviews_test.map(change_to_string,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8e029ff4-84b2-4df7-b757-1649049dc737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650000/650000 [01:44<00:00, 6230.74 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650000/650000 [00:48<00:00, 13415.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#tokenize test data\n",
    "dbert_tokenized_test_data=reviews_test.map(partial(tokenize,tokenizer=distilled_bert_tokenizer),batched=True)\n",
    "\n",
    "#make labels 0 index\n",
    "dbert_tokenized_test_data=dbert_tokenized_test_data.map(lambda x: {\"label\": x[\"label\"]-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f86e97a-879a-4091-b16c-47c8fe0f20a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved to cuda\n",
      "evaluating.....\n"
     ]
    }
   ],
   "source": [
    "dbert_tokenized_test_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "dbert_model.to('cuda')\n",
    "print(\"moved to cuda\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"evaluating.....\")\n",
    "\n",
    "\n",
    "\n",
    "dbert_test_res=distilbert_trainer.evaluate(dbert_tokenized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06ab4a09-63d1-4f99-9878-c4404fb774e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8291406035423279,\n",
       " 'eval_accuracy': 0.6424815384615384,\n",
       " 'eval_runtime': 363.4842,\n",
       " 'eval_samples_per_second': 1788.248,\n",
       " 'eval_steps_per_second': 55.884,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbert_test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6bd1b-7943-4e34-a932-3c39d827c886",
   "metadata": {},
   "source": [
    "#### Roberta for larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f29007d-887d-468f-a7d6-d3566f313795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='FacebookAI/roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load roberta tokenizer\n",
    "roberta_tokenizer_2=AutoTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "roberta_tokenizer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efaa3906-bee1-437f-b1f5-bf51d8653c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400000/2400000 [06:05<00:00, 6563.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600000/600000 [01:29<00:00, 6673.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#tokrnize the train and val df\n",
    "roberta_tokenized_train_data=reviews_train.map(partial(tokenize,tokenizer=roberta_tokenizer_2),batched=True)\n",
    "roberta_tokenized_val_data=reviews_val.map(partial(tokenize,tokenizer=roberta_tokenizer_2),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbbbd555-c7c5-4543-9652-24f6d8b2ae74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "label: int64\n",
       "review_title: string\n",
       "review_text: string\n",
       "input_ids: list<item: int32>\n",
       "  child 0, item: int32\n",
       "attention_mask: list<item: int8>\n",
       "  child 0, item: int8\n",
       "----\n",
       "label: [[4,2,4,2,4,...,1,4,5,4,2],[5,4,4,1,4,...,1,4,5,3,1],...,[4,2,3,5,3,...,1,5,3,1,2],[5,4,4,5,2,...,1,2,4,4,1]]\n",
       "review_title: [[\"Safety Leash for Pedometer\",\"Buggy but fun\",\"Love my undies!!!!!!\",\"horrid game =(\",\"Truth in advertising\",...,\"Of interst only to the author's friends and family\",\"Worth the $15!\",\"I like my firest yoga dve\",\"Sacred Companions: the gift of spiritual friendship & direction\",\"Bought This CD Mainly For One Song\"],[\"Not quite magic, and need some elbow grease, but does what it says...\",\"This book was very knowledgable!\",\"Tongue cleaner\",\"horrible\",\"This is a BIG compact net! WOW\",...,\"Useless in Canada.\",\"Everything needed\",\"Food steaming is the way to go\",\"Good Primer\",\"This book is Pants\"],...,[\"good\",\"nice WiFi; lousy compatibility; not worth upgrading\",\"A different slice of history\",\"Da BEST RAP CD EVER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\",\"This is a good book, But definitely not the best.\",...,\"BAD BATTERY\",\"Insanely good!!\",\"Frustrating\",\"Automatic Yeast Dispenser? I Don't Think So!\",\"Bummer.\"],[\"Before PTSD\",\"A fine cage topper\",\"Nice Charm, Weak Link\",\"It was a gift for family, they loved it!\",\"Excellent Writing Marred By Loathsome Characters\",...,\"Not a pleasant experience\",\"Editor please\",\"GOOD\",\"Am happy with JumpStart Typing for my 9 year old daughter.\",\"Vista users beware\"]]\n",
       "review_text: [[\"These are great for keeping your pedometer from falling off and losing it. I have bought these before and worn them until the clasp wore out. The only problem I have with this one is it is a little to long.\",\"First off a caveat: This game is riddled with bugs! If you're using old (as in more than 6 months old) drivers for your video card, sound card or even (in my case) your microsoft keyboard then prepare to have a really bad time with getting this game to run more than a few hours. However, with some effort (and a phone call to Interplay's UK tech Support hotline) I've been able to work around most of these and when it works I think it's great fun.I like the older (movie) style vessels and as a player of Star Fleet Battles (the board game) in the 80's it was like a trip back to my sad, geeky youth. If it had been shipped in a better state or if a decent patch was available I'd have given it 3.5 stars.When it becomes a budget (10 quid) title then pick it up and have a go!\",\"I was completely excessive and decided to purchase about 25 pairs of Necessary Objects underwear when I saw their sale and I couldn't be more pleased with each item. They are comfortable and really adorable which is a perfect combo. I know that I will be a repeat customer.\",\"I bought this game, hoping to go home and play it. But no after I finished installing it, and started my game, I ran into numerous bugs and gliches. This game looked so promising, but it turned out to just be a horrid experience and a waste of my money. I gave it two stars for the great concept. Really is a cool idea for a game. Bottom line though don't waste your money on this game.\",\"You really need to be dedicated to smelling like cotton candy if you wear this. I know a lot of people say it's sexy or whatever, but I found it kind of overwhelmingly powerful, like I had been involved in some kind of industrial accident involving a cotton candy booth at a state fair. Four stars though because it definitely does what it says on the tin!\",...,\"Journalists often give political analyses of situations they have reported on, and all too often reach facile and laughably naive conclusions. Such is the case with this book. The author might have given us an interesting read if he had stuck to what he is qualified to comment on: himself and his experiences as a journalist. Whether out of false modesty or an inflated sense of his grasp of the situation in Iraq, he chose to weave his personal story around the much larger, vastly more complicated, and more important story of what is happening to the Iraqi people. His conclusions about whether the Iraq invasion was warranted are unsupported, and blatantly self-serving. It can't be a coincidence that a young man who received a large advance to write a book about his experiences during the invasion of Iraq sees the campaign as necessary and ultimately for the best. It certainly was for him!\",\"I bought this sippy cup after reading about this cup on other sites. I wasnt intending to buy this one first but when I saw it in the store I thought okay.... I'll give it a try. I had a hug leak the first time we used it, but I fixed it and we have had nothing but happy sipping since. We travel alot on the weekends and its nice to have a cup that will keep milk cold for more than 2 hours. That alone is worth the purchase!\",\"I have heard that there are good and bad yoga dvd's out there but I found this one really enjoyable. I do it with my sisters and it makes for some nice family bonding. I especially like the short, 5 minute ones that included because it is nice not to have to stop in the middle of a session. I highly recommend shutting your phones off while you enjoy this.\",\"A great used book. Thanks. The book was in very good condition and arrived in the promised amount of time. I appreciate being able to order used books.\",\"I wanted a remastered version of \"Crying\" with k.d. Lang. This CD is remastered, but very badly. The instrumentation has been brought out so much that it literally drowns out the incredible vocals of Roy Orbison and k.d. Lang. It should have been remastered the other way around. The vocals should have been brought out further than the music. Very disappointing.\"],[\"Basically, the title is it. Requires elbow grease. Gets off most wall-gunk and marks better than a lot of other products. Went through about 4 on the walls of my 700 sq-ft apt before I moved out. Worked as expected and as advertised. Will definitely utilize the remainder in the new place. Just remember, it's not totally magic, but the walls looked GREAT after using them. I def recommend as a cheap and effective means to get your walls almost back to new-looking. Enjoy!\",\"This book expanded my knowledge of movies. I know alot more now, than I knew before I picked up the book. Roger was sensational as usual! I recommended this book to one of my friends that is also a movie buff, and now it's in his collection too!\",\"The Tongue cleaner is an excellent way to clear the film on your tongue before every meal; Especially in the morning; Also good before going to bed at night. EVERYTHING tastes so much better and it gets rid of bad breath too!.BUT, be careful when ordering; Perhaps best to order several at the same time, as otherwise the shipping charges are far too high for such a lightweight small item!\",\"i got this cd along with a shirt at an LS show for 5 bones. I am glad i only payed 5. this is crap. i hate it when bands bring god into everything. the lyrics turn me off, its so onesided. and these people are obviously not open minded about anything. if you want jesus christ shoved down your throat then listen to this. the message is awful. the music is awful. the kid who says linkin park is awesome is a complete moron.\",\"Quality of the product overall is fantastic, the size of the net is amazing. However, one down side to this system. The collapsible poles for the net have a bungee cord/shock cord inside them (not the issue)and the caps on each fiberglass pole is made out of a thin metal. My 6 year old son was \"helping\" to clean up and was spinning the metal cap on the end of the pole. This metal cap cut the shock cord and caused the pole to come apart.This is an easy fix but something I wanted to share so nobody else has to fight pushing 30 feet of new shock cord through the poles.Have fun!Randy\",...,\"Can't stream anything in Canada. That says it all. I never knew fiction books were classified reading. This is a silly rule for content.\",\"I love this watch for everything I needed it for . . . it works great! The ONLY reason I gave it a 4 is for the color. The 'aqua' has a sheen I do not care for.\",\"I've become extremely health conscious with the foods I eat and how they're prepared. Getting a food steamer was the next logical step for me. Using the convenience of a microwave comes with problem that it destroys most of the nutrition in the foods. For my health and moreso for the health of my kids I had to get this and it's been a great investment. I can't recommend it enough. I have it out on my counter and whenever someone comes over we talk about it.\",\"This book is a good primer or intro, as it is marketed. My only problem with it is that it is heavy on large pictures, and limited on true text.\",\"I read the first chapter free on my kindle. I thought it was going to be funny. Nope. Instead the book had no character development at all. The author used the same phrases over and over to describe everything. I wish I could get my money back. The overall story read like the main character was 12 not in her early 30's. There was no plot at all. Avoid this book at all costs. In fact avoid this author until she takes a few writing classes.\"],...,[\"it seems to work well enough. in some of the other reviews they said it was surprisingly sharp, but when it arrived it felt pretty dull. i tried using it on a stave i have for a bow im working on. worked so much better than i thought it would, given the fact that it appears to be super dull\",\"I've been a Palm user since the very first days, and I usually get around to upgrading to the latest within a year or so of its release. My Tungsten E is great, but I upgraded to the T|X to get Wifi. What a mistake. Many of my apps are not compatible (due to a ?memory architecture? change), and even the software-install flaked out my previous desktop config. I just spent 3 hours fixing the problems and re-installing the old desktop for the Tungsten E. The TX is going back to the store... (If I'm going to spend this much $$ and time upgrading, I'll probably look at the HP or the Dell ... or the other mobile Win platforms.) I like Palm because it was so easy ... true plug and play. They loused this up badly.\",\"This is an interesting slice of history, from a specific point of view in regards to the perfume industry, and while it was interesting, it wasn't a compelling book where you hurried to turn the page and see what happens next. If you are interested in scents, perfumes, and related topics, its a good read, but if those aren't serious interests, this book would be a real chore to read. Its well written, just not particularly \"lively\" along the way.\",\"Ja rule is the greatest ever..this abum has a mix of Hardcore, undergroung and R&B. he is a great lyricist. and he has tigh beats.... Beleve me he is the greatest this alum went double platnum in 4 months and it is still in the billboard charts at high positions..............\",\"This book starts when the animorphs are at the the new Planet Hollywood when they see a man jumping out the window at a office building. Rachel and the animorphs save him , but really no credit- because Big ARNOLD S.had to help. they soon go see the man. He is a controller, going crazy because he eats- instant oatmeal! Then they decide to go underground- to the yeerk pool!\",...,\"Battery failed to hold a charge. When charged and attached to my camera light, battery went off after about ten minutes. It would work for a few seconds and then go off.\",\"Don't judge this album by its cover or its dull title. The music is amazing. I listened to it on repeat when I first got it. Kadri is famous in India, maybe not as well known outside India? If you like western sax or Indian music, you will enjoy this album.\",\"The songs \"Frustration\" and \"Jenny\" are definitively great. But the rest of the album is a little bit disappointing...\",\"Purchased two Panasonic SD-YD-250 with the hopes that one of them would dispense yeast and raise some bread. No yeast dispense so you have a flat burnt loaf, yummy! Support not much of a help when they are located in India. If you purchase one keep your box because you will be sending it back which Amazon is kind enough to pick it up at no charge. So I will order a different bread maker and write a new review. You are much better off adding the yeast yourself and let the machine do the rest.\",\"I had high hopes for this movie and bought it for my sons 4th birthday because he is obsessed with horses. I'm also a fan of old school disney but this movie was lame! Boring and I'm sad I actually bought a new DVD instead of a cheaper used one. Oh well, probably won't get a lot of use. My son doesn't like it too much.\"],[\"I am \"hooked\" on Bill Murray's \"The Razor's Edge\" and have recommended it to both PTSD sufferers and to family members for a different type of insight than \"The Deer Hunter\" and other PTSD movies. In this version the book is followed much more closely and a slightly different slant is given to the character \"Larry\" but the amazing thing is the way that the PTSD that Larry suffers shows through. I have recently purchased both versions for a friend of mine in the medical field who will be stationed in Afgan soon - hoping they watch both (along with some of Paul Fussell's books) prior to being in country and having to deal with the very problems projected by \"Larry\" in both versions of this outstanding novel adaptation.\",\"The wire cage topper works great. It attaches securely to my 10-gallon aquarium via two ingeniously designed spring clips, and the clasps fit securely at all four corners. It's easy to take on and off, and it allows plenty of air circulation into the acquarium.I've decided not to use the plastic accessories, though. My hamster holed up in the little house immediately, and I figured I'd never seen him again as long as the house was there. Instead, I provide plenty of pine shavings, and if he wants to burrow completely under them, he does.Also, after realizing how smooth and slippery the ramps are, I decided not to set them up in the cage. They're quite steep when configured as designed, and since hamsters are nearsighted and clumsy, it seemed likely my critter could tumble quite a long way.Even without the plastic accessories, the tank topper is worthwhile. The price at Amazon is very reasonable, especially with free Super Saver shipping.\",\"I loved this little sandal charm! It was dangling nicely on my Italian charm bracelet, until one day, I looked down, and it was gone! Apparently, it got hung up on my clothing, and it broke the gold looking chain. I felt blessed to find it again, So I know that it was the round bead at the end of the chain that broke off. Funny thing, is that I did not know when it broke, I just looked down & found that it was gone. I must find a better way to hold the charm on the bracelet. Know that it's cute, but I'm giving it 4 stars because the chain broke, and I almost lost it.\",\"It was a gift for family, they loved it! I didn't actually watch it myself.\",\"The trilogy the title refers to, revolves around Patrick Melrose at three points in his life. The first is at five when he's raped by his sadistic father. The second is in his late twenties as a a drug addict who has come to claim his father's ashes. The final section is a glimpse at Patrick as a recovered addict, navigating life while trying to put the demon of his father to rest. The technical writing is great. Beautiful witty prose, descriptive and vivid characters and smart witty dialogue. Yet I found it increasingly difficult to hang in there when I detested nearly everyone in the pages of this book. Sneering snobs, and drug addicts. Not my idea of an enjoyable read.\",...,\"If you have been constipated for a long time, this product will clean you out. I do not recommend as it was bought thinking it was a fiber supplement. I did not enjoy using the product. Cheers!\",\"These reviewers are not exaggerating; there are multiple typos on nearly every page. The poor editing broke the flow of the book and I found myself frustrated during the read (perhaps this isn't very ACT minded of me). While reading, I thought we would never accept this of a graduate psychology student's academic or professional writing, yet they can sell it to me for $20. The ACT principles are communicated intelligently and clearly in this book.\",\"XP is a nice fresh way of looking at the software development process and this book explains XP concisely and clearly. You may or may not agree to -all- of the principles they advocate, but if you have any interest in your work at all, you will find parts of this gripping. Especially the emphasis on unit tests and the test-first programming idea. And so useful in practice too! Anyone involved with making software, or indeed anyone else who is interested in increasing the quality of what you're working on whatever it is, should read this! So better borrow it from your neighbour (reading once is sufficient, no need to own this book).\",\"My daugher has happily substituted the games in JumpStart Typing for the Neopets arcade games she was playing before. The program does a good job at enforcing acquiring new material by having a \"team strength bar\" that dimishes to zero as you make mistakes while playing; to play more, you have to do some lesson work until it is back up to 100%. It also steadily makes the games harder as you learn more letters and get better. The result is overall quite a bit more work than just playing regular games, but the net is still positive. However, some parental encouragment is needed;our rule is that JumpStart Typing time is unlimited, after homework and other duties have been met, while all other computer use is limited to 90 minutes a week. The other typing programs I looked at woudl all have required a little more parental help and encouragement. A good value.\",\"This software was just the ticket to keep track of those ever annoying passwords I thought. Well long story short, it worked slowely on XP Pro. I would usually give up on waiting and just enter the password manually. Upgrading to a new computer with Vista it works with a couple accounts but won't work with most. Save your money!\"]]\n",
       "input_ids: [[[0,40592,1063,1671,13,...,1,1,1,1,1],[0,46379,4740,53,1531,...,10,869,9,2141,2],...,[0,39429,2050,44289,2485,...,1,1,1,1,1],[0,387,12807,152,7522,...,1,1,1,1,1]],[[0,7199,1341,8375,6,...,328,2,1,1,1],[0,713,1040,21,182,...,1,1,1,1,1],...,[0,12350,15896,254,2,...,1,1,1,1,1],[0,713,1040,16,42616,...,1,1,1,1,1]],...,[[0,8396,2,2,405,...,1,1,1,1,1],[0,34033,26784,131,38909,...,12031,16,164,124,2],...,[0,37434,29177,11467,1988,...,3563,109,5,1079,2],[0,387,22539,4,2,...,1,1,1,1,1]],[[0,17206,24679,2,2,...,9,1206,274,687,2],[0,250,2051,16051,7,...,7,12601,4610,2198,2],...,[0,10127,1372,19,22824,...,6,53,5,1161,2],[0,846,6377,1434,36798,...,1,1,1,1,1]]]\n",
       "attention_mask: [[[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,0,0,0,0,0]],[[1,1,1,1,1,...,1,1,0,0,0],[1,1,1,1,1,...,0,0,0,0,0],...,[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,0,0,0,0,0]],...,[[1,1,1,1,1,...,0,0,0,0,0],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,0,0,0,0,0]],[[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,1,1,1,1,1],[1,1,1,1,1,...,0,0,0,0,0]]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenized_val_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3eeb4b8b-21f8-46ab-b214-017c5340ed40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "roberta_model_2=create_model('FacebookAI/roberta-base',num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3990a2f-182a-4ee4-a730-6c25a4463127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model_2.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "999d631a-fbef-409d-8a9a-e006444d5432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400000/2400000 [02:24<00:00, 16651.31 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600000/600000 [00:35<00:00, 16669.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#make the labels 0-index\n",
    "roberta_tokenized_train_data = roberta_tokenized_train_data.map(lambda x: {\"label\": x[\"label\"]-1})\n",
    "\n",
    "roberta_tokenized_val_data = roberta_tokenized_val_data.map(lambda x: {\"label\": x[\"label\"]-1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "579d9bbc-c354-48f3-8cc4-ee7adc465d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n",
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "print(set(roberta_tokenized_train_data['label']))\n",
    "print(set(roberta_tokenized_val_data['label']))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4bf7ba7-779e-42f9-b47f-9706f7516c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250415_183958-tykcw00l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20second%20dataset/runs/tykcw00l' target=\"_blank\">RoBERTa</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20second%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20second%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20second%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20second%20dataset/runs/tykcw00l' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20second%20dataset/runs/tykcw00l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3885/352122093.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375000' max='375000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375000/375000 23:59:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.821400</td>\n",
       "      <td>0.800067</td>\n",
       "      <td>0.653472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.771400</td>\n",
       "      <td>0.793339</td>\n",
       "      <td>0.659042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.743300</td>\n",
       "      <td>0.785434</td>\n",
       "      <td>0.662037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.717400</td>\n",
       "      <td>0.793792</td>\n",
       "      <td>0.660605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.803194</td>\n",
       "      <td>0.660132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668400</td>\n",
       "      <td>0.827363</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.645200</td>\n",
       "      <td>0.845383</td>\n",
       "      <td>0.656132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>0.856486</td>\n",
       "      <td>0.654717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.605800</td>\n",
       "      <td>0.869739</td>\n",
       "      <td>0.652527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.886404</td>\n",
       "      <td>0.651422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 09:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator=DataCollatorWithPadding(tokenizer=roberta_tokenizer_2)\n",
    "roberta_trainer,val_res=train_model(roberta_tokenizer_2,roberta_model_2,roberta_tokenized_train_data,roberta_tokenized_val_data,learning_rate=lr,\n",
    "                                    epochs=10,run_name='RoBERTa',data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "da972f7d-15aa-424b-98b2-73acfc21ee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7854337692260742,\n",
       " 'eval_accuracy': 0.6620366666666667,\n",
       " 'eval_runtime': 586.3292,\n",
       " 'eval_samples_per_second': 1023.316,\n",
       " 'eval_steps_per_second': 31.979,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c96cb9a-20eb-4571-bfbb-df47e42e3f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650000/650000 [00:02<00:00, 282597.92 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650000/650000 [01:39<00:00, 6565.35 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 650000/650000 [00:40<00:00, 15859.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#tokenize test data\n",
    "roberta_tokenized_test_data=reviews_test.map(partial(tokenize,tokenizer=roberta_tokenizer_2),batched=True)\n",
    "\n",
    "#make labels 0 index\n",
    "roberta_tokenized_test_data=roberta_tokenized_test_data.map(lambda x: {\"label\": x[\"label\"]-1})\n",
    "\n",
    "# roberta_tokenized_test_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# roberta_model_2.to('cuda')\n",
    "# print(\"moved to cuda\")\n",
    "\n",
    "# #load the best model\n",
    "# roberta_best_model_2=AutoModelForSequenceClassification.from_pretrained('results/checkpoint-112500')\n",
    "# print(\"loaded best model\")\n",
    "\n",
    "# print(\"evaluating.....\")\n",
    "# test_trainer=Trainer(model=roberta_best_model_2,\n",
    "#                 tokenizer=roberta_tokenizer_2)\n",
    "\n",
    "\n",
    "# roberta_test_res=test_trainer.evaluate(roberta_tokenized_test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3137436-82cf-47d6-958d-14fde17c297f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "print(set(roberta_tokenized_test_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57ac93c9-04e6-4655-b85c-9e28e1fcb662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load best model\n",
    "roberta_best_model_2=AutoModelForSequenceClassification.from_pretrained('results/checkpoint-150000')\n",
    "print(roberta_best_model_2.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7e7ac1a-388f-4aaf-ab40-c8b10cf5b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved to cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81250' max='81250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81250/81250 29:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#moveto cuda\n",
    "roberta_best_model_2.to('cuda')\n",
    "print(\"moved to cuda\")\n",
    "\n",
    "test_trainer=Trainer(model=roberta_best_model_2,\n",
    "                     processing_class=roberta_tokenizer_2,compute_metrics=eval_metrics)\n",
    "\n",
    "\n",
    "roberta_test_res=test_trainer.evaluate(roberta_tokenized_test_data)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d30aeeae-dab7-45f1-826a-b13684b5de8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7945932745933533,\n",
       " 'eval_model_preparation_time': 0.0068,\n",
       " 'eval_accuracy': 0.6597876923076923,\n",
       " 'eval_runtime': 1786.7356,\n",
       " 'eval_samples_per_second': 363.792,\n",
       " 'eval_steps_per_second': 45.474}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "09f2c9f3-f639-484d-a3d9-398304383113",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./acc_dict.txt',\"w\") as f:\n",
    "    for key, value in acc_dict.items():\n",
    "        f.write(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05ed9b-eee9-4ec4-b27a-63a42d40eb96",
   "metadata": {},
   "source": [
    "#### Compare both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04d09e1a-aa05-4705-903a-f1de1da678a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_results={\"Model\":['DistilBert','Roberta'],\n",
    "             \"Accuracies\":[dbert_test_res['eval_accuracy'],0.65]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d36b80e-c429-45d6-b0cd-a2d983c2e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df=pd.DataFrame(acc_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4dc96-4785-47b5-8267-3afafb344f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d799371a-1d92-43fc-a813-cb4a6f17a0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DistilBert</td>\n",
       "      <td>0.642482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roberta</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Accuracies\n",
       "0  DistilBert    0.642482\n",
       "1     Roberta    0.650000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca7269ca-09e2-449a-bbda-4ec3e66149af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Model', ylabel='Accuracies'>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot\n",
    "\n",
    "sns.barplot(accuracy_df,x=\"Model\",y=\"Accuracies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0f70d-3f8c-4ab8-b684-3c2a180cfeb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4740e215-c27c-461b-af36-7c994673b586",
   "metadata": {},
   "source": [
    "### Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfd1455-2aaa-4fdb-81f5-d36202c974d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews_small=pd.read_csv('amazon_cells_labelled.txt',delimiter='\\t',names=['Sentence','Class'],header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b26eb4-f857-4417-a544-dcf4e4979037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_df,val_small_df,test_small_df=split_train_test_val(amazon_reviews_small,'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd1cfbbe-285e-42a7-92df-b1e3ff8299bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_dict_small={\"Model\":[],\n",
    "          'Accuracy':[]}\n",
    "def update_dict(model,acc):\n",
    "    acc_dict_small[\"Model\"].append(model)\n",
    "    acc_dict_small['Accuracy'].append(acc)\n",
    "\n",
    "    return acc_dict_small\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46515a1d-dc40-49b3-8a65-309db76510ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af4a0945-5c30-407e-a8c4-919fa5996356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer=AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "#tokenize train_df\n",
    "train_small_df['Bert Tokenizer']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[0])\n",
    "train_small_df['Bert Attention_Mask']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_small_df['Bert Tokenizer']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[0])\n",
    "val_small_df['Bert Attention_Mask']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[1])\n",
    "\n",
    "#create bert model\n",
    "bert_model=create_model('google-bert/bert-base-uncased',num_labels=2)\n",
    "\n",
    "#we will create the class of our dataset\n",
    "bert_train_small_df=HuggingFaceDataset(train_small_df['Bert Tokenizer'],train_small_df['Bert Attention_Mask'],train_small_df['Class'])\n",
    "bert_val_small_df=HuggingFaceDataset(val_small_df['Bert Tokenizer'],val_small_df['Bert Attention_Mask'],val_small_df['Class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "189eb3c9-7ef6-4a6e-8c3c-1fb79018d969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b4c8bbeb-ba0d-4024-b942-1b02e03b73fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–…â–â–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–â–â–ˆâ–ˆ</td></tr><tr><td>eval/loss</td><td>â–ƒâ–‡â–â–‚â–ƒâ–ƒâ–„â–†â–‡â–‡â–ˆâ–â–</td></tr><tr><td>eval/runtime</td><td>â–â–â–ƒâ–„â–…â–â–†â–†â–…â–†â–…â–…â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–†â–†â–„â–ƒâ–ƒâ–†â–â–â–‚â–‚â–‚â–ƒâ–ˆ</td></tr><tr><td>eval/steps_per_second</td><td>â–†â–†â–„â–ƒâ–ƒâ–†â–â–â–‚â–‚â–‚â–ƒâ–ˆ</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–â–‚â–„â–ˆâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–†â–†â–‡â–†</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–„â–ƒâ–ƒâ–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–†â–„â–‚â–…â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.64248</td></tr><tr><td>eval/loss</td><td>0.82914</td></tr><tr><td>eval/runtime</td><td>363.4842</td></tr><tr><td>eval/samples_per_second</td><td>1788.248</td></tr><tr><td>eval/steps_per_second</td><td>55.884</td></tr><tr><td>total_flos</td><td>7.9484691456e+17</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>375000</td></tr><tr><td>train/grad_norm</td><td>10.0127</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5613</td></tr><tr><td>train_loss</td><td>0.64388</td></tr><tr><td>train_runtime</td><td>49110.809</td></tr><tr><td>train_samples_per_second</td><td>488.691</td></tr><tr><td>train_steps_per_second</td><td>7.636</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">./results</strong> at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/huggingface/runs/iz4azxqo' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/huggingface/runs/iz4azxqo</a><br> View project at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/huggingface' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250419_225945-id8ccfuc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/id8ccfuc' target=\"_blank\">Bert</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/id8ccfuc' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/id8ccfuc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12638/2605672323.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 02:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>0.374759</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.402620</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.420298</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.493469</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.478819</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>0.479655</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>0.510994</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.507010</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.520842</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.523285</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='191' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 1:01:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator=DataCollatorWithPadding(bert_tokenizer)\n",
    "bert_trainer,bert_res=train_model(bert_tokenizer,bert_model,bert_train_small_df,bert_val_small_df,learning_rate=lr,epochs=10,run_name=\"Bert\",\n",
    "                                 data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4851bd6-e3fc-42dc-a349-97de134471b1",
   "metadata": {},
   "source": [
    "#### Test the Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b6d82-b061-427a-b3a1-ec576f68b850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5a4fb4f0-e1a3-425e-88b4-69394466c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize test data\n",
    "test_small_df['Bert Tokenizer']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[0])\n",
    "test_small_df['Bert Attention_Mask']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,bert_tokenizer)[1])\n",
    "\n",
    "\n",
    "bert_test_small_df=HuggingFaceDataset(test_small_df['Bert Tokenizer'],test_small_df['Bert Attention_Mask'],test_small_df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a591bf1a-eadc-49be-a48c-bd76bf96a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "bert_test_res_small=bert_trainer.evaluate(bert_test_small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "76c805b3-2027-4736-93a1-5ea168871274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2822708487510681,\n",
       " 'eval_accuracy': 0.9266666666666666,\n",
       " 'eval_runtime': 0.8529,\n",
       " 'eval_samples_per_second': 175.872,\n",
       " 'eval_steps_per_second': 44.554,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_test_res_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dcca7d0d-efe8-4422-8236-1c6ea0453b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Bert'], 'Accuracy': [0.9266666666666666]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_dict(\"Bert\",bert_test_res_small['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddea52e-677e-4156-9f65-de1275ea0dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7834afb9-106d-4f37-9c7f-27444fb906e8",
   "metadata": {},
   "source": [
    "#### Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45c0ad0-1c39-4e79-bc18-92459f51b88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "roberta_tokenizer=AutoTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "\n",
    "##tokenize train_df\n",
    "train_small_df['Roberta Tokenizer']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[0])\n",
    "train_small_df['Roberta Attention_Mask']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_small_df['Roberta Tokenizer']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[0])\n",
    "val_small_df['Roberta Attention_Mask']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[1])\n",
    "\n",
    "\n",
    "#create roberta model\n",
    "roberta_model=create_model('FacebookAI/roberta-base',num_labels=2)\n",
    "\n",
    "\n",
    "#create the class of our dataset\n",
    "roberta_train_small_df=HuggingFaceDataset(train_small_df['Roberta Tokenizer'],train_small_df['Roberta Attention_Mask'],train_small_df['Class'])\n",
    "roberta_val_small_df=HuggingFaceDataset(val_small_df['Roberta Tokenizer'],val_small_df['Roberta Attention_Mask'],val_small_df['Class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ebc0150-481c-4ce3-b332-b89af96a4f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Roberta</strong> at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/3r2e3q3n' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/3r2e3q3n</a><br> View project at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250420_174808-kzngwbwo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/kzngwbwo' target=\"_blank\">Roberta</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/kzngwbwo' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/kzngwbwo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15847/2605672323.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 03:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.514300</td>\n",
       "      <td>0.363093</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.257700</td>\n",
       "      <td>0.405656</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.611728</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.484678</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.587915</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.783616</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.802131</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.755294</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.827626</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.827826</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 04:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator=DataCollatorWithPadding(roberta_tokenizer)\n",
    "roberta_trainer,roberta_res=train_model(roberta_tokenizer,roberta_model,roberta_train_small_df,roberta_val_small_df,learning_rate=lr,epochs=10,\n",
    "                                        run_name=\"Roberta\",data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31b94bac-4ce6-4ad2-84ac-2989a39cd8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.36309289932250977,\n",
       " 'eval_accuracy': 0.9333333333333333,\n",
       " 'eval_runtime': 0.7867,\n",
       " 'eval_samples_per_second': 190.658,\n",
       " 'eval_steps_per_second': 48.3,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9ff324f-1880-4c6e-b495-ec209df466fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7f5bd797ef10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f3d33-3dc6-4f99-be4d-1af946caea48",
   "metadata": {},
   "source": [
    "#### test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9efd2d4-8c5a-4e67-b67c-29f9d20d0e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2804804742336273, 'eval_accuracy': 0.9466666666666667, 'eval_runtime': 0.8224, 'eval_samples_per_second': 182.398, 'eval_steps_per_second': 46.207, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#tokenize test data\n",
    "test_small_df['Roberta Tokenizer']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[0])\n",
    "test_small_df['Roberta Attention_Mask']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,roberta_tokenizer)[1])\n",
    "\n",
    "\n",
    "roberta_test_small_df=HuggingFaceDataset(test_small_df['Roberta Tokenizer'],test_small_df['Roberta Attention_Mask'],test_small_df['Class'])\n",
    "\n",
    "roberta_test_res=roberta_trainer.evaluate(roberta_test_small_df)\n",
    "print(roberta_test_res)\n",
    "\n",
    "# #load best model\n",
    "# best_roberta_model=AutoModelForSequenceClassification.from_pretrained('small_data/results/checkpoint-525')\n",
    "\n",
    "# test_trainer=Trainer(model=best_roberta_model,\n",
    "#                      processing_class=roberta_tokenizer)\n",
    "\n",
    "# roberta_test_small_res=test_trainer.evaluate(roberta_test_small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da98d004-6237-4e9e-a25f-d8fef5fb651e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Bert', 'Roberta'],\n",
       " 'Accuracy': [0.9266666666666666, 0.9466666666666667]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_dict(\"Roberta\",roberta_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68415526-1a8b-4bf5-89d3-d97d2057e7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda18f4a-e5d0-4d81-a9a2-3dada252ae27",
   "metadata": {},
   "source": [
    "### Albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57e1e38b-4fa0-4219-8880-4f2c8572fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "albert_tokenizer=AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "#tokenize train_df\n",
    "train_small_df['Albert Tokenizer']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[0])\n",
    "train_small_df['Albert Attention_Mask']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_small_df['Albert Tokenizer']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[0])\n",
    "val_small_df['Albert Attention_Mask']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[1])\n",
    "\n",
    "#create bert model\n",
    "albert_model=create_model('albert-base-v2',num_labels=2)\n",
    "\n",
    "#we will create the class of our dataset\n",
    "albert_train_small_df=HuggingFaceDataset(train_small_df['Albert Tokenizer'],train_small_df['Albert Attention_Mask'],train_small_df['Class'])\n",
    "albert_val_small_df=HuggingFaceDataset(val_small_df['Albert Tokenizer'],val_small_df['Albert Attention_Mask'],val_small_df['Class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46bfee7d-4ea3-462d-8ba2-d25303aa5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–†â–…â–ƒâ–…â–…â–‚â–‚â–‚â–â–‚â–†â–ˆ</td></tr><tr><td>eval/loss</td><td>â–‚â–ƒâ–…â–„â–…â–‡â–ˆâ–‡â–ˆâ–ˆâ–‚â–</td></tr><tr><td>eval/runtime</td><td>â–â–…â–…â–ƒâ–†â–â–…â–…â–„â–ƒâ–†â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–„â–„â–†â–ƒâ–ˆâ–„â–„â–…â–†â–ƒâ–</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–„â–„â–†â–ƒâ–ˆâ–„â–„â–…â–†â–ƒâ–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–â–ˆâ–â–â–â–â–â–â–â–</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–‡â–†â–†â–…â–„â–ƒâ–ƒâ–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–„â–„â–‚â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.94667</td></tr><tr><td>eval/loss</td><td>0.28048</td></tr><tr><td>eval/runtime</td><td>0.8224</td></tr><tr><td>eval/samples_per_second</td><td>182.398</td></tr><tr><td>eval/steps_per_second</td><td>46.207</td></tr><tr><td>total_flos</td><td>1798610730000000.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>1750</td></tr><tr><td>train/grad_norm</td><td>0.02589</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0214</td></tr><tr><td>train_loss</td><td>0.12557</td></tr><tr><td>train_runtime</td><td>215.0563</td></tr><tr><td>train_samples_per_second</td><td>32.55</td></tr><tr><td>train_steps_per_second</td><td>8.137</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Roberta</strong> at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/kzngwbwo' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/kzngwbwo</a><br> View project at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250420_184319-4uoyszgu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/4uoyszgu' target=\"_blank\">Albert</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/4uoyszgu' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/4uoyszgu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15847/2605672323.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer=Trainer(model=model,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 02:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.540367</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.389675</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.480966</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.570745</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.608569</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.614811</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.642575</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.648912</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.649712</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.input_ids[idx],dtype=torch.long),\n",
      "/tmp/ipykernel_15847/2168056350.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'attention_mask': torch.tensor(self.attention_mask[idx],dtype=torch.long),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 13:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator=DataCollatorWithPadding(albert_tokenizer)\n",
    "albert_trainer,albert_res=train_model(albert_tokenizer,albert_model,albert_train_small_df,albert_val_small_df,learning_rate=lr,epochs=10,\n",
    "                                        run_name=\"Albert\",data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2f202b1-5433-4f38-9999-e08731d89919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4279579818248749, 'eval_accuracy': 0.9266666666666666, 'eval_runtime': 1.2113, 'eval_samples_per_second': 123.832, 'eval_steps_per_second': 31.371, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "#tokenize test data\n",
    "test_small_df['Albert Tokenizer']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[0])\n",
    "test_small_df['Albert Attention_Mask']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,albert_tokenizer)[1])\n",
    "\n",
    "\n",
    "albert_test_small_df=HuggingFaceDataset(test_small_df['Albert Tokenizer'],test_small_df['Albert Attention_Mask'],test_small_df['Class'])\n",
    "\n",
    "albert_test_res=albert_trainer.evaluate(albert_test_small_df)\n",
    "print(albert_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c83558f-e2fe-4205-93a7-b3e7dc2731df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Bert', 'Roberta', 'Albert'],\n",
       " 'Accuracy': [0.9266666666666666, 0.9466666666666667, 0.9266666666666666]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_dict(\"Albert\",albert_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875ed96-ffb3-4510-b3de-67f230b0013b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ec2908-151f-486c-a06f-44ed2128cd30",
   "metadata": {},
   "source": [
    "### DisltilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1228081d-d7f8-469e-9ff8-a9560a241a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "distilbert_tokenizer=AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "\n",
    "#tokenize traindf\n",
    "train_small_df['DistilBert Tokenizer']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[0])\n",
    "train_small_df['DistilBert Attention_Mask']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[1])\n",
    "\n",
    "#tokenize valdf\n",
    "val_small_df['DistilBert Tokenizer']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[0])\n",
    "val_small_df['DistilBert Attention_Mask']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[1])\n",
    "\n",
    "\n",
    "\n",
    "#create model\n",
    "distilbert_model=create_model('distilbert/distilbert-base-uncased',num_labels=2)\n",
    "\n",
    "#create instance from data classes\n",
    "distilbert_train_small_df=HuggingFaceDataset(train_small_df['DistilBert Tokenizer'],train_small_df['DistilBert Attention_Mask'],train_small_df['Class'])\n",
    "distilbert_val_small_df=HuggingFaceDataset(val_small_df['DistilBert Tokenizer'],val_small_df['DistilBert Attention_Mask'],val_small_df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8435db4c-dbb3-4380-af64-5ab279fe360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–†â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–†â–†</td></tr><tr><td>eval/loss</td><td>â–…â–â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–â–‚â–‚</td></tr><tr><td>eval/runtime</td><td>â–â–â–â–â–ˆâ–â–â–ƒâ–„â–â–â–ƒâ–…</td></tr><tr><td>eval/samples_per_second</td><td>â–ˆâ–ˆâ–‡â–ˆâ–â–ˆâ–ˆâ–†â–…â–‡â–ˆâ–†â–ƒ</td></tr><tr><td>eval/steps_per_second</td><td>â–ˆâ–ˆâ–‡â–ˆâ–â–ˆâ–ˆâ–†â–…â–‡â–ˆâ–†â–ƒ</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–…â–ˆâ–â–â–â–â–â–â–â–</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–‡â–†â–†â–…â–„â–ƒâ–ƒâ–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–…â–ƒâ–‚â–‚â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.92667</td></tr><tr><td>eval/loss</td><td>0.42796</td></tr><tr><td>eval/runtime</td><td>1.2113</td></tr><tr><td>eval/samples_per_second</td><td>123.832</td></tr><tr><td>eval/steps_per_second</td><td>31.371</td></tr><tr><td>total_flos</td><td>163365930000000.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>1750</td></tr><tr><td>train/grad_norm</td><td>0.00737</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0062</td></tr><tr><td>train_loss</td><td>0.09667</td></tr><tr><td>train_runtime</td><td>150.8654</td></tr><tr><td>train_samples_per_second</td><td>46.399</td></tr><tr><td>train_steps_per_second</td><td>11.6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Albert</strong> at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/4uoyszgu' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/4uoyszgu</a><br> View project at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250420_192802-fo7i65dd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/fo7i65dd' target=\"_blank\">DistilBert</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/fo7i65dd' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/fo7i65dd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 02:16, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.355628</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.317041</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.108700</td>\n",
       "      <td>0.453195</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.471001</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.506673</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.530916</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.549626</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.560677</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.569720</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.574439</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 21:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train\n",
    "data_collator=DataCollatorWithPadding(distilbert_tokenizer)\n",
    "dbert_small_trainer,dbert_small_res=train_model(distilbert_tokenizer,distilbert_model,distilbert_train_small_df,\n",
    "                                                distilbert_val_small_df,learning_rate=lr,epochs=10,run_name=\"DistilBert\",data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "748f90ae-85d2-4e77-9b55-e79bdebddd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35724058747291565, 'eval_accuracy': 0.8933333333333333, 'eval_runtime': 0.5003, 'eval_samples_per_second': 299.828, 'eval_steps_per_second': 75.956, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "#tokenize test data\n",
    "test_small_df['DistilBert Tokenizer']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[0])\n",
    "test_small_df['DistilBert Attention_Mask']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,distilbert_tokenizer)[1])\n",
    "\n",
    "distilbert_test_small_df=HuggingFaceDataset(test_small_df['DistilBert Tokenizer'],test_small_df['DistilBert Attention_Mask'],test_small_df['Class'])\n",
    "\n",
    "dbert_test_res=dbert_small_trainer.evaluate(distilbert_test_small_df)\n",
    "print(dbert_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e340c6e-e1f5-4853-bf1d-ff6feac512d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Bert', 'Roberta', 'Albert', 'DistilBert'],\n",
       " 'Accuracy': [0.9266666666666666,\n",
       "  0.9466666666666667,\n",
       "  0.9266666666666666,\n",
       "  0.8933333333333333]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_dict(\"DistilBert\",dbert_test_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b928d27-50bb-4a4a-a8c3-fc8bc2ba8f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "842a167c-93d9-4e63-b1aa-64a342493c26",
   "metadata": {},
   "source": [
    "### XLNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e323296a-3ba9-42f4-a3eb-5239505ad9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet/xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "xlnet_tokenizer=AutoTokenizer.from_pretrained('xlnet/xlnet-base-cased')\n",
    "\n",
    "##tokenize train_df\n",
    "train_small_df['XLNet Tokenizer']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[0])\n",
    "train_small_df['XLNet Attention_Mask']=train_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[1])\n",
    "\n",
    "#tokenize val_df\n",
    "val_small_df['XLNet Tokenizer']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[0])\n",
    "val_small_df['XLNet Attention_Mask']=val_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[1])\n",
    "\n",
    "\n",
    "#create roberta model\n",
    "xlnet_model=create_model('xlnet/xlnet-base-cased',num_labels=2)\n",
    "\n",
    "\n",
    "#create the class of our dataset\n",
    "xlnet_train_small_df=HuggingFaceDataset(train_small_df['XLNet Tokenizer'],train_small_df['XLNet Attention_Mask'],train_small_df['Class'])\n",
    "xlnet_val_small_df=HuggingFaceDataset(val_small_df['XLNet Tokenizer'],val_small_df['XLNet Attention_Mask'],val_small_df['Class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe08eb6a-5649-4029-b83a-6a8fd35d7bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting training arguments\n",
      "Setting up the trainer....\n",
      "move to cuda\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>â–â–‡â–‡â–†â–ƒâ–…â–‚â–ˆâ–ˆâ–ˆâ–</td></tr><tr><td>eval/loss</td><td>â–â–„â–ƒâ–„â–†â–†â–ˆâ–…â–…â–…â–</td></tr><tr><td>eval/runtime</td><td>â–ˆâ–â–„â–â–â–â–â–â–‚â–â–ƒ</td></tr><tr><td>eval/samples_per_second</td><td>â–â–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†</td></tr><tr><td>eval/steps_per_second</td><td>â–â–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–„â–ƒâ–ˆâ–â–â–â–â–â–â–</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–‡â–†â–†â–…â–„â–ƒâ–ƒâ–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–…â–„â–‚â–‚â–‚â–â–‚â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.89333</td></tr><tr><td>eval/loss</td><td>0.27911</td></tr><tr><td>eval/runtime</td><td>4.1565</td></tr><tr><td>eval/samples_per_second</td><td>36.088</td></tr><tr><td>eval/steps_per_second</td><td>9.142</td></tr><tr><td>total_flos</td><td>1947423786000000.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>1750</td></tr><tr><td>train/grad_norm</td><td>0.00644</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0334</td></tr><tr><td>train_loss</td><td>0.17881</td></tr><tr><td>train_runtime</td><td>447.0073</td></tr><tr><td>train_samples_per_second</td><td>15.66</td></tr><tr><td>train_steps_per_second</td><td>3.915</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">XLNET</strong> at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/l5ewru3i' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/l5ewru3i</a><br> View project at: <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250420_223531-8x6u9qn7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/8x6u9qn7' target=\"_blank\">XLNET</a></strong> to <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/8x6u9qn7' target=\"_blank\">https://wandb.ai/venky_ai-lule-university-of-technology/lab1_Sentiment%20analysis%20small%20dataset/runs/8x6u9qn7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 07:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.324500</td>\n",
       "      <td>0.511418</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.542039</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.463358</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.639185</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.629335</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.624401</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.612341</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.628461</td>\n",
       "      <td>0.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.589297</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 14:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator=DataCollatorWithPadding(xlnet_tokenizer)\n",
    "xlnet_small_trainer,xlnet_small_res=train_model(xlnet_tokenizer,xlnet_model,xlnet_train_small_df,\n",
    "                                                xlnet_val_small_df,learning_rate=lr,epochs=10,run_name=\"XLNET\",data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ce6687d-3fbd-4380-8428-8842fe9efa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4633583724498749, 'eval_accuracy': 0.92, 'eval_runtime': 4.1743, 'eval_samples_per_second': 35.934, 'eval_steps_per_second': 9.103, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "#tokenize test data\n",
    "test_small_df['XLNET Tokenizer']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[0])\n",
    "test_small_df['XLNET Attention_Mask']=test_small_df['Sentence'].apply(lambda x: tokenize_using_hf(x,xlnet_tokenizer)[1])\n",
    "\n",
    "xlnet_test_small_df=HuggingFaceDataset(test_small_df['XLNET Tokenizer'],test_small_df['XLNET Attention_Mask'],test_small_df['Class'])\n",
    "\n",
    "xlnet_test_res=xlnet_small_trainer.evaluate(xlnet_test_small_df)\n",
    "print(xlnet_small_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "935bbfbb-14e3-4b91-89a7-f56ab63e4fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Model': ['Bert', 'Roberta', 'Albert', 'DistilBert', 'XLNET'],\n",
       " 'Accuracy': [0.9266666666666666,\n",
       "  0.9466666666666667,\n",
       "  0.9266666666666666,\n",
       "  0.8933333333333333,\n",
       "  0.92]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_dict(\"XLNET\",xlnet_small_res['eval_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0458923-51f2-4f06-a529-e48402fe4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./small_acc_dict.txt', \"w\") as f:\n",
    "    for key,value in acc_dict_small.items():\n",
    "        f.write(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f3d58-10a9-44d3-8c9e-fd2047fd7cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5f23cb3-245c-48ec-af36-8d40adb4a9f6",
   "metadata": {},
   "source": [
    "### Using classical deep learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403d5c3-d14d-4205-b107-a31e03a7042e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65f3b78-25d2-47ba-8072-d68ab8ff80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data\n",
    "train_small_df['Sentence']=train_small_df['Sentence'].apply(lambda x:preprocess_text(x))\n",
    "val_small_df['Sentence']=val_small_df['Sentence'].apply(lambda x:preprocess_text(x))\n",
    "test_small_df['Sentence']=test_small_df['Sentence'].apply(lambda x:preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8eec7cb-f9d0-4969-9003-f1722bbbea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokrnize text\n",
    "train_small_df['Tokenized Text']=train_small_df['Sentence'].apply(lambda x: tokenize_text(x))\n",
    "val_small_df['Tokenized Text']=val_small_df['Sentence'].apply(lambda x: tokenize_text(x))\n",
    "test_small_df['Tokenized Text']=test_small_df['Sentence'].apply(lambda x: tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26be5a5f-f52c-4598-b173-2f1b571eeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_df['Vectorized Tokens Pretrained']=train_small_df['Tokenized Text'].apply(lambda x: vectorize_word2vec_pretrained(x))\n",
    "val_small_df['Vectorized Tokens Pretrained']=val_small_df['Tokenized Text'].apply(lambda x: vectorize_word2vec_pretrained(x))\n",
    "test_small_df['Vectorized Tokens Pretrained']=test_small_df['Tokenized Text'].apply(lambda x: vectorize_word2vec_pretrained(x))\n",
    "\n",
    "\n",
    "\n",
    "train_small_df=train_small_df[train_small_df['Vectorized Tokens Pretrained'].apply(lambda x: len(x) > 0)]\n",
    "val_small_df=val_small_df[val_small_df['Vectorized Tokens Pretrained'].apply(lambda x: len(x) > 0)]\n",
    "test_small_df=test_small_df[test_small_df['Vectorized Tokens Pretrained'].apply(lambda x: len(x) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eef751d2-f6e9-4559-80a0-90c4f36ac1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Class</th>\n",
       "      <th>Tokenized Text</th>\n",
       "      <th>Vectorized Tokens Pretrained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worst software ever used if i could give this ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[worst, software, ever, used, could, give, zer...</td>\n",
       "      <td>[[0.16601562, -0.08203125, 0.45117188, 0.12255...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bluetooth does not work phone locks up screens...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bluetooth, work, phone, locks, screens, flash...</td>\n",
       "      <td>[[-0.107910156, -0.23046875, -0.21484375, 0.37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im still infatuated with this phone</td>\n",
       "      <td>1</td>\n",
       "      <td>[im, still, infatuated, phone]</td>\n",
       "      <td>[[-0.036621094, 0.014526367, 0.03515625, 0.230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i have this phone and it is a thorn in my side...</td>\n",
       "      <td>0</td>\n",
       "      <td>[phone, thorn, side, really, abhor]</td>\n",
       "      <td>[[-0.014465332, -0.12792969, -0.115722656, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it doesnt work in europe or asia</td>\n",
       "      <td>0</td>\n",
       "      <td>[doesnt, work, europe, asia]</td>\n",
       "      <td>[[-0.075683594, 0.033691406, -0.064941406, 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Class  \\\n",
       "0  worst software ever used if i could give this ...      0   \n",
       "1  bluetooth does not work phone locks up screens...      0   \n",
       "2                im still infatuated with this phone      1   \n",
       "3  i have this phone and it is a thorn in my side...      0   \n",
       "4                   it doesnt work in europe or asia      0   \n",
       "\n",
       "                                      Tokenized Text  \\\n",
       "0  [worst, software, ever, used, could, give, zer...   \n",
       "1  [bluetooth, work, phone, locks, screens, flash...   \n",
       "2                     [im, still, infatuated, phone]   \n",
       "3                [phone, thorn, side, really, abhor]   \n",
       "4                       [doesnt, work, europe, asia]   \n",
       "\n",
       "                        Vectorized Tokens Pretrained  \n",
       "0  [[0.16601562, -0.08203125, 0.45117188, 0.12255...  \n",
       "1  [[-0.107910156, -0.23046875, -0.21484375, 0.37...  \n",
       "2  [[-0.036621094, 0.014526367, 0.03515625, 0.230...  \n",
       "3  [[-0.014465332, -0.12792969, -0.115722656, -0....  \n",
       "4  [[-0.075683594, 0.033691406, -0.064941406, 0.1...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55c1d8cb-9523-4dcd-9a59-4209fbc04da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataloaders\n",
    "train_loader_small=DataLoader(Word2VecData(train_small_df),batch_size=16,collate_fn=collate_fn,num_workers=0)\n",
    "val_loader_small=DataLoader(Word2VecData(val_small_df),batch_size=16,collate_fn=collate_fn)\n",
    "test_loader_small=DataLoader(Word2VecData(test_small_df),batch_size=16,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783f51d8-05fa-4c2c-b7d3-1183fb649d5c",
   "metadata": {},
   "source": [
    "#### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b4081-e7fb-4a66-8d42-1c88ea08e0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4c6336e-027a-4f38-b038-8eeb851830aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordToVecRNNPT(\n",
      "  (rnn): RNN(300, 128)\n",
      "  (hidden_to_op): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "word2vec_pt_model=WordToVecRNNPT(input_size,hidden_size,output_size)\n",
    "print(word2vec_pt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "720460c8-7884-4d33-9c13-9b7e4e68b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:  training loss : 0.694806157187982, val loss: 0.7002431154251099\n",
      "epoch 10:  training loss : 0.6947851749983701, val loss: 0.7002291262149811\n",
      "epoch 15:  training loss : 0.6947641995820132, val loss: 0.7002150893211365\n",
      "epoch 20:  training loss : 0.6947432458400726, val loss: 0.7002010524272919\n",
      "epoch 25:  training loss : 0.6947223015806891, val loss: 0.7001870632171631\n",
      "epoch 30:  training loss : 0.6947013613852587, val loss: 0.7001730382442475\n",
      "epoch 35:  training loss : 0.6946804171258752, val loss: 0.7001590609550477\n",
      "epoch 40:  training loss : 0.6946594999595122, val loss: 0.7001450777053833\n",
      "epoch 45:  training loss : 0.6946385882117532, val loss: 0.700131106376648\n",
      "epoch 50:  training loss : 0.6946176845919002, val loss: 0.700117152929306\n",
      "epoch 55:  training loss : 0.6945967850360003, val loss: 0.7001031637191772\n",
      "epoch 60:  training loss : 0.6945759152824228, val loss: 0.7000892102718353\n",
      "epoch 65:  training loss : 0.6945550374009393, val loss: 0.7000752687454224\n",
      "epoch 70:  training loss : 0.6945341690020128, val loss: 0.7000613510608673\n",
      "epoch 75:  training loss : 0.6945133019577373, val loss: 0.7000474452972412\n",
      "epoch 80:  training loss : 0.694492444396019, val loss: 0.7000334978103637\n",
      "epoch 85:  training loss : 0.694471609863368, val loss: 0.7000195920467377\n",
      "epoch 90:  training loss : 0.6944507942958311, val loss: 0.7000056803226471\n",
      "epoch 95:  training loss : 0.6944299638271332, val loss: 0.6999917924404144\n",
      "epoch 100:  training loss : 0.6944091590968046, val loss: 0.6999778985977173\n",
      "epoch 105:  training loss : 0.694388357075778, val loss: 0.6999640226364136\n",
      "epoch 110:  training loss : 0.6943675496361472, val loss: 0.6999501764774323\n",
      "epoch 115:  training loss : 0.6943467652255838, val loss: 0.6999363005161285\n",
      "epoch 120:  training loss : 0.6943259916522286, val loss: 0.6999224483966827\n",
      "epoch 125:  training loss : 0.6943052234974775, val loss: 0.6999085903167724\n",
      "epoch 130:  training loss : 0.6942844607613303, val loss: 0.699894779920578\n",
      "epoch 135:  training loss : 0.6942637115716934, val loss: 0.6998809218406677\n",
      "epoch 140:  training loss : 0.6942429569634524, val loss: 0.6998671293258667\n",
      "epoch 145:  training loss : 0.6942222335121848, val loss: 0.6998532950878144\n",
      "epoch 150:  training loss : 0.6942015059969642, val loss: 0.6998394787311554\n"
     ]
    }
   ],
   "source": [
    "#train the network\n",
    "train_network(word2vec_pt_model,train_loader_small,val_loader_small,epochs,lr,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d6b067a2-8d62-411b-959f-d2c89da2782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "rnn_test_acc=test_model(word2vec_pt_model,test_loader_small)\n",
    "print(f\"{rnn_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd655ba-7c54-4eeb-9f1c-486826e2ee97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "088db486-edfa-4395-90d0-6f2a51a8d11f",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a666b88-cf14-4d6b-9aa6-c7e863a20b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (LSTM): LSTM(300, 128)\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid_layer): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lstm_model=LSTMModel(input_size,hidden_size,output_size)\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac2f6286-c1f3-480e-9028-d1657357250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:  training loss : 0.6937635134566914, val loss: 0.6941172420978546\n",
      "epoch 10:  training loss : 0.6937615559859709, val loss: 0.6941155254840851\n",
      "epoch 15:  training loss : 0.6937596147710626, val loss: 0.6941137909889221\n",
      "epoch 20:  training loss : 0.6937576708468524, val loss: 0.6941121041774749\n",
      "epoch 25:  training loss : 0.6937557147307829, val loss: 0.6941103875637055\n",
      "epoch 30:  training loss : 0.693753791126338, val loss: 0.6941086828708649\n",
      "epoch 35:  training loss : 0.6937518323009665, val loss: 0.6941069483757019\n",
      "epoch 40:  training loss : 0.6937498870221052, val loss: 0.6941052496433258\n",
      "epoch 45:  training loss : 0.6937479309060357, val loss: 0.6941035449504852\n",
      "epoch 50:  training loss : 0.6937459937550805, val loss: 0.6941018283367157\n",
      "epoch 55:  training loss : 0.6937440457669172, val loss: 0.6941001117229462\n",
      "epoch 60:  training loss : 0.6937420977787538, val loss: 0.6940983951091766\n",
      "epoch 65:  training loss : 0.6937401646917517, val loss: 0.694096690416336\n",
      "epoch 70:  training loss : 0.6937382126396353, val loss: 0.694094979763031\n",
      "epoch 75:  training loss : 0.693736267360774, val loss: 0.6940932691097259\n",
      "epoch 80:  training loss : 0.6937343125993555, val loss: 0.6940915524959564\n",
      "epoch 85:  training loss : 0.6937323632565412, val loss: 0.6940898478031159\n",
      "epoch 90:  training loss : 0.6937304369427941, val loss: 0.6940881371498108\n",
      "epoch 95:  training loss : 0.6937284848906777, val loss: 0.6940864264965058\n",
      "epoch 100:  training loss : 0.6937265382571653, val loss: 0.6940847337245941\n",
      "epoch 105:  training loss : 0.6937246038155123, val loss: 0.6940830290317536\n",
      "epoch 110:  training loss : 0.6937226504087448, val loss: 0.694081312417984\n",
      "epoch 115:  training loss : 0.6937207051298835, val loss: 0.6940796196460723\n",
      "epoch 120:  training loss : 0.6937187720428813, val loss: 0.6940778851509094\n",
      "epoch 125:  training loss : 0.693716821345416, val loss: 0.6940761923789978\n",
      "epoch 130:  training loss : 0.6937148679386486, val loss: 0.6940744698047638\n",
      "epoch 135:  training loss : 0.6937129240144383, val loss: 0.6940727710723877\n",
      "epoch 140:  training loss : 0.6937109963460402, val loss: 0.6940710663795471\n",
      "epoch 145:  training loss : 0.693709051067179, val loss: 0.6940693497657776\n",
      "epoch 150:  training loss : 0.6937071057883176, val loss: 0.6940676629543304\n"
     ]
    }
   ],
   "source": [
    "#train the lstm model \n",
    "train_network(lstm_model,train_loader_small,val_loader_small,epochs,lr,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9cfd6114-1863-4258-9e1a-a090a275b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc=test_model(lstm_model,test_loader_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72cb5e58-40b8-4298-ac9d-d9ebfd2681dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4563758373260498"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5d8d948-a19b-4d61-b60c-cea1abb96182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5:  training loss : 0.6985453827814623, val loss: 0.696148031949997\n",
      "epoch 10:  training loss : 0.6985357593406331, val loss: 0.696139395236969\n",
      "epoch 15:  training loss : 0.6985261778939854, val loss: 0.6961307287216186\n",
      "epoch 20:  training loss : 0.6985165571624582, val loss: 0.6961220860481262\n",
      "epoch 25:  training loss : 0.6985069621693004, val loss: 0.6961134254932404\n",
      "epoch 30:  training loss : 0.6984973644668405, val loss: 0.696104770898819\n",
      "epoch 35:  training loss : 0.6984877748922869, val loss: 0.69609614610672\n",
      "epoch 40:  training loss : 0.698478178544478, val loss: 0.6960875213146209\n",
      "epoch 45:  training loss : 0.6984685903245752, val loss: 0.6960788786411285\n",
      "epoch 50:  training loss : 0.6984590251337398, val loss: 0.6960702419281006\n",
      "epoch 55:  training loss : 0.6984494409777902, val loss: 0.696061646938324\n",
      "epoch 60:  training loss : 0.6984398771416057, val loss: 0.696053010225296\n",
      "epoch 65:  training loss : 0.6984302984042601, val loss: 0.696044385433197\n",
      "epoch 70:  training loss : 0.6984207399866798, val loss: 0.6960357844829559\n",
      "epoch 75:  training loss : 0.6984111802144484, val loss: 0.6960271656513214\n",
      "epoch 80:  training loss : 0.6984016272154722, val loss: 0.6960185706615448\n",
      "epoch 85:  training loss : 0.6983920782804489, val loss: 0.6960099339485168\n",
      "epoch 90:  training loss : 0.6983825144442645, val loss: 0.6960013628005981\n",
      "epoch 95:  training loss : 0.6983729736371473, val loss: 0.6959927380084991\n",
      "epoch 100:  training loss : 0.6983634355393323, val loss: 0.6959841549396515\n",
      "epoch 105:  training loss : 0.6983538893136111, val loss: 0.6959755659103394\n",
      "epoch 110:  training loss : 0.6983443701809103, val loss: 0.6959669828414917\n",
      "epoch 115:  training loss : 0.6983348456296053, val loss: 0.6959583997726441\n",
      "epoch 120:  training loss : 0.6983253278515555, val loss: 0.6959498405456543\n",
      "epoch 125:  training loss : 0.6983157911083915, val loss: 0.6959412634372711\n",
      "epoch 130:  training loss : 0.6983062760396437, val loss: 0.6959327101707459\n",
      "epoch 135:  training loss : 0.6982967650348489, val loss: 0.6959241271018982\n",
      "epoch 140:  training loss : 0.6982872594486583, val loss: 0.695915573835373\n",
      "epoch 145:  training loss : 0.6982777376066555, val loss: 0.6959069907665253\n",
      "epoch 150:  training loss : 0.698268240148371, val loss: 0.6958984315395356\n"
     ]
    }
   ],
   "source": [
    "#create an instnace of the model\n",
    "gru_model=GRUModel(input_size,hidden_size,output_size)\n",
    "\n",
    "\n",
    "#train the model\n",
    "train_network(gru_model,train_loader_small,val_loader_small,epochs,lr,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870d09f-0329-4215-ad1c-a493afa45543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "612b8b13-f5cf-4964-9730-1b0613326659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50\n"
     ]
    }
   ],
   "source": [
    "gru_test_acc=test_model(gru_model,test_loader_small)\n",
    "print(f\"{gru_test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "453129ad-9b9d-4801-b155-83a6bf235ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('small_acc_dict.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "    model_part = data.split(\"Accuracy\")[0].replace(\"Model :\", \"\").strip()\n",
    "    acc_part = data.split(\"Accuracy :\")[1].strip()\n",
    "\n",
    "    acc_small_dict = {\n",
    "        \"Model\": eval(model_part),\n",
    "        \"Accuracy\": eval(acc_part)\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d8138244-566c-4ea0-aefd-5f4f1f2afe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_small_dict['Model'].extend([\"RNN\",\"LSTM\",\"GRU\"])\n",
    "acc_small_dict['Accuracy'].extend([rnn_test_acc,test_acc,gru_test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "631fc767-ceda-45aa-8fbb-9ba949d0632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_small_df=pd.DataFrame(acc_small_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f39cf2a3-2abb-4e57-839d-f2728515350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Model', ylabel='Accuracy'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAK6pJREFUeJzt3QmcTnX///GPbWzZxxLJvmYnY61bYaJEq9xlhEaJKI9kC0UZkSWZuMmgInMTpTtxl5IKTc1YWqQQJruUtYzM9Xt8vv//mfu6Zq5ZzHJd13zn9Xw8DnOd61zXdc65zjnX+3zP55yTz+VyuQQAAMAS+f09AgAAANmJcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYJWCksckJibKkSNHpESJEpIvXz5/jw4AAMgAvSzfuXPnpHLlypI/f9ptM3ku3GiwqVq1qr9HAwAAZEJ8fLxcd911aQ6T58KNttg4M6dkyZL+Hh0AAJABZ8+eNY0Tzu94WvJcuHEORWmwIdwAAJC7ZKSkhIJiAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUK+nsE4H8tR74hNoidHubvUQAABABabgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArMKp4G44JTpv4fsGADvRcgMAAKxCuAEAAFbhsBSAPIHDkEDeQcsNAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAKVygG8hiu1AvAdrTcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYxe/hJjIyUqpXry5FihSRkJAQiYmJSXP42bNnS7169aRo0aJStWpVeeqpp+Svv/7y2fgCAIDA5tdwEx0dLSNGjJCJEydKXFycNG3aVEJDQ+XEiRNeh1++fLmMHj3aDL97925ZtGiReY+xY8f6fNwBAEBg8mu4mTlzpoSHh0v//v2lYcOGMn/+fClWrJhERUV5HX7Lli3Svn17+ec//2lae7p27Sp9+vRJs7Xn0qVLcvbsWY8OAADYy2/hJiEhQWJjY6Vz587/G5n8+c3jrVu3en1Nu3btzGucMLN//35Zt26ddO/ePdXPiYiIkFKlSiV1eigLAADYq6C/PvjUqVNy5coVqVixokd/ffzjjz96fY222OjrOnToIC6XS/7++2957LHH0jwsNWbMGHPoy6EtNwQcAADs5feC4quxadMmmTJlirz22mumRmf16tXywQcfyOTJk1N9TeHChaVkyZIeHQAAsJffWm6Cg4OlQIECcvz4cY/++rhSpUpeXzN+/Hjp27evPPLII+Zx48aN5cKFCzJo0CAZN26cOawFAADyNr+lgaCgIGnZsqVs3LgxqV9iYqJ53LZtW6+vuXjxYooAowFJ6WEqAAAAv7XcKK2F6devn7Rq1Upat25trmGjLTF69pQKCwuTKlWqmKJg1aNHD3OGVfPmzc01cfbu3Wtac7S/E3IAAEDe5tdw07t3bzl58qRMmDBBjh07Js2aNZP169cnFRkfOnTIo6Xm2WeflXz58pn/Dx8+LOXLlzfB5sUXX/TjVAAAgEDi13Cjhg4darrUCojdFSxY0FzATzsAAABvqMAFAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWMXv4SYyMlKqV68uRYoUkZCQEImJiUlz+D/++EOGDBki1157rRQuXFjq1q0r69at89n4AgCAwFbQnx8eHR0tI0aMkPnz55tgM3v2bAkNDZU9e/ZIhQoVUgyfkJAgXbp0Mc+tWrVKqlSpIgcPHpTSpUv7ZfwBAEDg8Wu4mTlzpoSHh0v//v3NYw05H3zwgURFRcno0aNTDK/9T58+LVu2bJFChQqZftrqAwAA4PfDUtoKExsbK507d/7fyOTPbx5v3brV62vWrl0rbdu2NYelKlasKI0aNZIpU6bIlStXUv2cS5cuydmzZz06AABgL7+Fm1OnTplQoiHFnT4+duyY19fs37/fHI7S12mdzfjx42XGjBnywgsvpPo5ERERUqpUqaSuatWq2T4tAAAgcPi9oPhqJCYmmnqbBQsWSMuWLaV3794ybtw4czgrNWPGjJEzZ84kdfHx8T4dZwAAkEdqboKDg6VAgQJy/Phxj/76uFKlSl5fo2dIaa2Nvs7RoEED09Kjh7mCgoJSvEbPqNIOAADkDX5rudEgoq0vGzdu9GiZ0cdaV+NN+/btZe/evWY4x08//WRCj7dgAwAA8h6/HpbS08AXLlwoS5culd27d8vgwYPlwoULSWdPhYWFmcNKDn1ez5YaPny4CTV6ZpUWFGuBMQAAgN9PBdeamZMnT8qECRPMoaVmzZrJ+vXrk4qMDx06ZM6gcmgx8IYNG+Spp56SJk2amOvcaNAZNWqUH6cCAAAEEr+GGzV06FDTebNp06YU/fSQ1bZt23wwZgAAIDfKVWdLAQAApIdwAwAArEK4AQAAeTvc6L2cJk2aZIp9AQAAcn24efLJJ2X16tVSs2ZNc4fuFStWmPs3AQAA5Npws2PHDomJiTFXB37iiSfMRfT0jKe4uLicGUsAAICcrrlp0aKFzJkzR44cOSITJ06U119/XW688UZzrZqoqChxuVyZfWsAAADfX+fm8uXLsmbNGlm8eLF89NFH0qZNGxk4cKD8+uuvMnbsWPn4449l+fLlmR8zAAAAX4QbPfSkgebtt982Vw/WWyTMmjVL6tevnzTMXXfdZVpxAAAAAj7caGjRQuJ58+ZJr169zF26k6tRo4Y88MAD2TWOAAAAORdu9u/fL9WqVUtzmOLFi5vWHQAAgIAvKD5x4oR89dVXKfprv2+++Sa7xgsAAMA34WbIkCESHx+fov/hw4fNcwAAALkq3Pzwww/mNPDkmjdvbp4DAADIVeGmcOHCcvz48RT9jx49KgULZvrMcgAAAP+Em65du8qYMWPkzJkzSf3++OMPc20bPYsKAADAn666qeXll1+Wm266yZwxpYeilN6OoWLFivLmm2/mxDgCAADkXLipUqWK7Nq1S5YtWyY7d+6UokWLSv/+/aVPnz5er3kDAADgS5kqktHr2AwaNCj7xwYAACCLMl0BrGdGHTp0SBISEjz633nnnVkdJwAAAN9eoVjvHfXtt99Kvnz5ku7+rX+rK1euZH5sAAAAfH221PDhw829o/RKxcWKFZPvv/9eNm/eLK1atZJNmzZldXwAAAB823KzdetW+eSTTyQ4ONjcFVy7Dh06SEREhAwbNky2b9+etTECAADwZcuNHnYqUaKE+VsDzpEjR8zfemr4nj17sjIuAAAAvm+5adSokTkFXA9NhYSEyLRp0yQoKEgWLFggNWvWzPoYAQAA+DLcPPvss3LhwgXz96RJk+SOO+6Qjh07Srly5SQ6Ojor4wIAAOD7cBMaGpr0d+3ateXHH3+U06dPS5kyZZLOmAIAAMgVNTeXL182N8f87rvvPPqXLVuWYAMAAHJfuNHbK1x//fVcywYAANhzWGrcuHHmDuB6k0xtsQEAAIGh5cg3xAax08N8G27mzp0re/fulcqVK5vTv/U+U+7i4uKyNEIAAAA+DTe9evXK0gcCAAAEVLiZOHFizowJAACAP65QDAAAYFXLjd5LKq3TvjmTCgAA5Kpws2bNmhTXvtGbZS5dulSef/757Bw3AACAnA83PXv2TNHv3nvvlRtuuMHcfmHgwIFXPxYAAACBVnPTpk0b2bhxY3a9HQAAgP/CzZ9//ilz5syRKlWqZMfbAQAA+O6wVPIbZLpcLjl37pwUK1ZM3nrrrcyPCQAAgD/CzaxZszzCjZ49Vb58eQkJCTHBBwAAIFeFm4cffjhnxgQAAMAfNTeLFy+WlStXpuiv/fR0cAAAgFwVbiIiIiQ4ODhF/woVKsiUKVOya7wAAAB8E24OHTokNWrUSNFf7xCuzwEAAOSqcKMtNLt27UrRf+fOnVKuXLnsGi8AAADfFBT36dNHhg0bJiVKlJCbbrrJ9Pvss89k+PDh8sADD2RuLAAAyEYtR74hNoidHubvUcgb4Wby5Mly4MABufXWW6Vgwf/38sTERAkLC6PmBgAA5L5wExQUZO4h9cILL8iOHTukaNGi0rhxY1NzAwAAkOvCjaNOnTqmAwAAyNUFxffcc4+89NJLKfpPmzZN7rvvvuwaLwAAAN+Em82bN0v37t1T9O/WrZt5DgAAIFeFm/Pnz5u6m+QKFSokZ8+eza7xAgAA8E240eJhLShObsWKFdKwYcPMjQUAAIC/CorHjx8vd999t+zbt09uueUW02/jxo2yfPlyWbVqVXaNFwAAgG/CTY8ePeTdd98117TRMKOngjdt2lQ++eQTKVu2bObGAgAAwJ+ngt9+++2mU1pn8/bbb8vTTz8tsbGxcuXKlewaNwAAgJyvuXHomVH9+vWTypUry4wZM8whqm3btmX27QAAAHzfcnPs2DFZsmSJLFq0yLTY3H///XLp0iVzmIpiYgAAkKtabrTWpl69euaO4LNnz5YjR47Iq6++mrNjBwAAkFMtNx9++KG5G/jgwYO57QIAAMj9LTdffPGFnDt3Tlq2bCkhISEyd+5cOXXqVM6OHQAAQE6FmzZt2sjChQvl6NGj8uijj5qL9mkxcWJionz00Ucm+AAAAOS6U8GLFy8uAwYMMN2ePXtMcfHUqVNl9OjR0qVLF1m7du1Vj0RkZKRMnz7dFCzrNXO0lqd169bpvk4DVp8+faRnz56mqBkA4KnlyDfEBrHTw/w9CsgLp4IrLTDWu4H/+uuv5lo3maG3chgxYoRMnDhR4uLiTLgJDQ2VEydOpPm6AwcOmGvrdOzYMZNjDwAAbJSlcOMoUKCA9OrVK1OtNjNnzpTw8HDp37+/OZ18/vz5UqxYMYmKikr1NXqhwAcffFCef/55qVmzZhbHHgAA2CRbwk1mJSQkmKsad+7c+X8jlD+/ebx169ZUXzdp0iSpUKGCDBw4MN3P0Ovw6DV53DsAAGAvv4YbPdtKW2EqVqzo0V8fa/1NamdtaZ2PFjdnREREhJQqVSqpq1q1araMOwAACEx+DTdXS8/I6tu3rwk2wcHBGXrNmDFj5MyZM0ldfHx8jo8nAADIZTfOzC4aULRe5/jx4x799XGlSpVSDL9v3z5TSKxXS3boqeiqYMGC5uytWrVqebymcOHCpgMAAHmDX1tugoKCzEUBN27c6BFW9HHbtm1TDF+/fn359ttvZceOHUndnXfeKZ06dTJ/c8gJAAD4teVG6WngenfxVq1amWvb6H2rLly4YM6eUmFhYVKlShVTO1OkSBFp1KiRx+tLly5t/k/eHwAA5E1+Dze9e/eWkydPyoQJE0wRcbNmzWT9+vVJRcaHDh0yZ1ABAADkinCjhg4dajpvNm3alOZrlyxZkkNjBQAAciOaRAAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrBES4iYyMlOrVq0uRIkUkJCREYmJiUh124cKF0rFjRylTpozpOnfunObwAAAgb/F7uImOjpYRI0bIxIkTJS4uTpo2bSqhoaFy4sQJr8Nv2rRJ+vTpI59++qls3bpVqlatKl27dpXDhw/7fNwBAEDg8Xu4mTlzpoSHh0v//v2lYcOGMn/+fClWrJhERUV5HX7ZsmXy+OOPS7NmzaR+/fry+uuvS2JiomzcuNHr8JcuXZKzZ896dAAAwF5+DTcJCQkSGxtrDi0ljVD+/OaxtspkxMWLF+Xy5ctStmxZr89HRERIqVKlkjpt6QEAAPbya7g5deqUXLlyRSpWrOjRXx8fO3YsQ+8xatQoqVy5skdAcjdmzBg5c+ZMUhcfH58t4w4AAAJTQcnFpk6dKitWrDB1OFqM7E3hwoVNBwAA8ga/hpvg4GApUKCAHD9+3KO/Pq5UqVKar3355ZdNuPn444+lSZMmOTymAAAgt/DrYamgoCBp2bKlRzGwUxzctm3bVF83bdo0mTx5sqxfv15atWrlo7EFAAC5gd8PS+lp4P369TMhpXXr1jJ79my5cOGCOXtKhYWFSZUqVUxhsHrppZdkwoQJsnz5cnNtHKc255prrjEdAADI2/webnr37i0nT540gUWDip7irS0yTpHxoUOHzBlUjnnz5pmzrO69916P99Hr5Dz33HM+H38AABBY/B5u1NChQ03njRYLuztw4ICPxgoAAORGfr+IHwAAQHYi3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYJSDCTWRkpFSvXl2KFCkiISEhEhMTk+bwK1eulPr165vhGzduLOvWrfPZuAIAgMDm93ATHR0tI0aMkIkTJ0pcXJw0bdpUQkND5cSJE16H37Jli/Tp00cGDhwo27dvl169epnuu+++8/m4AwCAwOP3cDNz5kwJDw+X/v37S8OGDWX+/PlSrFgxiYqK8jr8K6+8IrfddpuMHDlSGjRoIJMnT5YWLVrI3LlzfT7uAAAg8BT054cnJCRIbGysjBkzJqlf/vz5pXPnzrJ161avr9H+2tLjTlt63n33Xa/DX7p0yXSOM2fOmP/Pnj2bYtgrl/4UG3ibtrQw3bkb050xTHfuxnRnjM3Tffb/93O5XOm/gcuPDh8+rGPo2rJli0f/kSNHulq3bu31NYUKFXItX77co19kZKSrQoUKXoefOHGi+Qw6Ojo6Ojo6yfVdfHx8uvnCry03vqCtQu4tPYmJiXL69GkpV66c5MuXz6fjoqmzatWqEh8fLyVLlpS8gulmuvMCppvpzgvO+nG6tcXm3LlzUrly5XSH9Wu4CQ4OlgIFCsjx48c9+uvjSpUqeX2N9r+a4QsXLmw6d6VLlxZ/0gUiL60MDqY7b2G68xamO28p6afpLlWqVOAXFAcFBUnLli1l48aNHi0r+rht27ZeX6P93YdXH330UarDAwCAvMXvh6X0kFG/fv2kVatW0rp1a5k9e7ZcuHDBnD2lwsLCpEqVKhIREWEeDx8+XG6++WaZMWOG3H777bJixQr55ptvZMGCBX6eEgAAEAj8Hm569+4tJ0+elAkTJsixY8ekWbNmsn79eqlYsaJ5/tChQ+YMKke7du1k+fLl8uyzz8rYsWOlTp065kypRo0aSaDTw2N6PZ/kh8lsx3Qz3XkB08105wWFc8l059OqYn+PBAAAgDUX8QMAAMhOhBsAAGAVwg0AALAK4QZZsmnTJnMxxD/++EPy4vQuWbLE79dNyiqdntRuX5IRzz33nDkRwPHwww+bm9kCgL8QbrJAN+L6w+B0etVjvannrl27svze1atXN6fF+3IaChUqJDVq1JBnnnlG/vrrL/G3AwcOmPHasWOHzz9b72GmF5jUyw3k1uCY/LvVMxC7dOlibkqr15NyHD16VLp165bpIPT000+nuPaUr9aTzK4rV65cMWde3n333R799d5zevXVcePGpbv8abDV53Va3On3pf31+3O4T797p5eySD5/knc6fb6Ske2BPlekSBE5ePCgx2s10Orrk7/X1KlTPYbT5cfXV4f3Jq0QvnPnTrnzzjulQoUKZlr1O9Aze0+cOGHCfFrflzNtzvQ/9thjKd5/yJAh5jn3+eVPx44dM5dZqV27tple3Va0b99e5s2bJxcvXjTD6Dxwpk9vbt24cWN5/fXXPd4nrZ29rO5EXS3CTRbphk1/HLTTDXzBggXljjvuyNLNRP01Dfv375dZs2bJv/71L3Oqnz/5Yz64W7RokTzxxBOyefNmOXLkiM8///Lly9n63eoP9YcffiidOnUyGzFdRv/++28zjF7dOyundV5zzTUmsPhyPcnqMqLBVTfEetmJZcuWJfXX77xs2bIZXv51Oj7++GP59NNP0x128eLFSfPA6fTH9ZVXXvHol3zYr7/+WgJte6A/VHr5jvToD+VLL70kv//+u+QWemmSW2+91SwHGzZskN27d5vvQy/5r9dg0zDv/n1dd911MmnSpBTfodKgrAH2zz//dzNLDYp6OZPrr79eAsH+/fulefPm8t///lemTJki27dvNzt3Gmr/85//mOXb4Uznd999Jw899JCEh4eb7UogItxkkf4o6I+Ddto0P3r0aHPPDV1BlP59//33mzSrK0vPnj3ND03yvYcXX3zRrDz16tWTf/zjH2av6KmnnvLYE8jpadAVUcdF78quV31Wekf1YcOGJe3BdOjQwevG9ssvv5QmTZqYYdq0aWMWfndffPGFdOzYUYoWLWo+R99TNxQO3SuYPHmyuWijXtJ70KBBZq9R6Yqn80Dni9LP1xYIvX2HXopbL+oYFxeXbfPj/PnzEh0dLYMHDzYtN/ojmB7dI9FrLun0613q9Xt3995770mLFi3M8zVr1pTnn38+KVwonT7dS9K9xeLFi5uNhgYRVaZMmUzv5TnfrV4IUz9frw2l46IbJGe63PeoNDAMHTpUrr32WjOu1apVS7qAptOCcNddd3m0KCQ/LJWZ9cQf60rdunVNq4IGGt1g63zRH6I33njDXD09I/S7GjBggJme9Oh0OfPA6XQe6zLs3i/5sOXLlxdfSmt74NBl5K233kqxnienr9X3cpah3EC3ZdqCp60Suu3R7ZCuixr09G8N8+7flwblEiVKpPgOla5zOh9Xr16d1E//1mCj7x0IHn/8cRPS9WK4uv41aNDAbKN0/fvggw+kR48eScM606nPjxo1yqynyZeNQEG4yUb6o6grvDbt6Z6s7n3rD50uEJ9//rlZaXTF0D0j971O3ZPds2ePWUg0KevCn3xvwFd0Y7Vly5akjbum93feeUeWLl1qAoROm06T3nzU3ciRI81VozV46MZYVwin9WHfvn1mmu+55x5zKEKDg4Yd3UC6e/nll6Vp06Zmz2H8+PESExNj+uueg84DZwOhN07Tq1rre2zbts2Eiu7du5v+2eHf//631K9f3/x46t6JHsZJ63JQ2myrP7j6o6jfsR6WeOCBB5Ke1+9eQ5u2mPzwww9mT1iDhb7GnYYEDQ7ffvutCT8635UuGzr9uoefHW655RYzn903uI45c+bI2rVrzTzQz9VWDSfEOKHWaVXIbItC8vVE+Wtd0WCj86Jv374mUGtrhD6+Gvq96Xe2atUqsU3y7YFDD1loy1t6oU5/+LU14NVXX5Vff/1VcgP98dYdjzVr1qS53meUhl9dZxy6PXGuwO9vv/32m2mx0cNkGtS98bbDoIe1dfukLXIZ3RHwuXTvG45U9evXz1WgQAFX8eLFTaez89prr3XFxsaa5998801XvXr1XImJiUmvuXTpkqto0aKuDRs2JL1HxYoVTX931apVc82aNcun01C4cGEzDfnz53etWrXKdf78eVehQoVcy5YtSxo+ISHBVblyZde0adPM408//dS8ZsWKFUnD/Pbbb2Yao6OjzeOBAwe6Bg0a5PG5n3/+ufmcP//8M2l6e/Xq5THML7/8Yt57+/btaU7DlStXXCVKlHC9//772TBHXK527dq5Zs+ebf6+fPmyKzg42Eyn+/T+/vvv5vHixYvN423btiW9fvfu3abfV199ZR7feuutrilTpnh8hi4buqw4dPgnn3zSY5jkn5WZ77Znz55en+vdu7erQYMGSZ+9Zs0a8/cTTzzhuuWWWzyWWXfuwzomTpzoatq0aaqfm9564u91xfm+GjdubL7vjC5/+t2XKlXK/D169GhX3bp1zev1+9LXOcuM0sdFihRJmgdOd/DgwQzNY19Ja3uQfPy+//57M+zmzZtNf/3O9fXeloM2bdq4BgwYYP7W1wbCT09a68fYsWNdBQsWdJUtW9Z12223me3dsWPHvA6b2vLnvP+JEyfMvDxw4IDpdDk4efJkivnlD9u2bTPfxerVqz36lytXLmkZfeaZZ5KmMygoyPTTeaOv0/nz888/e10n/L1c03KTRdpcqQWH2mlLg+59anGmNpVrUdrevXvN3qjuhWqnzXh6zFVbMxxamOXP9OtMw1dffWVaRHSvQltZdBx1j1r30hxaZKj3ANPj0O7cb1yq06itHs4wOh+0pcKZB9rpfNL0/8svvyS9Tu8vlhF6F3g9bKMtNtqkr4extDVAb9WRVdoqoN9jnz59zGNtrtVCQq3BSY0Oc+ONNyY91lYfPazgPv3asuA+/Tr+2srgFOtdzfRnB93WeNsj00M/uizo96eHDnWvLqfXE+XPdUX3pLVAUpfFzLYuaBO9HmLT90qNHtZw5oHT6eG1QJPa9iC5hg0bmhbJjByS07obbf1Nvt0IVNqqqkW28+fPlxtuuMH8r+u1ttBdLW3Jdg5vawuO/q2H1ANZTEyMWQZ02rU0wb2FXvt/8sknEhISYpZpbYENRH6/t1Rup0157l+uHqfVH9yFCxeaH1y967l7waLD/Th6as2B/pgG3Thrs7z+mLv/YGeFzodHH33U/Fgm515Ul9H5oBtcbU7VwzRaE6I1AhqusqMIWadbm6Tdf3Q0COhnzJ07N9PTr4eZkp+Zo7Tmwh/Lgf7IODVN7rRGQH/ktSZHDwfqMXitm8jqIZe01pMXXnjBb+uKHnLRDbSGOB2PgQMHmum+2jo3DbNjxowx33NqhdJ6uCNQfwgysj3QeZOcTq/WLqV3FsxNN91kAq3Oo0A5Qyg9esj0vvvuM50eWtMaGT10riEtM4emnMPwkZGREihq165tlnXdqXOnNTVKayTdaSjT12i3cuVKs7OhO2UadJXuaGotpe64ut8T0jnjU9d5X6HlJpvpgqJfqlbH6w/Fzz//bIpxnQXC6dL7knXvVE9X9TUddy061RuT1qpVy4yH1j84tCVHay2chdmhtS8OPQ77008/mcI0pfNBa02SzwPt0toLd55LPh90fDQoaZ2N7llo8Dh16lSWp11DjdbNaO2Q+961tipo2Hn77bdTfZ0W4zl0Q6Ers/v0az9v0+++Acjo9GeV7nXpHqi3vXFnA6WtVRo8tD5Kj607NVbacpcd4+O+nih/rCvaaqY/tFo4rq0V+gOue6y6l54ZWr+j05RdtVGBwH174H7Gj0OLZfVHW4dJ7zvQ4u3333/fnImT2+gypttD95MgroZTO+bUlgVSgOvSpYvZcbvaadPvXrcTGlgd2uKr28Pkl09wTvjQIOwrhJss0iY7bb7UTveGdQOne6FaUPvggw+apKtV51okqXvEeu0L/WFOr/lbizj1NOTDhw9nyw/31dA9FS0E1LN3dMOvTZF6yqwGFD2coj8Kyffi9LCLFntqAaL+YOh0O9eQ0CZ73UPWjaAu9PojpmemJC8oTk5/6HTPQT9bD0XpGQxKD0e9+eabZn5r07nO5+R7GJmhBaoazHTa9C7z7p0GgdQOTekPvn7vOi6xsbFm+vWMMT18p7RIVUOT7uV+//33Zrz1rBz9wUiLtkppCNDx0kMeulxldvnU5Ug3MLoHqsujti7oIYXkZs6caULcjz/+aAKq7p1pi4Nz7QpdLvV71ve8mtN701pPlD/WFd0oa6uccx0WfR/dM9cieveztDSYJj+c5O1UfW2F0+9Yi7K90cDrzAOny+yPpT+2B6m1OOh81MsluJ8y7I3u5ev3nNr88QfdpiT/bnXboicS6Hqn64B+/7pcrFu3ziyfmaHzT5d73Ybq34HktddeM4FEW2B0Z0bHU6dZi/51O5DW+OpJEhpYnZ073dns2rWraanS7YSux7r91jOyNAjpWZs+47PqHgtpMZjOQqfTotYbb7zRo/ju6NGjrrCwMFOUqkVlNWvWdIWHh7vOnDmTZlHb1q1bXU2aNEkq6svJafD2+REREa7y5cubomItMnXGv3379q6YmJgURa9azHvDDTeYgrPWrVu7du7c6fF++pouXbq4rrnmGlOQptP24osvpluUt3DhQlfVqlVNUePNN99s+sXFxblatWplCvPq1KnjWrlyZbYUYN9xxx2u7t27e31Oi4N1Ol955ZUUBcVaQPfOO++Y71bnUefOnVMUiq5fv94UKmuBbMmSJc08WrBgQbrFdpMmTXJVqlTJlS9fvqsuPnRfPrUAUL9PHbeoqChThO3ts3WcmjVrZr4jHU8thtb57Vi7dq2rdu3a5v10nme0oDi99cTX68qmTZtMMawWtifXtWtXU1S9f/9+j/F27+Lj470WT/7999+uhg0bei0o9tbpehZoBcXpbQ+8jZ8WzGv/1AqK3Yu0dRsRCD89yZdLp+vUqZNZ7rRAXNfX0qVLm+VVv+/MFBSnJhAKih1HjhxxDR061FWjRg1zEolup3UbNX36dNeFCxfSnM7Q0FBXt27dkh7rtnHYsGGuWrVqmfmn22gtSj537pzLl/LpP76LUgAAADmLw1IAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwCsp7dy0FtZODfwywi9HcPs2bNzdLwA5AzCDQC/0/txafh47LHHUjw3ZMgQ81xuuZs0AP8j3AAICHqXYb2hqPvdp//66y9Zvny5XH/99X4dNwC5C+EGQEBo0aKFCTirV69O6qd/a7Bp3ry5xx3G9W7hetd4vRN3hw4d5Ouvv/Z4L72Dc926dc3d4jt16uRxl2/HF198IR07djTD6Ofqe+aGu3QDSB/hBkDAGDBggCxevDjpcVRUlPTv399jmGeeeUbeeecdWbp0qcTFxUnt2rUlNDRUTp8+bZ6Pj4+Xu+++W3r06CE7duyQRx55REaPHu3xHvv27ZPbbrtN7rnnHtm1a5dER0ebsDN06FAfTSmAnES4ARAwHnroIRMyDh48aLovv/zS9HNoy8q8efNk+vTp0q1bN2nYsKEsXLjQtL4sWrTIDKPP16pVS2bMmCH16tWTBx98MEW9TkREhOn/5JNPSp06daRdu3YyZ84ceeONN8yhMAC5W0F/jwAAOMqXLy+33367LFmyRFwul/k7ODjYo8Xl8uXL0r59+6R+hQoVktatW8vu3bvNY/0/JCTE433btm3r8Xjnzp2mxWbZsmVJ/fTzEhMT5ZdffpEGDRrk4FQCyGmEGwABd2jKOTwUGRmZI59x/vx5efTRR02dTXIULwO5H+EGQEDRWpiEhARz+rfW0rjTw01BQUHmcFW1atVMP23J0YJiPcSktNVl7dq1Hq/btm1biuLlH374wdTrALAPNTcAAkqBAgXMoSUNH/q3u+LFi8vgwYNl5MiRsn79ejNMeHi4XLx4UQYOHGiG0Wvl/Pzzz2aYPXv2mFPJ9TCXu1GjRsmWLVtMC5EWHevw7733HgXFgCUINwACTsmSJU3nzdSpU81ZTn379jUtMHv37pUNGzZImTJlkg4r6dlU7777rjRt2lTmz58vU6ZM8XiPJk2ayGeffSY//fSTOR1cTzWfMGGCVK5c2SfTByBn5XNpFR0AAIAlaLkBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgNjk/wB7wXVC9xpIGAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(acc_small_dict,x=\"Model\",y=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6dc866-8ffd-4f0a-aef0-4a879bf0f0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venky-env)",
   "language": "python",
   "name": "venky-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
